<!doctype html><html lang=ko dir=ltr class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated=false><head><meta charset=UTF-8><meta name=generator content="Docusaurus v3.9.1"><title data-rh=true>VLA Test Review | gracefullight.dev</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"/><meta data-rh=true name=twitter:card content=summary_large_image /><meta data-rh=true property=og:url content=https://gracefullight.dev/2025/09/04/vlatest-review/ /><meta data-rh=true property=og:locale content=ko /><meta data-rh=true property=og:locale:alternate content=en /><meta data-rh=true name=docusaurus_locale content=ko /><meta data-rh=true name=docusaurus_tag content=default /><meta data-rh=true name=docsearch:language content=ko /><meta data-rh=true name=docsearch:docusaurus_tag content=default /><meta data-rh=true content=gcY9SiftHgQoJjBZ7IgwNNN5_atLPAX6kWb1nFVfa6E name=google-site-verification /><meta data-rh=true content=65AD1E28C0D057CEB3C68FBC0293E55B name=msvalidate.01 /><meta data-rh=true content=d024c2837887f72dc7b3792b958be74d69ba9593 name=naver-site-verification /><meta data-rh=true content=f7c93483a6f87c79 name=yandex-verification /><meta data-rh=true content=yZEdU1ABcR name=baidu-site-verification /><meta data-rh=true content=uelupjqqsm5egzlhy1aev2rfxow5yt name=facebook-domain-verification /><meta data-rh=true property=og:title content="VLA Test Review | gracefullight.dev"/><meta data-rh=true name=description content="VLA Test Review"/><meta data-rh=true property=og:description content="VLA Test Review"/><meta data-rh=true property=og:type content=article /><meta data-rh=true property=article:published_time content=2025-09-03T14:38:22.388Z /><meta data-rh=true property=article:author content=https://github.com/gracefullight /><meta data-rh=true property=article:tag content=vlm /><link data-rh=true rel=icon href=/img/favicon.ico /><link data-rh=true rel=canonical href=https://gracefullight.dev/2025/09/04/vlatest-review/ /><link data-rh=true rel=alternate href=https://gracefullight.dev/2025/09/04/vlatest-review/ hreflang=ko /><link data-rh=true rel=alternate href=https://gracefullight.dev/en/2025/09/04/vlatest-review/ hreflang=en /><link data-rh=true rel=alternate href=https://gracefullight.dev/2025/09/04/vlatest-review/ hreflang=x-default /><link data-rh=true rel=preconnect href=https://RFS69RSYOJ-dsn.algolia.net crossorigin=anonymous /><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@id":"https://gracefullight.dev/2025/09/04/vlatest-review","@type":"BlogPosting","author":{"@type":"Person","description":"Owner","image":"https://avatars.githubusercontent.com/u/11773683?v=4","name":"Eunkwang Shin","url":"https://github.com/gracefullight"},"datePublished":"2025-09-03T14:38:22.388Z","description":"VLA Test Review","headline":"VLA Test Review","isPartOf":{"@id":"https://gracefullight.dev/","@type":"Blog","name":"Blog"},"keywords":[],"mainEntityOfPage":"https://gracefullight.dev/2025/09/04/vlatest-review","name":"VLA Test Review","url":"https://gracefullight.dev/2025/09/04/vlatest-review"}</script><link rel=alternate type=application/rss+xml href=/rss.xml title="gracefullight.dev RSS Feed"><link rel=alternate type=application/atom+xml href=/atom.xml title="gracefullight.dev Atom Feed"><link rel=alternate type=application/json href=/feed.json title="gracefullight.dev JSON Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-E99DNE7S05"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-E99DNE7S05",{})</script><link rel=search type=application/opensearchdescription+xml title=gracefullight.dev href=/opensearch.xml><link href=/img/favicon-32x32.png rel=icon><link href=/manifest.json rel=manifest><meta content=#f28913 name=theme-color><meta content=yes name=mobile-web-app-capable><meta content=#f28913 name=apple-mobile-web-app-status-bar-style><link href=/img/apple-touch-icon.png rel=apple-touch-icon><link rel=preconnect href=https://pagead2.googlesyndication.com><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3004788392777865" async crossorigin=anonymous></script><link rel=preconnect href=https://www.clarity.ms><script>!function(t,e,n,a,c,i,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(i=e.createElement(a)).async=1,i.src="https://www.clarity.ms/tag/"+c,(r=e.getElementsByTagName(a)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script","aongv9xgi6")</script><link rel=preconnect href=https://wcs.naver.net><script src=https://wcs.naver.net/wcslog.js async></script><script>if(!wcs_add)var wcs_add={};wcs_add.wa="156bc73a81e3bd0",window.wcs&&wcs_do()</script><link rel=preconnect href=https://cdn.channel.io><script>!function(){var n=window;if(n.ChannelIO)return(window.console.error||window.console.log||function(){})("ChannelIO script included twice.");var e=function(){e.c(arguments)};function t(){if(!n.ChannelIOInitialized){n.ChannelIOInitialized=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cdn.channel.io/plugin/ch-plugin-web.js",e.charset="UTF-8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}}e.q=[],e.c=function(n){e.q.push(n)},n.ChannelIO=e,"complete"===document.readyState?t():window.attachEvent?window.attachEvent("onload",t):(window.addEventListener("DOMContentLoaded",t,!1),window.addEventListener("load",t,!1))}(),ChannelIO("boot",{pluginKey:"0fd130ba-a1a6-4b7e-802a-e82a885a7fd8"})</script><link rel=preconnect href=https://static.cloudflareinsights.com><script src=https://static.cloudflareinsights.com/beacon.min.js defer data-cf-beacon='{"token":"c0899829e72b45e98dff77241127252c"}'></script><link href=https://mc.yandex.ru rel=preconnect><script>!function(e,t,c,n,r,a,s){e[r]=e[r]||function(){(e[r].a=e[r].a||[]).push(arguments)},e[r].l=+new Date;for(var i=0;i<document.scripts.length;i++)if(document.scripts[i].src===n)return;a=t.createElement(c),s=t.getElementsByTagName(c)[0],a.async=1,a.src=n,s.parentNode.insertBefore(a,s)}(window,document,"script","https://mc.yandex.ru/metrika/tag.js?id=104072655","ym"),ym(0x63405cf,"init",{ssr:!0,clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!1})</script><link href=https://cdn.jsdelivr.net rel=preconnect><script type=application/ld+json>{"@context":"http://schema.org","@type":"Person","email":"mailto:gracefullight.dev@gmail.com","image":"https://avatars.githubusercontent.com/u/11773683?v=4","jobTitle":"FullStack JavaScript Developer","logo":"https://gracefullight.dev/img/apple-touch-icon.png","name":"Eunkwang Shin","nationality":"Korean","sameAs":["https://github.com/gracefullight","https://linkedin.com/in/gracefullight"],"url":"https://gracefullight.dev"}</script><link rel=stylesheet crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css type=text/css><link rel=stylesheet href=/assets/css/styles.b00a13cc.css /><script src=/assets/js/runtime~main.b5e932f5.js defer></script><script src=/assets/js/main.9388732d.js defer></script></head><body class=navigation-with-keyboard><svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/img/favicon-32x32.png /><link rel=preload as=image href="https://avatars.githubusercontent.com/u/11773683?v=4"/><div role=region aria-label="본문으로 건너뛰기"><a class=skipToContent_OqUV href=#__docusaurus_skipToContent_fallback>본문으로 건너뛰기</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><div class=navbar__logo><img src=/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_leTS themedComponent--light_FZCI"/><img src=/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_leTS themedComponent--dark_BlTQ"/></div><b class="navbar__title text--truncate">gracefullight.dev</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_PGu2><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>한국어</a><ul class=dropdown__menu><li><a href=/2025/09/04/vlatest-review/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=ko>한국어</a><li><a href=/en/2025/09/04/vlatest-review/ target=_self rel="noopener noreferrer" class=dropdown__link lang=en>English</a></ul></div><a class="navbar__item navbar__link" href=/archive/>Archives</a><a class="navbar__item navbar__link" href=/tags/>Tags</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_ahFW colorModeToggle_FNLe"><button class="clean-btn toggleButton_lGUf toggleButtonDisabled_s8P8" type=button disabled title="system mode" aria-label="어두운 모드와 밝은 모드 전환하기 (현재 system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_BxIe lightToggleIcon_rxM3"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_BxIe darkToggleIcon_QmJt"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_BxIe systemToggleIcon_M5r8"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_H3dU><button type=button class="DocSearch DocSearch-Button" aria-label="검색 (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>검색</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_drMG"><div class="container margin-vert--lg"><div class=row><aside class="col col--3"><nav class="sidebar_iDDe thin-scrollbar" aria-label="최근 블로그 문서 둘러보기"><div class="sidebarItemTitle_wrtL margin-bottom--md">최근 포스트</div><div role=group><h3 class=yearGroupHeading_Hgwk>2025</h3><ul class="sidebarItemList_ioQQ clean-list"><li class=sidebarItem_ZWzP><a class=sidebarItemLink_gh84 href=/2025/10/13/fundamentals-of-software-development-009/>FSD +009</a><li class=sidebarItem_ZWzP><a class=sidebarItemLink_gh84 href=/2025/10/12/introduction-to-ai-010/>IAI +010</a><li class=sidebarItem_ZWzP><a class=sidebarItemLink_gh84 href=/vocab/vocab-ai-011/>Vocabulary for AI 011</a><li class=sidebarItem_ZWzP><a class=sidebarItemLink_gh84 href=/2025/10/09/introduction-to-ai-009/>IAI +010</a><li class=sidebarItem_ZWzP><a class=sidebarItemLink_gh84 href=/2025/10/08/fundamentals-of-data-analytics-010/>FDA +010</a><li class=sidebarItem_ZWzP><a class=sidebarItemLink_gh84 href=/vocab/vocab-ai-010/>Vocabulary for AI 010</a><li class=sidebarItem_ZWzP><a class=sidebarItemLink_gh84 href=/vocab/vocab-ai-009/>Vocabulary for AI 009</a><li class=sidebarItem_ZWzP><a class=sidebarItemLink_gh84 href=/2025/09/22/introduction-to-ai-008/>IAI +008</a><li class=sidebarItem_ZWzP><a class=sidebarItemLink_gh84 href=/2025/09/21/fundamentals-of-data-analytics-009/>FDA +009</a><li class=sidebarItem_ZWzP><a class=sidebarItemLink_gh84 href=/2025/09/17/fundamentals-of-data-analytics-008/>FDA +008</a></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class=title_oc8d>VLA Test Review</h1><div class="container_OrIy margin-vert--md"><time datetime=2025-09-03T14:38:22.388Z>2025년 9월 3일</time> · <!-- -->약 6분</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_wGk4"><div class="avatar margin-bottom--sm"><a class=avatar__photo-link href=/authors/me/><img class="avatar__photo authorImage_vBZr" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt="Eunkwang Shin"/></a><div class="avatar__intro authorDetails_rwrS"><div class=avatar__name><a href=/authors/me/><span class=authorName_D1HT translate=no>Eunkwang Shin</span></a></div><small class=authorTitle_Ydry title=Owner>Owner</small><div class=authorSocials_Ed3g><a href=https://www.linkedin.com/in/gracefullight/ target=_blank rel="noopener noreferrer" class=authorSocialLink_a2Xd title=LinkedIn><svg xmlns=http://www.w3.org/2000/svg width=1em height=1em preserveAspectRatio=xMidYMid viewBox="0 0 256 256" style=--dark:#0a66c2;--light:#ffffffe6 class="authorSocialIcon_YNer linkedinSvg_PleH"><path d="M218.123 218.127h-37.931v-59.403c0-14.165-.253-32.4-19.728-32.4-19.756 0-22.779 15.434-22.779 31.369v60.43h-37.93V95.967h36.413v16.694h.51a39.907 39.907 0 0 1 35.928-19.733c38.445 0 45.533 25.288 45.533 58.186l-.016 67.013ZM56.955 79.27c-12.157.002-22.014-9.852-22.016-22.009-.002-12.157 9.851-22.014 22.008-22.016 12.157-.003 22.014 9.851 22.016 22.008A22.013 22.013 0 0 1 56.955 79.27m18.966 138.858H37.95V95.967h37.97v122.16ZM237.033.018H18.89C8.58-.098.125 8.161-.001 18.471v219.053c.122 10.315 8.576 18.582 18.89 18.474h218.144c10.336.128 18.823-8.139 18.966-18.474V18.454c-.147-10.33-8.635-18.588-18.966-18.453"/></svg></a><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=authorSocialLink_a2Xd title=GitHub><svg xmlns=http://www.w3.org/2000/svg width=1em height=1em viewBox="0 0 256 250" preserveAspectRatio=xMidYMid style=--dark:#000;--light:#fff class="authorSocialIcon_YNer githubSvg_Njx3"><path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46 6.397 1.185 8.746-2.777 8.746-6.158 0-3.052-.12-13.135-.174-23.83-35.61 7.742-43.124-15.103-43.124-15.103-5.823-14.795-14.213-18.73-14.213-18.73-11.613-7.944.876-7.78.876-7.78 12.853.902 19.621 13.19 19.621 13.19 11.417 19.568 29.945 13.911 37.249 10.64 1.149-8.272 4.466-13.92 8.127-17.116-28.431-3.236-58.318-14.212-58.318-63.258 0-13.975 5-25.394 13.188-34.358-1.329-3.224-5.71-16.242 1.24-33.874 0 0 10.749-3.44 35.21 13.121 10.21-2.836 21.16-4.258 32.038-4.307 10.878.049 21.837 1.47 32.066 4.307 24.431-16.56 35.165-13.12 35.165-13.12 6.967 17.63 2.584 30.65 1.255 33.873 8.207 8.964 13.173 20.383 13.173 34.358 0 49.163-29.944 59.988-58.447 63.157 4.591 3.972 8.682 11.762 8.682 23.704 0 17.126-.148 30.91-.148 35.126 0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002 256 57.307 198.691 0 128.001 0Zm-80.06 182.34c-.282.636-1.283.827-2.194.39-.929-.417-1.45-1.284-1.15-1.922.276-.655 1.279-.838 2.205-.399.93.418 1.46 1.293 1.139 1.931Zm6.296 5.618c-.61.566-1.804.303-2.614-.591-.837-.892-.994-2.086-.375-2.66.63-.566 1.787-.301 2.626.591.838.903 1 2.088.363 2.66Zm4.32 7.188c-.785.545-2.067.034-2.86-1.104-.784-1.138-.784-2.503.017-3.05.795-.547 2.058-.055 2.861 1.075.782 1.157.782 2.522-.019 3.08Zm7.304 8.325c-.701.774-2.196.566-3.29-.49-1.119-1.032-1.43-2.496-.726-3.27.71-.776 2.213-.558 3.315.49 1.11 1.03 1.45 2.505.701 3.27Zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033-1.448-.439-2.395-1.613-2.103-2.626.301-1.01 1.747-1.484 3.207-1.028 1.446.436 2.396 1.602 2.095 2.622Zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95-1.53.034-2.769-.82-2.786-1.86 0-1.065 1.202-1.932 2.733-1.958 1.522-.03 2.768.818 2.768 1.868Zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37-1.485.271-2.861-.365-3.05-1.386-.184-1.056.893-2.114 2.376-2.387 1.514-.263 2.868.356 3.061 1.403Z"/></svg></a><a href=mailto:gracefullight.dev@gmail.com target=_blank rel="noopener noreferrer" class=authorSocialLink_a2Xd title=Email><svg xmlns=http://www.w3.org/2000/svg viewBox="0 0 24 24" fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 class=authorSocialIcon_YNer><path stroke=none d="M0 0h24v24H0z"/><path d="M7.2 12a4.8 4.8 0 1 0 9.6 0 4.8 4.8 0 1 0-9.6 0"/><path d="M16.8 12v1.8a3 3 0 0 0 6 0V12a10.8 10.8 0 1 0-6.6 9.936"/></svg></a></div></div></div></div></div></header><div id=__blog-post-container class=markdown><h2 class="anchor anchorWithStickyNavbar_GkGg" id=vlatest-testing-and-evaluating-vision-language-action--models-for-robotic-manipulation>VLATest: Testing and Evaluating Vision-Language-Action  Models for Robotic Manipulation<a href=#vlatest-testing-and-evaluating-vision-language-action--models-for-robotic-manipulation class=hash-link aria-label="VLATest: Testing and Evaluating Vision-Language-Action  Models for Robotic Manipulation에 대한 직접 링크" title="VLATest: Testing and Evaluating Vision-Language-Action  Models for Robotic Manipulation에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li>VLATest fuzzes 18,604 manipulation scenes (10 operators, 4 tasks) to systematically stress-test VLA robustness.</li>
<li>Seven VLA models show low success and brittleness to confounders, lighting/camera changes, unseen objects, and instruction mutations; larger pretraining helps.</li>
<li>Priorities: scale/augment demo data (incl. sim2real), use stepwise/CoT prompting & multi-agent setups, and expand benchmarks with online risk assessment.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=motivation--gap>Motivation & Gap<a href=#motivation--gap class=hash-link aria-label="Motivation & Gap에 대한 직접 링크" title="Motivation & Gap에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>Problem:</strong> Current VLA models are typically evaluated on <strong>small, hand-crafted scenes</strong>, leaving <strong>general performance and robustness</strong> in diverse scenarios underexplored.</li>
<li><strong>Goal:</strong> Introduce <strong>VLATest</strong>, a <strong>generation-based fuzzing framework</strong> that automatically creates robotic manipulation scenes to <strong>test performance and robustness</strong> of VLA models.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=what-are-vla-models>What Are VLA Models?<a href=#what-are-vla-models class=hash-link aria-label="What Are VLA Models?에 대한 직접 링크" title="What Are VLA Models?에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>Vision-Language-Action (VLA)</strong> models take <strong>natural language instructions</strong> + <strong>camera images</strong> and output <strong>low-level robot actions</strong> (Δx, Δθ, Δgrip).</li>
<li><strong>Inference loop:</strong> Tokenize text/image → transformer predicts action token <strong>A₁</strong> → execute → append <strong>A₁</strong> + new image tokens <strong>I₂</strong> → predict <strong>A₂</strong> → … until success or step limit.</li>
</ul>
<p><img decoding=async loading=lazy alt="VLA Architecture" src=/assets/images/vla-architecture-6c70ccb1f0cb43def096bc28e5b0abf9.png width=2022 height=514 class=img_tzfx /></p>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=training--evaluation>Training & Evaluation<a href=#training--evaluation class=hash-link aria-label="Training & Evaluation에 대한 직접 링크" title="Training & Evaluation에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>Training:</strong> (1) Train from scratch on robot demonstrations, or (2) <strong>fine-tune a large VLM</strong> (e.g., Llava) with <code>></code>1B params pretraining.</li>
<li><strong>Evaluation:</strong> Task-specific metrics (e.g., <strong>grasp</strong>, <strong>lift</strong>, <strong>hold</strong> for “pick up”), either in <strong>sim</strong> (auto-metrics) or <strong>real</strong> (manual labels).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=vlatest-framework>VLATest Framework<a href=#vlatest-framework class=hash-link aria-label="VLATest Framework에 대한 직접 링크" title="VLATest Framework에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>Ten testing operators</strong> grouped across:<!-- -->
<ul>
<li><strong>Target objects:</strong> type, position, orientation</li>
<li><strong>Confounding objects:</strong> type, position, orientation, <strong>count</strong></li>
<li><strong>Lighting:</strong> <strong>intensity</strong></li>
<li><strong>Camera:</strong> <strong>position</strong>, <strong>orientation</strong></li>
</ul>
</li>
<li><strong>Scene generation (Alg. 1):</strong> sample valid targets → (optional) confounders → mutate lighting (factor <strong>α</strong>) → mutate camera pose (<strong>d</strong>, <strong>θ</strong>). Semantic validity checks prevent infeasible scenes.</li>
</ul>
<p><img decoding=async loading=lazy alt="VLA Test" src=/assets/images/vla-test-7989c6573c1574d8dfc5e67ca632ca0d.png width=1926 height=340 class=img_tzfx /></p>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=research-questions-rq>Research Questions (RQ)<a href=#research-questions-rq class=hash-link aria-label="Research Questions (RQ)에 대한 직접 링크" title="Research Questions (RQ)에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>RQ1:</strong> Basic performance on popular manipulation tasks</li>
<li><strong>RQ2:</strong> Effect of <strong>confounding object count</strong></li>
<li><strong>RQ3:</strong> Effect of <strong>lighting changes</strong></li>
<li><strong>RQ4:</strong> Effect of <strong>camera pose changes</strong></li>
<li><strong>RQ5:</strong> Robustness to <strong>unseen objects</strong> (OOD)</li>
<li><strong>RQ6:</strong> Robustness to <strong>instruction mutations</strong></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=tasks--prompting>Tasks & Prompting<a href=#tasks--prompting class=hash-link aria-label="Tasks & Prompting에 대한 직접 링크" title="Tasks & Prompting에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>Tasks:</strong>
<ol>
<li><strong>Pick up</strong> an object (grasp + lift ≥0.02 m for 5 frames)</li>
<li><strong>Move A near B</strong> (≤0.05 m)</li>
<li><strong>Put A on B</strong> (stable stacking)</li>
<li><strong>Put A into B</strong> (fully inside)</li>
</ol>
</li>
<li><strong>Standard prompts (RQ1–RQ5):</strong>
<ul>
<li><code>pick up [obj]</code> · <code>move [objA] near [objB]</code> · <code>put [objA] on [objB]</code> · <code>put [objA] into [objB]</code></li>
</ul>
</li>
<li><strong>Instruction mutations (RQ6):</strong> 10 paraphrases per task (GPT-4o), manually validated for semantic equivalence.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=experimental-setup>Experimental Setup<a href=#experimental-setup class=hash-link aria-label="Experimental Setup에 대한 직접 링크" title="Experimental Setup에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>Scenes:</strong> <strong>18,604</strong> across 4 tasks (ManiSkill2).</li>
<li><strong>Models:</strong> 7 public VLAs (RT-1-1k/58k/400k, RT-1-X, Octo-small/base, OpenVLA-7b).</li>
<li><strong>Compute:</strong> <code>></code><strong>580 GPU hours</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=key-results--findings>Key Results & Findings<a href=#key-results--findings class=hash-link aria-label="Key Results & Findings에 대한 직접 링크" title="Key Results & Findings에 대한 직접 링크" translate=no>​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_GkGg" id=rq1--overall-performance>RQ1 — Overall Performance<a href=#rq1--overall-performance class=hash-link aria-label="RQ1 — Overall Performance에 대한 직접 링크" title="RQ1 — Overall Performance에 대한 직접 링크" translate=no>​</a></h3>
<ul>
<li>VLA models <strong>underperform</strong> overall; no single model dominates across tasks.</li>
<li>Example best-case rates (default settings): <strong>34.4%</strong> (Task1, RT-1-400k), <strong>12.7%</strong> (Task2, OpenVLA-7b), <strong>2.2%</strong> (Task3, RT-1-X), <strong>2.1%</strong> (Task4, Octo-small).</li>
<li><strong>Stepwise breakdown (Task 1):</strong> grasp <strong>23.3%</strong> → lift <strong>15.7%</strong> → hold <strong>12.4%</strong> ⇒ difficulty <strong>composing sequential actions</strong>.<!-- -->
<ul>
<li><strong>Implication (Finding 2):</strong> Consider <strong>stepwise prompting / chain-of-thought</strong> to decompose complex tasks.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_GkGg" id=rq1--coverage-metric>RQ1 — Coverage Metric<a href=#rq1--coverage-metric class=hash-link aria-label="RQ1 — Coverage Metric에 대한 직접 링크" title="RQ1 — Coverage Metric에 대한 직접 링크" translate=no>​</a></h3>
<ul>
<li>No established coverage for VLA; adopted <strong>trajectory coverage</strong> (pragmatic).</li>
<li>Increasing cases from <strong>n=10</strong> to <strong>n=1000</strong> achieved <strong>100%</strong> coverage across tasks (object-position novelty relative to workspace).</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_GkGg" id=rq2--confounding-objects>RQ2 — Confounding Objects<a href=#rq2--confounding-objects class=hash-link aria-label="RQ2 — Confounding Objects에 대한 직접 링크" title="RQ2 — Confounding Objects에 대한 직접 링크" translate=no>​</a></h3>
<ul>
<li><strong>More confounders ⇒ worse performance</strong>; models struggle to <strong>locate the correct object</strong>.</li>
<li><strong>Similarity doesn’t matter much:</strong> Mann–Whitney U shows <strong>no significant difference</strong> between <strong>similar</strong> vs <strong>dissimilar</strong> distractors (p = 0.443, 0.614, 0.657, 0.443; effect sizes ≈ 0.23–0.29).</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_GkGg" id=rq3--lighting-robustness>RQ3 — Lighting Robustness<a href=#rq3--lighting-robustness class=hash-link aria-label="RQ3 — Lighting Robustness에 대한 직접 링크" title="RQ3 — Lighting Robustness에 대한 직접 링크" translate=no>​</a></h3>
<ul>
<li><strong>Lighting perturbations significantly hurt performance.</strong></li>
<li><strong>OpenVLA-7b</strong> most robust (<strong>77.9%</strong> of previously passed cases still pass), plausibly due to <strong>SigLIP + DINOv2</strong> pretraining and LLaVA 1.5 mixture.</li>
<li><strong>Sensitivity:</strong> even <strong>α <code>&lt;</code> 2.5</strong> increase drops success to ~<strong>0.7×</strong>; <strong>α <code>></code> 8</strong> ⇒ ~<strong>40%</strong> of default-pass scenes succeed.</li>
<li><strong>Decreasing</strong> light hurts <strong>less</strong> than increasing; <strong>α <code>&lt;</code> 0.2</strong> still ~<strong>60%</strong> pass.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_GkGg" id=rq4--camera-pose-robustness>RQ4 — Camera Pose Robustness<a href=#rq4--camera-pose-robustness class=hash-link aria-label="RQ4 — Camera Pose Robustness에 대한 직접 링크" title="RQ4 — Camera Pose Robustness에 대한 직접 링크" translate=no>​</a></h3>
<ul>
<li>Small pose changes (≤<strong>5°</strong> rotation, ≤<strong>5 cm</strong> shift) reduce success to <strong>34.0%</strong> of default.</li>
<li><strong>RT-1-400k</strong> most robust (<strong>45.6%</strong> retain), <strong>OpenVLA-7b</strong> at <strong>31.3%</strong>; <strong>Octo</strong> models <code>&lt;</code><strong>10%</strong>.<!-- -->
<ul>
<li>Likely due to <strong>training data scale</strong> differences.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_GkGg" id=rq5--unseen-objects>RQ5 — Unseen Objects<a href=#rq5--unseen-objects class=hash-link aria-label="RQ5 — Unseen Objects에 대한 직접 링크" title="RQ5 — Unseen Objects에 대한 직접 링크" translate=no>​</a></h3>
<ul>
<li>Using <strong>YCB (56 unseen objects)</strong> leads to large performance drops versus seen objects: avg <strong>–74.2%</strong>, <strong>–66.7%</strong>, <strong>–66.7%</strong>, <strong>–20.0%</strong> on Tasks 1–4.</li>
<li><strong>Transfer rate</strong> across steps:<!-- -->
<ul>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mstyle scriptlevel=0 displaystyle=true><msubsup><mi>T</mi><mi>r</mi><mi>n</mi></msubsup><mo>=</mo><mfrac><msub><mtext>Success rate</mtext><mi>n</mi></msub><msub><mtext>Success rate</mtext><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mfrac></mstyle></mrow><annotation encoding=application/x-tex>\displaystyle T_r^n = \frac{\text{Success rate}_n}{\text{Success rate}_{n-1}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9614em;vertical-align:-0.247em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em>T</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.7144em><span style=top:-2.453em;margin-left:-0.1389em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>r</span></span></span><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.2547em;vertical-align:-0.8943em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.3603em><span style=top:-2.314em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord text"><span class=mord>Success rate</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2083em><span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:0.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord text"><span class=mord>Success rate</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.8943em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>, with <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mtext>Success rate</mtext><mn>0</mn></msub><mo>=</mo><mn>100</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>\text{Success rate}_0 = 100\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord text"><span class=mord>Success rate</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em></span><span class=mord>100%</span></span></span></span></li>
<li>Paired t-tests show significant differences on <strong><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msubsup><mi>T</mi><mi>r</mi><mn>1</mn></msubsup></mrow><annotation encoding=application/x-tex>T_r^1</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0611em;vertical-align:-0.247em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em>T</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-2.453em;margin-left:-0.1389em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>r</span></span></span><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span></span></span></span></strong> for <strong>Task 1 & 2</strong> (p = 0.011, 0.007; Cohen’s d = 1.34, 0.891).</li>
<li><strong>Primary failure mode:</strong> <strong>recognizing/locating unseen objects</strong>.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_GkGg" id=rq6--instruction-mutations>RQ6 — Instruction Mutations<a href=#rq6--instruction-mutations class=hash-link aria-label="RQ6 — Instruction Mutations에 대한 직접 링크" title="RQ6 — Instruction Mutations에 대한 직접 링크" translate=no>​</a></h3>
<ul>
<li>Mutated instructions generally <strong>reduce performance</strong> (avg drops: <strong>–32.8%</strong> T1, <strong>–1.7%</strong> T2, <strong>–8.3%</strong> T3; negligible on T4).</li>
<li><strong>Larger language backbones help:</strong> <strong>OpenVLA-7b (Llama 2-7B)</strong> is <strong>more robust</strong>, sometimes <strong>improving</strong> under mutations (e.g., T1, T4).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=implications--directions>Implications & Directions<a href=#implications--directions class=hash-link aria-label="Implications & Directions에 대한 직접 링크" title="Implications & Directions에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>Scale matters:</strong> larger <strong>pretraining</strong> and <strong>robot-demo datasets</strong> improve robustness (lighting/camera).</li>
<li><strong>Data enrichment:</strong> use <strong>data augmentation</strong> and <strong>sim-to-real</strong> to diversify external factors; leverage <strong>traditional controllers</strong> to auto-generate demonstrations.</li>
<li><strong>Prompting strategies:</strong> adopt <strong>stepwise/CoT prompting</strong>; consider <strong>multi-agent</strong> decompositions.</li>
<li><strong>Benchmarking:</strong> the <strong>18,604</strong> VLATest scenes serve as an <strong>early benchmark</strong>; expand to more tasks/robots/conditions.</li>
<li><strong>Online risk assessment:</strong> explore <strong>uncertainty estimation</strong> and <strong>safety monitoring</strong> for runtime quality control.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=related-work>Related Work<a href=#related-work class=hash-link aria-label="Related Work에 대한 직접 링크" title="Related Work에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>Robotics foundation models:</strong> (1) LLMs for planning/rewards; (2) <strong>Multi-modal</strong> FMs (VLMs/VLAs) for manipulation & perception.</li>
<li><strong>CPS testing:</strong> gray-box/black-box fuzzing and search-based testing exist, but <strong>not directly applicable</strong> to VLAs (multimodality, autoregression, scale).</li>
<li><strong>FM evaluation:</strong> beyond static benchmarks, VLATest <strong>dynamically generates</strong> 3D manipulation test cases—distinct from <strong>text-only</strong> testing.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=threats-to-validity-mitigations-in-study>Threats to Validity (mitigations in study)<a href=#threats-to-validity-mitigations-in-study class=hash-link aria-label="Threats to Validity (mitigations in study)에 대한 직접 링크" title="Threats to Validity (mitigations in study)에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>Internal:</strong> randomness (mitigated by <strong>18,604</strong> scenes); potential prompt bias (mutations <strong>manually validated</strong>).</li>
<li><strong>External:</strong> generalization to other tasks/models; chose <strong>popular tasks</strong> (Open X-Embodiment) and <strong>SOTA public models</strong>.</li>
<li><strong>Construct:</strong> limited operators (lighting/camera/confounders chosen; future: #lights, camera intrinsics, resolution).<!-- -->
<ul>
<li>Coverage: <strong>trajectory coverage</strong> used as a pragmatic proxy.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Conclusion에 대한 직접 링크" title="Conclusion에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li><strong>VLATest</strong>: early, <strong>generation-based fuzzing</strong> framework (10 operators) for VLA testing in ManiSkill2.</li>
<li><strong>Empirical evidence</strong> across <strong>7 models / 4 tasks / 18,604 scenes</strong> shows <strong>limited robustness</strong> (lighting, camera, unseen objects, instruction variation).</li>
<li>Points to <strong>data scaling</strong>, <strong>prompting</strong>, <strong>benchmarking</strong>, and <strong>risk assessment</strong> as practical paths to <strong>more reliable</strong> VLA systems.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_GkGg" id=ref>Ref<a href=#ref class=hash-link aria-label="Ref에 대한 직접 링크" title="Ref에 대한 직접 링크" translate=no>​</a></h2>
<ul>
<li>Wang, Z., Zhou, Z., Song, J., Huang, Y., Shu, Z., & Ma, L. (2025). VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation. Proceedings of the ACM on Software Engineering, 2(FSE), 1615–1638.</li>
</ul></div><footer class=docusaurus-mt-lg><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class=col><b>태그:</b><ul class="tags_I9gl padding--none margin-left--sm"><li class=tag_gTs5><a rel=tag title="Vision-Language Models" class="tag_QmFV tagRegular_P3VH" href=/tags/vlm/>vlm</a></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="블로그 게시물 탐색"><a class="pagination-nav__link pagination-nav__link--prev" href=/2025/09/04/sentence-structures/><div class=pagination-nav__sublabel>이전 게시물</div><div class=pagination-nav__label>Sentence structures</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/2025/09/02/introduction-to-ai-005/><div class=pagination-nav__sublabel>다음 게시물</div><div class=pagination-nav__label>IAI +005</div></a></nav></main><div class="col col--2"><div class="tableOfContents_mx9C thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#vlatest-testing-and-evaluating-vision-language-action--models-for-robotic-manipulation class="table-of-contents__link toc-highlight">VLATest: Testing and Evaluating Vision-Language-Action  Models for Robotic Manipulation</a><li><a href=#motivation--gap class="table-of-contents__link toc-highlight">Motivation & Gap</a><li><a href=#what-are-vla-models class="table-of-contents__link toc-highlight">What Are VLA Models?</a><li><a href=#training--evaluation class="table-of-contents__link toc-highlight">Training & Evaluation</a><li><a href=#vlatest-framework class="table-of-contents__link toc-highlight">VLATest Framework</a><li><a href=#research-questions-rq class="table-of-contents__link toc-highlight">Research Questions (RQ)</a><li><a href=#tasks--prompting class="table-of-contents__link toc-highlight">Tasks & Prompting</a><li><a href=#experimental-setup class="table-of-contents__link toc-highlight">Experimental Setup</a><li><a href=#key-results--findings class="table-of-contents__link toc-highlight">Key Results & Findings</a><ul><li><a href=#rq1--overall-performance class="table-of-contents__link toc-highlight">RQ1 — Overall Performance</a><li><a href=#rq1--coverage-metric class="table-of-contents__link toc-highlight">RQ1 — Coverage Metric</a><li><a href=#rq2--confounding-objects class="table-of-contents__link toc-highlight">RQ2 — Confounding Objects</a><li><a href=#rq3--lighting-robustness class="table-of-contents__link toc-highlight">RQ3 — Lighting Robustness</a><li><a href=#rq4--camera-pose-robustness class="table-of-contents__link toc-highlight">RQ4 — Camera Pose Robustness</a><li><a href=#rq5--unseen-objects class="table-of-contents__link toc-highlight">RQ5 — Unseen Objects</a><li><a href=#rq6--instruction-mutations class="table-of-contents__link toc-highlight">RQ6 — Instruction Mutations</a></ul><li><a href=#implications--directions class="table-of-contents__link toc-highlight">Implications & Directions</a><li><a href=#related-work class="table-of-contents__link toc-highlight">Related Work</a><li><a href=#threats-to-validity-mitigations-in-study class="table-of-contents__link toc-highlight">Threats to Validity (mitigations in study)</a><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a><li><a href=#ref class="table-of-contents__link toc-highlight">Ref</a></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Support Me</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://www.buymeacoffee.com/LOUB2kN target=_blank rel="noopener noreferrer" style="cursor: pointer;">
                <img src=https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png style="height: auto !important;width: auto !important;">
              </a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Feeds</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://gracefullight.dev/rss.xml target=_blank rel="noopener noreferrer" class=footer__link-item>RSS<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_ZLQs><use href=#theme-svg-external-link /></svg></a><li class=footer__item><a href=https://gracefullight.dev/atom.xml target=_blank rel="noopener noreferrer" class=footer__link-item>Atom<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_ZLQs><use href=#theme-svg-external-link /></svg></a></ul></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2016-2025 Eunkwang Shin.</div></div></div></footer></div></body>