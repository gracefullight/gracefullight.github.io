"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["29078"],{53115(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var s=i(83658),t=i(74848),r=i(28453);let o={title:"Octo Review",date:new Date("2025-08-27T10:54:09.447Z"),description:"Octo, An Open-Source Generalist Robot Policy Review",authors:"me",tags:["vlm"]},l,a={authorsImageUrls:[void 0]},c=[{value:"Octo",id:"octo",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Prior GRPs &amp; Gaps",id:"prior-grps--gaps",level:2},{value:"Contribution (What is Octo?)",id:"contribution-what-is-octo",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Training Data &amp; Objective",id:"training-data--objective",level:2},{value:"Experiments",id:"experiments",level:2},{value:"Results",id:"results",level:2},{value:"Limitations / Future Work",id:"limitations--future-work",level:2},{value:"One-line Takeaway",id:"one-line-takeaway",level:2},{value:"Ref",id:"ref",level:2}];function d(e){let n={br:"br",code:"code",em:"em",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"octo",children:"Octo"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Octo is a transformer-based policy with modular tokenizers (language via T5, images via CNN patches), blockwise masking, and readout tokens, trained on 800k multi-robot trajectories."}),"\n",(0,t.jsx)(n.li,{children:"Actions are generated through a diffusion head that produces continuous, multimodal, chunked predictions, enabling precise control and broad generalization."}),"\n",(0,t.jsx)(n.li,{children:"It achieves state-of-the-art zero-shot performance across 7 robots and allows efficient finetuning to new sensors and action spaces, while being fully open-source."}),"\n"]}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Category"}),(0,t.jsx)(n.th,{children:"Simple Analogy"}),(0,t.jsx)(n.th,{children:"Actual Tokenization"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Language"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"[Sentence]"})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"[l\u2081, l\u2082, l\u2083, \u2026]"})," ",(0,t.jsx)("br",{}),"\u2192 multiple tokens from a tokenized sentence"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Goal Image"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"[Goal]"})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"[g\u2081, g\u2082, g\u2083, \u2026]"})," ",(0,t.jsx)("br",{}),"\u2192 image split into patches"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Observation (time t)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"[Observation]"})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"[o\u209C\xb9, o\u209C\xb2, o\u209C\xb3, \u2026]"})," ",(0,t.jsx)("br",{}),"\u2192 camera frames/sensors tokenized into patches"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Readout Token"})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"[ ]"})," (empty slot)"]}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"[TR,t]"})," ",(0,t.jsx)("br",{}),"\u2192 one per timestep, reserved for predicting actions"]})]})]})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"Time t-1: [l] [g] [o_{t-1}] [TR,t-1]\nTime t:   [l] [g] [o_t]     [TR,t]\nTime t+1: [l] [g] [o_{t+1}] [TR,t+1]\n\n[TR,t-1], [TR,t], [TR,t+1]  \u2500\u2500\u25BA  Diffusion head  \u2500\u2500\u25BA  [a_t, a_{t+1}, \u2026]\n"})}),"\n",(0,t.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Traditional robot learning trains policies ",(0,t.jsx)(n.strong,{children:"from scratch"})," on robot/task-specific datasets \u2192 costly data collection, narrow generalization."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Generalist Robot Policies (GRPs)"})," pretrained on diverse robots/tasks can be ",(0,t.jsx)(n.strong,{children:"finetuned with little in-domain data"})," while generalizing broadly."]}),"\n",(0,t.jsxs)(n.li,{children:["Real-world deployments face challenges across ",(0,t.jsx)(n.strong,{children:"robot embodiments, sensor setups, action spaces, task specs, and environments"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prior-grps--gaps",children:"Prior GRPs & Gaps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["GRPs aim for ",(0,t.jsx)(n.strong,{children:"low-level visuomotor control"})," across tasks, environments, and robotic systems."]}),"\n",(0,t.jsxs)(n.li,{children:["Existing models often have ",(0,t.jsx)(n.strong,{children:"restricted inputs (e.g., a single camera)"}),", ",(0,t.jsx)(n.strong,{children:"lack efficient finetuning to new domains"}),", and importantly, ",(0,t.jsx)(n.strong,{children:"largest models are not publicly available"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"contribution-what-is-octo",children:"Contribution (What is Octo?)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Octo"}),": a large transformer-based policy trained on ",(0,t.jsx)(n.strong,{children:"800k trajectories"})," from the Open X-Embodiment dataset."]}),"\n",(0,t.jsxs)(n.li,{children:["Accepts ",(0,t.jsx)(n.strong,{children:"language instructions or goal images"}),", and can be ",(0,t.jsx)(n.strong,{children:"finetuned within hours on consumer GPUs"})," to new sensors and action spaces."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"First GRP"})," to support ",(0,t.jsx)(n.strong,{children:"effective finetuning to new observations and actions"})," and to be ",(0,t.jsx)(n.strong,{children:"fully open-source"})," (training pipeline, checkpoints, data)."]}),"\n",(0,t.jsxs)(n.li,{children:["Novelty lies in combining: ",(0,t.jsx)(n.strong,{children:"transformer backbone + language/goal image conditioning + diffusion head"})," for expressive action distributions."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input tokenizers"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Language via pretrained ",(0,t.jsx)(n.strong,{children:"T5-base"})]}),"\n",(0,t.jsx)(n.li,{children:"Images via shallow CNN \u2192 patch tokens"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transformer backbone"}),": processes unified token sequence."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Blockwise masking + Readout tokens"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Nonexistent modalities are masked"}),"\n",(0,t.jsxs)(n.li,{children:["Readout tokens ",(0,t.jsx)(n.em,{children:"only attend"})," to past observations/tasks, not vice versa"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Diffusion action head"}),": predicts ",(0,t.jsx)(n.strong,{children:"continuous, multimodal, chunked actions"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Modularity"}),": new sensors/outputs can be added by only training lightweight encoders or heads; pretrained backbone remains unchanged."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Octo Architecture",src:i(33483).A+"",width:"803",height:"415"})}),"\n",(0,t.jsx)(n.h2,{id:"training-data--objective",children:"Training Data & Objective"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Mixture of ",(0,t.jsx)(n.strong,{children:"25 heterogeneous robot datasets"}),": diverse robots, sensors (with/without wrist cams), labels (with/without language)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conditional diffusion decoding"})," predicts continuous, multimodal action distributions.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Transformer runs ",(0,t.jsx)(n.strong,{children:"one forward pass"}),"; denoising steps are contained in the small diffusion head."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"experiments",children:"Experiments"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Evaluated on ",(0,t.jsx)(n.strong,{children:"7 robotic platforms across 4 institutions"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Key questions:","\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Zero-shot multi-robot control?"}),"\n",(0,t.jsx)(n.li,{children:"Do Octo weights improve finetuning vs. scratch or standard pretrained representations?"}),"\n",(0,t.jsx)(n.li,{children:"Which design choices matter for generalist robot policies?"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"results",children:"Results"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Achieves ",(0,t.jsx)(n.strong,{children:"state-of-the-art zero-shot multi-robot control"}),", competitive with RT-1-X and RT-2-X."]}),"\n",(0,t.jsxs)(n.li,{children:["Provides a ",(0,t.jsx)(n.strong,{children:"versatile policy initialization"}),": significantly outperforms baselines for ",(0,t.jsx)(n.strong,{children:"data-efficient finetuning"})," to new obs/action spaces."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"limitations--future-work",children:"Limitations / Future Work"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Needs ",(0,t.jsx)(n.strong,{children:"better language conditioning"}),", ",(0,t.jsx)(n.strong,{children:"improved wrist camera support"}),", and ",(0,t.jsx)(n.strong,{children:"data beyond optimal demonstrations"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"one-line-takeaway",children:"One-line Takeaway"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Octo = modular, efficient, open-source GRP"}),":",(0,t.jsx)(n.br,{}),"\n","A transformer + diffusion policy trained on large-scale multi-robot data that ",(0,t.jsx)(n.strong,{children:"adapts quickly with little in-domain data"})," to new sensors and action spaces, enabling broad generalization."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"ref",children:"Ref"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Mees, O., Ghosh, D., Pertsch, K., Black, K., Walke, H. R., Dasari, S., Hejna, J., Kreiman, T., Xu, C., & Luo, J. (2024). Octo: An open-source generalist robot policy. First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024."}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},33483(e,n,i){i.d(n,{A:()=>s});let s=i.p+"assets/images/octo-architecture-49b9dd94643695f0566e74ac5a0801bd.png"},28453(e,n,i){i.d(n,{R:()=>o,x:()=>l});var s=i(96540);let t={},r=s.createContext(t);function o(e){let n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}},83658(e){e.exports=JSON.parse('{"permalink":"/en/2025/08/27/octo-review","source":"@site/blog/2025/08/27/octo-review.md","title":"Octo Review","description":"Octo, An Open-Source Generalist Robot Policy Review","date":"2025-08-27T10:54:09.447Z","tags":[{"inline":false,"label":"vlm","permalink":"/en/tags/vlm","description":"Vision-Language Models"}],"readingTime":3.14,"hasTruncateMarker":false,"authors":[{"name":"Gracefullight","title":"Owner","url":"https://github.com/gracefullight","imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me","page":null}],"frontMatter":{"title":"Octo Review","date":"2025-08-27T10:54:09.447Z","description":"Octo, An Open-Source Generalist Robot Policy Review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"Logistic regression","permalink":"/en/2025/08/28/logistic-regression"},"nextItem":{"title":"IAI +004","permalink":"/en/2025/08/26/introduction-to-ai-004"}}')}}]);