"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["88088"],{91758:function(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>c});var t=i(9756),s=i(65813),r=i(661);let a={title:"RT-1, Robot Transformer 1 Review",date:new Date("2025-08-24T05:52:19.257Z"),description:"RT-1 review",authors:"me",tags:["vlm"]},o,l={authorsImageUrls:[void 0]},c=[{value:"RT-1",id:"rt-1",level:2},{value:"Introduction &amp; Motivation",id:"introduction--motivation",level:2},{value:"Model &amp; Architecture",id:"model--architecture",level:2},{value:"Experiments &amp; Results",id:"experiments--results",level:2},{value:"General Performance",id:"general-performance",level:3},{value:"Absorbing Simulation Data",id:"absorbing-simulation-data",level:3},{value:"Absorbing Multi-Robot Data",id:"absorbing-multi-robot-data",level:3},{value:"Long-Horizon Scenarios (SayCan Integration)",id:"long-horizon-scenarios-saycan-integration",level:3},{value:"Data Quantity vs Diversity",id:"data-quantity-vs-diversity",level:3},{value:"Conclusions &amp; Limitations",id:"conclusions--limitations",level:2},{value:"Future Directions",id:"future-directions",level:2},{value:"Ref",id:"ref",level:2}];function d(e){let n={h2:"h2",h3:"h3",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"rt-1",children:"RT-1"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'RT-1 discretizes robot actions into 256-bin tokens, creating a shared "action language" across robots.'}),"\n",(0,s.jsx)(n.li,{children:"It absorbs heterogeneous data from simulation and other robot morphologies without losing performance."}),"\n",(0,s.jsx)(n.li,{children:"It generalizes robustly to new tasks, environments, and long-horizon scenarios (up to 50 steps)."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"RT-1 Architecture",src:i(39331).A+"",width:"820",height:"1315"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction--motivation",children:"Introduction & Motivation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Leveraging large, diverse, task-agnostic datasets enables high performance in zero-shot or small task-specific settings."}),"\n",(0,s.jsx)(n.li,{children:'Data collection and curation is a critical bottleneck in robotics ("the unsung hero" of large-scale ML).'}),"\n",(0,s.jsx)(n.li,{children:"Transformer-based controllers are powerful but inefficient for real-time robotics, requiring architectural adaptations."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"model--architecture",children:"Model & Architecture"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"RT-1 architecture: EfficientNet + FiLM layers + TokenLearner for compact vision-language tokenization."}),"\n",(0,s.jsx)(n.li,{children:"Action tokenization: 11 action dimensions (7 arm, 3 base, 1 mode) discretized into 256 bins each."}),"\n",(0,s.jsx)(n.li,{children:'This abstraction converts continuous robot actions into a discrete "token language", enabling cross-domain and cross-robot transfer.'}),"\n",(0,s.jsx)(n.li,{children:"Real-time feasibility: optimized design achieves ~3Hz inference speed suitable for real-world control."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"experiments--results",children:"Experiments & Results"}),"\n",(0,s.jsx)(n.h3,{id:"general-performance",children:"General Performance"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["RT-1 executes over 700 unique instructions at ",(0,s.jsx)(n.strong,{children:"97% success rate"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["On unseen instructions: ",(0,s.jsx)(n.strong,{children:"76% success"}),", outperforming next-best baseline by +24%."]}),"\n",(0,s.jsx)(n.li,{children:"Robustness: 83% success with distractors, 59% with background changes (significantly higher than baselines)."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"absorbing-simulation-data",children:"Absorbing Simulation Data"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Adding sim data does not degrade real-task performance."}),"\n",(0,s.jsxs)(n.li,{children:["Objects/tasks only seen in simulation: performance boosted ",(0,s.jsx)(n.strong,{children:"23% \u21D2 87%"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Unseen instructions with sim objects: ",(0,s.jsx)(n.strong,{children:"7% \u21D2 33%"}),", showing strong sim-to-real domain transfer."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"absorbing-multi-robot-data",children:"Absorbing Multi-Robot Data"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Mixed RT-1 + Kuka datasets: only 2% drop in original tasks."}),"\n",(0,s.jsx)(n.li,{children:"Bin-picking eval: RT-1 only 22% \u21D2 mixed training 39% (almost 2\xd7)."}),"\n",(0,s.jsx)(n.li,{children:"Kuka-only training: 0% on EDR robots \u21D2 morphology transfer alone fails."}),"\n",(0,s.jsxs)(n.li,{children:["Mixed data enables RT-1 to ",(0,s.jsx)(n.strong,{children:"leverage cross-robot experiences"})," without explicit demonstrations."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"long-horizon-scenarios-saycan-integration",children:"Long-Horizon Scenarios (SayCan Integration)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Evaluated in two kitchens:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Kitchen1: 67% execution success."}),"\n",(0,s.jsx)(n.li,{children:"Kitchen2 (novel environment): also 67% execution success."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Outperforms Gato (0% in Kitchen2) and BC-Z (13% in Kitchen2)."}),"\n",(0,s.jsxs)(n.li,{children:["Demonstrated execution of ",(0,s.jsx)(n.strong,{children:"ultra-long tasks up to 50 steps"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"data-quantity-vs-diversity",children:"Data Quantity vs Diversity"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Data Diversity",src:i(83408).A+"",width:"733",height:"376"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reducing dataset size \u21D2 gradual performance/generalization decline."}),"\n",(0,s.jsxs)(n.li,{children:["Reducing task diversity \u21D2 ",(0,s.jsx)(n.strong,{children:"much sharper decline"}),", especially in generalization."]}),"\n",(0,s.jsxs)(n.li,{children:["Key takeaway: ",(0,s.jsx)(n.strong,{children:"Data diversity is more critical than data quantity."})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conclusions--limitations",children:"Conclusions & Limitations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"RT-1 proves large-scale data absorption and strong generalization in robotics."}),"\n",(0,s.jsxs)(n.li,{children:["Limitations:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Based on imitation learning \u21D2 cannot surpass demonstrator performance."}),"\n",(0,s.jsx)(n.li,{children:"Generalization limited to recombinations of known concepts \u21D2 fails on truly novel motions."}),"\n",(0,s.jsx)(n.li,{children:"Dataset is large but not dexterous (fine manipulation limited)."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Enable non-experts to collect training data and prompt models for faster skill scaling."}),"\n",(0,s.jsx)(n.li,{children:"Increase environmental diversity to strengthen robustness to backgrounds/environments."}),"\n",(0,s.jsx)(n.li,{children:"Improve reaction speed and context retention via scalable attention and memory."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"ref",children:"Ref"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., & Hsu, J. (2022). Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817."}),"\n"]})]})}function u(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},39331:function(e,n,i){i.d(n,{A:()=>t});let t=i.p+"assets/images/rt-1-architecture-adc4a381e6221b91a20467516cdd1752.png"},83408:function(e,n,i){i.d(n,{A:()=>t});let t=i.p+"assets/images/rt-1-data-diversity-299befbcb9d0f7eb7912b666b02988e1.png"},661:function(e,n,i){i.d(n,{R:()=>a,x:()=>o});var t=i(59729);let s={},r=t.createContext(s);function a(e){let n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}},9756:function(e){e.exports=JSON.parse('{"permalink":"/en/2025/08/24/rt-1-review","source":"@site/blog/2025/08/24/rt-1-review.md","title":"RT-1, Robot Transformer 1 Review","description":"RT-1 review","date":"2025-08-24T05:52:19.257Z","tags":[{"inline":false,"label":"vlm","permalink":"/en/tags/vlm","description":"Vision-Language Models"}],"readingTime":2.42,"hasTruncateMarker":false,"authors":[{"name":"Gracefullight","title":"Owner","url":"https://github.com/gracefullight","imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me","page":null}],"frontMatter":{"title":"RT-1, Robot Transformer 1 Review","date":"2025-08-24T05:52:19.257Z","description":"RT-1 review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"PaLM-E An Embodied Multimodal Language Model Review","permalink":"/en/2025/08/24/palm-e-review"},"nextItem":{"title":"Do As I Can, Not As I Say Review","permalink":"/en/2025/08/24/do-as-i-can-not-as-i-say-review"}}')}}]);