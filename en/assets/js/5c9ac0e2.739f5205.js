"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["40438"],{54049(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>c});var t=i(15068),s=i(74848),r=i(28453);let l={title:"CLIPort Review",date:new Date("2025-08-21T13:02:02.850Z"),description:"CLIPort Review",authors:"me",tags:["vlm"]},a,o={authorsImageUrls:[void 0]},c=[{value:"Key Idea",id:"key-idea",level:2},{value:"Framework Contributions",id:"framework-contributions",level:2},{value:"Architectural Design",id:"architectural-design",level:2},{value:"Key Insights",id:"key-insights",level:2},{value:"Experimental Results",id:"experimental-results",level:2},{value:"Conclusion",id:"conclusion",level:2},{value:"Ref",id:"ref",level:2}];function d(e){let n={h2:"h2",li:"li",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"key-idea",children:"Key Idea"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["CLIPort proposes a ",(0,s.jsx)(n.strong,{children:"two-stream architecture"})," for vision-based manipulation:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Semantic pathway (what):"})," leverages CLIP for broad semantic understanding."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spatial pathway (where):"})," leverages Transporter for fine-grained spatial reasoning."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["This design is ",(0,s.jsx)(n.strong,{children:"inspired by the two-stream hypothesis in cognitive psychology"})," (ventral/dorsal pathways)."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"framework-contributions",children:"Framework Contributions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Benchmark Extension:"})," Expanded the Ravens benchmark with language-grounding tasks for manipulation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Two-Stream Architecture:"})," Uses pre-trained vision-language models (CLIP) to condition precise manipulation policies with language goals."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Empirical Results:"})," Demonstrates robustness on diverse manipulation tasks, including multi-task settings and real-robot experiments."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"architectural-design",children:"Architectural Design"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["CLIPort integrates ",(0,s.jsx)(n.strong,{children:"semantic (CLIP)"})," with ",(0,s.jsx)(n.strong,{children:"spatial (Transporter)"})," features by lateral fusion."]}),"\n",(0,s.jsxs)(n.li,{children:["The semantic stream is conditioned with ",(0,s.jsx)(n.strong,{children:"language features from CLIP\u2019s text encoder"})," and fused with intermediate spatial features."]}),"\n",(0,s.jsxs)(n.li,{children:["Enables ",(0,s.jsx)(n.strong,{children:"end-to-end learning of affordance predictions"})," (pick-and-place) without explicit object models, segmentations, or symbolic states."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-insights",children:"Key Insights"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Formulates manipulation as ",(0,s.jsx)(n.strong,{children:"action detection"})," (where to act), instead of object detection."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tabula rasa systems"})," (like plain Transporter) require new demonstrations for every goal/task. CLIPort addresses this with a ",(0,s.jsx)(n.strong,{children:"strong semantic prior"})," (from CLIP) to generalize across tasks and concepts."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language-conditioned policies"})," provide an intuitive interface for specifying goals and transferring concepts."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"experimental-results",children:"Experimental Results"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simulation (PyBullet, UR5 robot with suction gripper):"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"10 language-conditioned tasks with thousands of unique instances."}),"\n",(0,s.jsx)(n.li,{children:"Multi-task CLIPort outperformed or matched single-task models, even with fewer demonstrations."}),"\n",(0,s.jsx)(n.li,{children:"CLIP-only or Transporter-only baselines saturate, while CLIPort exceeds 90% success with just 100 demos."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generalization:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["CLIPort generalizes to ",(0,s.jsx)(n.strong,{children:"unseen attributes"})," (e.g., new colors, shapes, object categories)."]}),"\n",(0,s.jsxs)(n.li,{children:["Struggles with ",(0,s.jsx)(n.strong,{children:"completely novel attributes"})," (e.g., \u201Cpink\u201D or \u201Corange\u201D never seen in training)."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-World Robot Experiments (Franka Panda):"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Achieved ~70% success on real tasks with just 179 demonstrations."}),"\n",(0,s.jsx)(n.li,{children:"Performance trends were consistent with simulation, validating sim-to-real transfer."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["CLIPort shows that ",(0,s.jsx)(n.strong,{children:"multi-task, language-conditioned policies"})," generalize across tasks better than object-centric or tabula rasa methods."]}),"\n",(0,s.jsxs)(n.li,{children:["With ",(0,s.jsx)(n.strong,{children:"action abstraction"})," and ",(0,s.jsx)(n.strong,{children:"spatio-semantic priors"}),", end-to-end models can learn new skills ",(0,s.jsx)(n.strong,{children:"without requiring hand-engineered pipelines"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Limitations remain for ",(0,s.jsx)(n.strong,{children:"dexterous 6-DoF manipulation"})," and complex continuous control."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"ref",children:"Ref"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Shridhar, M., Manuelli, L., & Fox, D. (2022). Cliport: What and where pathways for robotic manipulation. Conference on robot learning."}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453(e,n,i){i.d(n,{R:()=>l,x:()=>a});var t=i(96540);let s={},r=t.createContext(s);function l(e){let n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(r.Provider,{value:n},e.children)}},15068(e){e.exports=JSON.parse('{"permalink":"/en/2025/08/21/CLIPort-review","source":"@site/blog/2025/08/21/CLIPort-review.md","title":"CLIPort Review","description":"CLIPort Review","date":"2025-08-21T13:02:02.850Z","tags":[{"inline":false,"label":"vlm","permalink":"/en/tags/vlm","description":"Vision-Language Models"}],"readingTime":1.85,"hasTruncateMarker":false,"authors":[{"name":"Gracefullight","title":"Owner","url":"https://github.com/gracefullight","imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me","page":null}],"frontMatter":{"title":"CLIPort Review","date":"2025-08-21T13:02:02.850Z","description":"CLIPort Review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"Vocabulary for AI +005","permalink":"/en/vocab/vocab-ai-005"},"nextItem":{"title":"Mitigating Hallucinations on Object Attributes Review","permalink":"/en/2025/08/18/mitigating-hallucinations-on-object-attributesusing-multiview-images-review"}}')}}]);