{
    "version": "https://jsonfeed.org/version/1",
    "title": "gracefullight.dev Blog",
    "home_page_url": "https://gracefullight.dev/en/",
    "description": "gracefullight.dev Blog",
    "items": [
        {
            "id": "https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"neural-network-development-history\">Neural Network Development History<a href=\"https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#neural-network-development-history\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>1950s-1960s: Early Foundations<!-- -->\n<ul>\n<li>McCulloch &amp; Pitts (1943): mathematical neuron model</li>\n<li>Rosenblatt’s Perceptron (1958): first trainable network</li>\n<li>Minsky &amp; Papert (1969): limitations (XOR problem) → AI Winter</li>\n</ul>\n</li>\n<li>1970s–1980s: First Revival<!-- -->\n<ul>\n<li>Werbos (1974); Rumelhart, Hinton, Williams (1986): Backpropagation</li>\n<li>Hopfield Networks (1982): associative memory</li>\n<li>Renewed optimism but limited by hardware</li>\n</ul>\n</li>\n<li>1990s: Consolidation<!-- -->\n<ul>\n<li>LeCun’s CNN (LeNet, 1989): digit recognition</li>\n<li>Elman, Jordan: Recurrent Neural Networks</li>\n<li>Symbolic AI still dominated mainstream</li>\n</ul>\n</li>\n<li>2000s: Deep Learning Foundations<!-- -->\n<ul>\n<li>Better hardware (GPUs) + large datasets</li>\n<li>Hinton (2006): Deep Belief Networks (unsupervised pretraining)</li>\n<li>Connectionism regains attention</li>\n</ul>\n</li>\n<li>2010s: Deep Learning Boom<!-- -->\n<ul>\n<li>ImageNet (2012): AlexNet breakthrough</li>\n<li>RNNs, LSTMs, GRUs → speech &amp; translation</li>\n<li>Transformers (2017): revolutionized NLP</li>\n</ul>\n</li>\n<li>2020s: Scaling &amp; Foundation Models<!-- -->\n<ul>\n<li>Large Language Models (GPT, BERT, etc.)</li>\n<li>Multimodal AI: vision, text, speech integration</li>\n<li>Connectionism dominates AI research &amp; industry</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"neural-network-models\">Neural Network Models<a href=\"https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#neural-network-models\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>a collection of units (neurons) connected together</li>\n<li>The properties of the network are determined by its topology and the properties of the neurons.</li>\n<li>Roughly speaking, the neuron fires when a linear combination of its inputs exceeds some (hard or soft) threshold.</li>\n</ul>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Simple Neuron\" src=\"https://gracefullight.dev/en/assets/images/simple-neuron-11bdb8ee67568f5375608d2d70af0bce.png\" width=\"3010\" height=\"1427\" class=\"img_f7zd\"></p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi><msub><mi>n</mi><mi>j</mi></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></msubsup><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">in_j = \\sum_{i=0}^{n} w_{ij}a_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9456em;vertical-align:-0.2861em\"></span><span class=\"mord mathnormal\">i</span><span class=\"mord\"><span class=\"mord mathnormal\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.104em;vertical-align:-0.2997em\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8043em\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">0</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2997em\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em\">ij</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em\"><span></span></span></span></span></span></span></span></span></span></li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>o</mi><mi>u</mi><msub><mi>t</mi><mi>j</mi></msub><mo>=</mo><mi>g</mi><mo stretchy=\"false\">(</mo><mi>i</mi><msub><mi>n</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">out_j = g(in_j)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9012em;vertical-align:-0.2861em\"></span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">u</span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em\">g</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">i</span><span class=\"mord\"><span class=\"mord mathnormal\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>a</mi><mi>j</mi></msub><mo>=</mo><mi>g</mi><mo stretchy=\"false\">(</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></msubsup><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>a</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">a_j = g(\\sum_{i=0}^{n} w_{ij} a_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.104em;vertical-align:-0.2997em\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em\">g</span><span class=\"mopen\">(</span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8043em\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">0</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2997em\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em\">ij</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"activation-function\">Activation function<a href=\"https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#activation-function\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"relu-function\">ReLU function<a href=\"https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#relu-function\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">ReLU(x) = max(0, x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em\">LU</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em\"></span><span class=\"mord mathnormal\">ma</span><span class=\"mord mathnormal\">x</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></p>\n<ul>\n<li>an abbreviation for rectified linear unit</li>\n<li>Commonly used</li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"softplus-function\">Softplus function<a href=\"https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#softplus-function\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>p</mi><mi>l</mi><mi>u</mi><mi>s</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mi>x</mi></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Softplus(x) = \\log(1 + e^x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em\">S</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em\">f</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em\">tpl</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">s</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em\">g</span></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222em\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6644em\"><span style=\"top:-3.063em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">x</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></p>\n<ul>\n<li>A smooth version of the ReLU function</li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"logistic-or-sigmoid-function\">Logistic or Sigmoid function<a href=\"https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#logistic-or-sigmoid-function\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>c</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">Logistic(x) = \\frac{1}{1 + e^{-x}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em\"></span><span class=\"mord mathnormal\">L</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em\">g</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">c</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2484em;vertical-align:-0.4033em\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8451em\"><span style=\"top:-2.655em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7027em\"><span style=\"top:-2.786em;margin-right:0.0714em\"><span class=\"pstrut\" style=\"height:2.5em\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mathnormal mtight\">x</span></span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em\"></span></span><span style=\"top:-3.394em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4033em\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></p>\n<ul>\n<li>Non-linear, can represent a nonlinear function</li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"tanh-function\">Tanh function<a href=\"https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#tanh-function\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><mn>2</mn><mi>x</mi></mrow></msup><mo>−</mo><mn>1</mn></mrow><mrow><msup><mi>e</mi><mrow><mn>2</mn><mi>x</mi></mrow></msup><mo>+</mo><mn>1</mn></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">tanh(x) = \\frac{e^{2x} -1}{e^{2x} + 1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.4213em;vertical-align:-0.4033em\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0179em\"><span style=\"top:-2.655em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7463em\"><span style=\"top:-2.786em;margin-right:0.0714em\"><span class=\"pstrut\" style=\"height:2.5em\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathnormal mtight\">x</span></span></span></span></span></span></span></span></span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.23em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em\"></span></span><span style=\"top:-3.394em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em\"><span style=\"top:-2.931em;margin-right:0.0714em\"><span class=\"pstrut\" style=\"height:2.5em\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathnormal mtight\">x</span></span></span></span></span></span></span></span></span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4033em\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></p>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"topology-of-a-neural-network\">Topology of a neural network<a href=\"https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#topology-of-a-neural-network\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Feed-forward network (FFN):<!-- -->\n<ul>\n<li>Every node receives inputs from \"upstream\" nodes and delivers output to \"downstream\" nodes.</li>\n<li>There are no loops.</li>\n<li>FFN represents a function of its current inputs, thus it has no internal state other than the weights themselves.</li>\n</ul>\n</li>\n<li>Recurrent Network (RNN):<!-- -->\n<ul>\n<li>A recurrent network feeds its outputs back into its own inputs.</li>\n<li>In a recurrent network, the neuron values can eventually settle down, keep cycling, or behave unpredictably.</li>\n<li>can support short-term memory</li>\n</ul>\n</li>\n</ul>\n<table><thead><tr><th>FFN</th><th>RNN</th></tr></thead><tbody><tr><td><img decoding=\"async\" loading=\"lazy\" alt=\"FFN\" src=\"https://gracefullight.dev/en/assets/images/ffn-ff75c9f4654345c4c880e672b8384331.png\" width=\"454\" height=\"436\" class=\"img_f7zd\"></td><td><img decoding=\"async\" loading=\"lazy\" alt=\"RNN\" src=\"https://gracefullight.dev/en/assets/images/rnn-be95c2621b75fdfbb5f8bce837be8e37.png\" width=\"454\" height=\"472\" class=\"img_f7zd\"></td></tr></tbody></table>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"training-process\">Training Process<a href=\"https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#training-process\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Go through each training sample.</li>\n<li>If correctly classified → do nothing.</li>\n<li>If misclassified → update the weights:</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>←</mo><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mi>α</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo>−</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo stretchy=\"false\">)</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_i \\leftarrow w_i + \\alpha(y - \\hat{y})x_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">←</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7333em;vertical-align:-0.15em\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em\">α</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em\">y</span><span class=\"mspace\" style=\"margin-right:0.2222em\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6944em\"><span style=\"top:-3em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em\">y</span></span><span style=\"top:-3em\"><span class=\"pstrut\" style=\"height:3em\"></span><span class=\"accent-body\" style=\"left:-0.1944em\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1944em\"><span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em\"><span class=\"pstrut\" style=\"height:2.7em\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em\"><span></span></span></span></span></span></span></span></span></span></li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"perceptron-for-binary-classification\">Perceptron for Binary Classification<a href=\"https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#perceptron-for-binary-classification\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<ul>\n<li>A perceptron separates data into two classes with a hyperplane.</li>\n<li>if <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mo>⋅</mo><mi>x</mi><mo>≥</mo><mn>0</mn><mo>→</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w \\cdot x \\geq 0 \\rightarrow 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4445em\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em\">w</span><span class=\"mspace\" style=\"margin-right:0.2222em\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7719em;vertical-align:-0.136em\"></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">≥</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em\"></span><span class=\"mord\">0</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em\"></span><span class=\"mord\">1</span></span></span></span></li>\n<li>if <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mo>⋅</mo><mi>x</mi><mo>≤</mo><mn>0</mn><mo>→</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">w \\cdot x \\le 0 \\rightarrow 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4445em\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em\">w</span><span class=\"mspace\" style=\"margin-right:0.2222em\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7719em;vertical-align:-0.136em\"></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em\"></span><span class=\"mord\">0</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em\"></span><span class=\"mord\">0</span></span></span></span></li>\n</ul>",
            "url": "https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/",
            "title": "Introduction to AI @005",
            "summary": "Introduction to AI @005",
            "date_modified": "2025-09-02T02:56:15.922Z",
            "author": {
                "name": "Gracefullight",
                "url": "https://github.com/gracefullight"
            },
            "tags": [
                "iai"
            ]
        },
        {
            "id": "https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"rt-x\">RT-X<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#rt-x\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>RT-X trains generalist robot policies by co-training RT-1/RT-2 on an X-embodiment mix of multi-robot, multi-task data, enabling efficient adaptation to new robots, tasks, and environments.</li>\n<li>It standardizes 1M+ trajectories from 22 embodiments into the Open X-Embodiment (RLDS/tfrecord) repository, unifying observations and 7-DoF actions via coarse alignment.</li>\n<li>Experiments show strong positive transfer and emergent skills (≈3× with RT-2-X on cross-robot tasks); performance scales with model capacity, short image histories, and web pretraining, while sensing/actuation diversity and frame alignment remain open problems.</li>\n</ul>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"RT-X Architecture\" src=\"https://gracefullight.dev/en/assets/images/rt-x-architecture-cc2128128460577bc8f720626e0d671d.png\" width=\"1552\" height=\"412\" class=\"img_f7zd\"></p>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"motivation\">Motivation<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#motivation\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Seeks a <strong>generalist X-robot policy</strong> that can be efficiently adapted to new robots, tasks, and environments.</li>\n<li>Mirrors a trend from CV/NLP where <strong>general-purpose, web-scale pretrained models</strong> outperform narrow, task-specific models.</li>\n<li>Robotics lacks comparably large, diverse <strong>interaction datasets</strong>, making direct transfer of these lessons challenging.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"objectives\">Objectives<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#objectives\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ol>\n<li><strong>Positive transfer:</strong> Test whether co-training on data from many robots improves performance on each training domain.</li>\n<li><strong>Ecosystem building:</strong> Organize large robotic datasets to enable future X-embodiment research.</li>\n</ol>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"core-approach\">Core Approach<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#core-approach\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Train <strong>RT-1</strong> and <strong>RT-2</strong> on data from <strong>9 different manipulators</strong>, producing <strong>RT-X</strong> variants that outperform policies trained only on the evaluation domain and show <strong>better generalization</strong> and <strong>new capabilities</strong>.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"whats-different-from-prior-transfer-methods\">What’s Different From Prior Transfer Methods<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#whats-different-from-prior-transfer-methods\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Many prior works reduce the <strong>embodiment gap</strong> via specialized mechanisms (shared action spaces, representation learning objectives, policy adaptation using embodiment metadata, decoupled robot/environment representations, domain translation).</li>\n<li><strong>RT-X directly trains on X-embodiment data without explicit gap-reduction machinery</strong> and still observes <strong>positive transfer</strong>.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"dataset--format-open-x-embodiment\">Dataset &amp; Format (Open X-Embodiment)<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#dataset--format-open-x-embodiment\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>1M+ real robot trajectories, 22 embodiments</strong> (single-arm, bimanual, quadrupeds), pooled from <strong>60 datasets / 34 labs</strong>, standardized for easy use.</li>\n<li>Uses <a href=\"https://github.com/google-research/rlds\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>RLDS</strong></a> (serialized <code>tfrecord</code>), supporting varied action spaces and input modalities (RGB, depth, point clouds), and efficient parallel loading across major DL frameworks.</li>\n<li>Language annotations are leveraged; <strong>PaLM</strong> is used to extract objects/behaviors from instructions.</li>\n</ul>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"RLDS\" src=\"https://gracefullight.dev/en/assets/images/rlds-5e68d1c660ef048892d5594530c62239.png\" width=\"726\" height=\"353\" class=\"img_f7zd\"></p>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"data-format-consolidation-coarse-alignment\">Data Format Consolidation (Coarse Alignment)<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#data-format-consolidation-coarse-alignment\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Observations:</strong> History of recent images + language instruction. One <strong>canonical camera view</strong> per dataset is resized to a common resolution.</li>\n<li><strong>Actions:</strong> Convert original controls to a <strong>7-DoF end-effector vector</strong> (x, y, z, roll, pitch, yaw, gripper or their rates). Actions are <strong>normalized before discretization</strong>; outputs are <strong>de-normalized per embodiment</strong>.</li>\n<li><strong>Deliberate non-alignment:</strong> Camera poses/properties are <strong>not</strong> standardized; action frame alignment across datasets is <strong>not</strong> enforced. The same action vector may cause <strong>different motions</strong> on different robots (absolute/relative, position/velocity allowed).</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"policy-architectures\">Policy Architectures<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#policy-architectures\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>RT-1 (≈35M params):</strong> Transformer for control. Inputs: 15-frame image history + natural-language instruction.<!-- -->\n<ul>\n<li>Vision via ImageNet-pretrained <strong>EfficientNet</strong>; language via <strong>USE</strong> embedding.</li>\n<li>Fuse via <strong>FiLM</strong> → 81 vision–language tokens → <strong>decoder-only Transformer</strong> outputs tokenized actions.</li>\n</ul>\n</li>\n<li><strong>RT-2 (VLA family):</strong> Internet-scale VLM co-fine-tuned to output <strong>action as text tokens</strong> (e.g., <code>1 128 91 241 5 101 127</code>).<!-- -->\n<ul>\n<li>Any pretrained VLM can be adapted; this work uses <strong>RT-2–PaLI-X</strong> (ViT backbone + UL2 LM; primarily pretrained on WebLI).</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"training-setup\">Training Setup<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#training-setup\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Robotics data mixture:</strong> Data from <strong>9 manipulators</strong> (a union of multiple well-known robotics datasets).</li>\n<li><strong>Loss:</strong> Standard <strong>categorical cross-entropy</strong> over tokenized actions.</li>\n<li><strong>Regimes:</strong>\n<ul>\n<li><strong>RT-1-X:</strong> Trained solely on the robotics mixture.</li>\n<li><strong>RT-2-X:</strong> <strong>Co-fine-tuned</strong> on a ~1:1 mix of original VLM data and the robotics mixture.</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"experimental-questions\">Experimental Questions<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#experimental-questions\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ol>\n<li>Does X-embodiment co-training improve in-domain performance (positive transfer)?</li>\n<li>Does it improve <strong>generalization</strong> to <strong>unseen tasks</strong>?</li>\n<li>How do <strong>model size</strong>, <strong>architecture</strong>, and <strong>dataset composition</strong> influence performance/generalization?</li>\n</ol>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"key-results\">Key Results<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#key-results\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Small-scale domains:</strong> <strong>RT-1-X</strong> outperforms the <strong>Original Method</strong> (the authors’ per-dataset baselines) on <strong>4/5</strong> datasets with a large average gain → <strong>limited data domains</strong> benefit greatly from X-embodiment co-training.</li>\n<li><strong>Large-scale domains:</strong>\n<ul>\n<li><strong>RT-1-X</strong> does <strong>not</strong> beat an RT-1 trained only on the embodiment-specific large dataset (suggests underfitting for this class).</li>\n<li><strong>RT-2-X</strong> (larger capacity) <strong>outperforms both</strong> Original Method and RT-1 → X-robot training helps even in <strong>data-rich</strong> regimes when using <strong>sufficient capacity</strong>.</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"generalization--emergent-skills\">Generalization &amp; Emergent Skills<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#generalization--emergent-skills\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Unseen objects/backgrounds/environments:</strong> RT-2 and RT-2-X perform <strong>on par</strong> (VLM backbone already strong here).</li>\n<li><strong>Emergent skills (transfer across robots):</strong> On Google Robot tasks that <strong>do not appear</strong> in RT-2’s dataset but exist in <strong>Bridge</strong> (for <strong>WidowX</strong>), <strong>RT-2-X ≈ 3×</strong> RT-2.<!-- -->\n<ul>\n<li>Removing <strong>Bridge</strong> from RT-2-X training <strong>significantly reduces</strong> hold-out performance → skills likely <strong>transferred</strong> from WidowX data.</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"design-insights-ablations\">Design Insights (Ablations)<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#design-insights-ablations\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Short image history</strong> notably <strong>improves generalization</strong>.</li>\n<li><strong>Web pretraining</strong> is <strong>critical</strong> for large models’ high performance.</li>\n<li><strong>Model capacity matters:</strong> <strong>55B</strong> model succeeds more than <strong>5B</strong> on emergent skills → greater capacity ⇒ greater cross-dataset transfer.</li>\n<li><strong>Co-fine-tuning vs. fine-tuning:</strong> Similar performance in this study (attributed to the <strong>greater diversity</strong> of robotics data in RT-2-X vs. prior works).</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"limitations-open-problems\">Limitations (Open Problems)<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#limitations-open-problems\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Does <strong>not</strong> cover robots with <strong>very different sensing/actuation modalities</strong>.</li>\n<li>Does <strong>not</strong> study generalization to <strong>new robots</strong> nor define a <strong>decision criterion</strong> for when positive transfer will occur.</li>\n<li>Camera pose/properties and control frame <strong>remain unaligned</strong>; a deliberate but still challenging domain gap to address in future work.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"ref\">Ref<a href=\"https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#ref\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>O’Neill, A., Rehman, A., Maddukuri, A., Gupta, A., Padalkar, A., Lee, A., Pooley, A., Gupta, A., Mandlekar, A., &amp; Jain, A. (2024). Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. 2024 IEEE International Conference on Robotics and Automation (ICRA).</li>\n</ul>",
            "url": "https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/",
            "title": "Open X-Embodiment review",
            "summary": "Open X-Embodiment review",
            "date_modified": "2025-09-01T03:47:28.350Z",
            "author": {
                "name": "Gracefullight",
                "url": "https://github.com/gracefullight"
            },
            "tags": [
                "vlm"
            ]
        },
        {
            "id": "https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"oop-vs-procedural-programming\">OOP vs Procedural Programming<a href=\"https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#oop-vs-procedural-programming\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"oop\">OOP<a href=\"https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#oop\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<ul>\n<li>a programming paradigm built around the concept of objects, which contain data and code to manipulate data.</li>\n<li>The idea to model real-world entities and their interactions.</li>\n<li>Global Data (fields) are enclosed in the objects.</li>\n<li>Program components/tasks are easily divided across the development team / Requires more planning and design preparation</li>\n<li>Easier to manage and maintain dependencies between objects / OOP programs are much larger and complex</li>\n<li>Objects export the interface and hide the implementation and data / Tend to use more memory and GPU</li>\n<li>Code is highly reusable and easy to scale and distribute / Making changes in one class potentially impact others, which can complicate the development of the code.</li>\n</ul>\n<!-- -->\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"procedural-programming\">Procedural Programming<a href=\"https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#procedural-programming\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<ul>\n<li>the concept of procedure calls by structuring the program around procedures. (or functions/subroutines)</li>\n<li>a sequential manner unless directed otherwise.</li>\n<li>Global data (elements) is exposed to all the functions.</li>\n<li>Easier to compile and interpret / Difficult to scale or extend</li>\n<li>Straightforward and simpler to code / Dependencies between elements are unclear and not well-structured.</li>\n<li>Less memory requirements / Data is exposed and insecure due to its exposure across the whole program</li>\n<li>Easy to track the program flow / Hard to divide the work among programmers in a team.</li>\n</ul>\n<!-- -->\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"classes\">Classes<a href=\"https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#classes\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>A class is a template/blueprint used to create objects</li>\n</ul>\n<table><thead><tr><th>java</th><th>python</th></tr></thead><tbody><tr><td>a pure OOP language</td><td>supports OOP</td></tr><tr><td>code must be written in classes</td><td>classes are optional</td></tr><tr><td>executable class must have <code>main()</code></td><td>scripts run without including a class</td></tr><tr><td>Encapsulation can be enforced by declaring fields as private</td><td>fields (global variables) are public by default</td></tr><tr><td>Visibility is managed through access modifiers</td><td>N/A (\"_\" to identify private data attributes, but still accessible)</td></tr></tbody></table>\n<div class=\"language-py codeBlockContainer_QFtC theme-code-block\" style=\"--prism-color:#393A34;--prism-background-color:#f6f8fa\"><div class=\"codeBlockContent_vdmG\"><pre tabindex=\"0\" class=\"prism-code language-py codeBlock_CpxK thin-scrollbar\" style=\"color:#393A34;background-color:#f6f8fa\"><code class=\"codeBlockLines__0Nl\"><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token keyword\" style=\"color:#00009f\">class</span><span class=\"token plain\"> </span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token keyword\" style=\"color:#00009f\">class</span><span class=\"token operator\" style=\"color:#393A34\">-</span><span class=\"token plain\">name</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">extend </span><span class=\"token operator\" style=\"color:#393A34\">-</span><span class=\"token plain\"> superclass</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token punctuation\" style=\"color:#393A34\">:</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    </span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">variable</span><span class=\"token operator\" style=\"color:#393A34\">-</span><span class=\"token plain\">name</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token plain\"> </span><span class=\"token operator\" style=\"color:#393A34\">=</span><span class=\"token plain\"> </span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">value</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token plain\"> </span><span class=\"token comment\" style=\"color:#999988;font-style:italic\">#Class fields - data members</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    </span><span class=\"token keyword\" style=\"color:#00009f\">def</span><span class=\"token plain\"> </span><span class=\"token function\" style=\"color:#d73a49\">__init</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token plain\">self</span><span class=\"token punctuation\" style=\"color:#393A34\">,</span><span class=\"token plain\"> </span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">parameters</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token punctuation\" style=\"color:#393A34\">:</span><span class=\"token plain\"> </span><span class=\"token comment\" style=\"color:#999988;font-style:italic\">#class constructor - object sbuilder</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">        </span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">code</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    </span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">method</span><span class=\"token operator\" style=\"color:#393A34\">-</span><span class=\"token plain\">name</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token plain\">self</span><span class=\"token punctuation\" style=\"color:#393A34\">,</span><span class=\"token plain\"> </span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">parameters</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token punctuation\" style=\"color:#393A34\">:</span><span class=\"token plain\"> </span><span class=\"token comment\" style=\"color:#999988;font-style:italic\">#methods</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">        </span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">code</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><br></span></code></pre></div></div>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"classes-py\">Classes Py<a href=\"https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#classes-py\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<table><thead><tr><th>Keywords</th><th>Functions</th></tr></thead><tbody><tr><td><code>class</code></td><td><code>__init__()</code></td></tr><tr><td><code>self</code>: keyword used to refer to object properties</td><td>del: the function is used to delete an object</td></tr><tr><td><code>pass</code>: keyword used to occupy no-code placement in a function</td><td><code>__str__()</code>: The function is used to return string representation of instances</td></tr><tr><td><code>cls</code>: keyword used to refer to class properties</td><td><code>super()</code>: the function is used call a parent method in a child class</td></tr></tbody></table>\n<ul>\n<li>Accessors: functions (with no parameters) in a Python class that provide access to the data attributes of an object.<!-- -->\n<ul>\n<li>known as getter methods, are named starting with the verb get, followed by the field name, which should start with an uppercase letter.</li>\n</ul>\n</li>\n<li>Mutators: procedures (with parameter) in a Python class that enable the developer to modify the values of object attributes.<!-- -->\n<ul>\n<li>known as setter methods, are named starting with the verb set, followed by the field name, which should start with an uppercase letter.</li>\n</ul>\n</li>\n</ul>\n<div class=\"language-py codeBlockContainer_QFtC theme-code-block\" style=\"--prism-color:#393A34;--prism-background-color:#f6f8fa\"><div class=\"codeBlockContent_vdmG\"><pre tabindex=\"0\" class=\"prism-code language-py codeBlock_CpxK thin-scrollbar\" style=\"color:#393A34;background-color:#f6f8fa\"><code class=\"codeBlockLines__0Nl\"><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token keyword\" style=\"color:#00009f\">def</span><span class=\"token plain\"> get</span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">Variable</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token punctuation\" style=\"color:#393A34\">:</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    </span><span class=\"token keyword\" style=\"color:#00009f\">return</span><span class=\"token plain\"> self</span><span class=\"token punctuation\" style=\"color:#393A34\">.</span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">field</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:#00009f\">def</span><span class=\"token plain\"> </span><span class=\"token builtin\">set</span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">Variable</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token plain\">self</span><span class=\"token punctuation\" style=\"color:#393A34\">,</span><span class=\"token plain\"> value</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token punctuation\" style=\"color:#393A34\">:</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    self</span><span class=\"token punctuation\" style=\"color:#393A34\">.</span><span class=\"token operator\" style=\"color:#393A34\">&lt;</span><span class=\"token plain\">field</span><span class=\"token operator\" style=\"color:#393A34\">&gt;</span><span class=\"token plain\"> </span><span class=\"token operator\" style=\"color:#393A34\">=</span><span class=\"token plain\"> value</span><br></span></code></pre></div></div>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"classes-java\">Classes Java<a href=\"https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#classes-java\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<div class=\"language-java codeBlockContainer_QFtC theme-code-block\" style=\"--prism-color:#393A34;--prism-background-color:#f6f8fa\"><div class=\"codeBlockContent_vdmG\"><pre tabindex=\"0\" class=\"prism-code language-java codeBlock_CpxK thin-scrollbar\" style=\"color:#393A34;background-color:#f6f8fa\"><code class=\"codeBlockLines__0Nl\"><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token keyword\" style=\"color:#00009f\">public</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:#00009f\">class</span><span class=\"token plain\"> </span><span class=\"token class-name\">Bank</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:#393A34\">{</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">  </span><span class=\"token keyword\" style=\"color:#00009f\">private</span><span class=\"token plain\"> </span><span class=\"token class-name\">Customer</span><span class=\"token plain\"> customer</span><span class=\"token punctuation\" style=\"color:#393A34\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">  </span><span class=\"token keyword\" style=\"color:#00009f\">private</span><span class=\"token plain\"> </span><span class=\"token class-name\">String</span><span class=\"token plain\"> branch</span><span class=\"token punctuation\" style=\"color:#393A34\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">  </span><span class=\"token keyword\" style=\"color:#00009f\">public</span><span class=\"token plain\"> </span><span class=\"token class-name\">Bank</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:#393A34\">{</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    customer </span><span class=\"token operator\" style=\"color:#393A34\">=</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:#00009f\">new</span><span class=\"token plain\"> </span><span class=\"token class-name\">Customer</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token punctuation\" style=\"color:#393A34\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">  </span><span class=\"token punctuation\" style=\"color:#393A34\">}</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">  </span><span class=\"token keyword\" style=\"color:#00009f\">public</span><span class=\"token plain\"> </span><span class=\"token class-name\">Bank</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token class-name\">String</span><span class=\"token plain\"> name</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:#393A34\">{</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    </span><span class=\"token keyword\" style=\"color:#00009f\">this</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token punctuation\" style=\"color:#393A34\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    </span><span class=\"token keyword\" style=\"color:#00009f\">this</span><span class=\"token punctuation\" style=\"color:#393A34\">.</span><span class=\"token plain\">branch </span><span class=\"token operator\" style=\"color:#393A34\">=</span><span class=\"token plain\"> name</span><span class=\"token punctuation\" style=\"color:#393A34\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">  </span><span class=\"token punctuation\" style=\"color:#393A34\">}</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">  </span><span class=\"token keyword\" style=\"color:#00009f\">public</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:#00009f\">boolean</span><span class=\"token plain\"> </span><span class=\"token function\" style=\"color:#d73a49\">find</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token class-name\">Bank</span><span class=\"token plain\"> bank</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:#393A34\">{</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">    </span><span class=\"token keyword\" style=\"color:#00009f\">return</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:#00009f\">this</span><span class=\"token punctuation\" style=\"color:#393A34\">.</span><span class=\"token plain\">branch</span><span class=\"token punctuation\" style=\"color:#393A34\">.</span><span class=\"token function\" style=\"color:#d73a49\">equals</span><span class=\"token punctuation\" style=\"color:#393A34\">(</span><span class=\"token plain\">bank</span><span class=\"token punctuation\" style=\"color:#393A34\">.</span><span class=\"token plain\">branch</span><span class=\"token punctuation\" style=\"color:#393A34\">)</span><span class=\"token punctuation\" style=\"color:#393A34\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\">  </span><span class=\"token punctuation\" style=\"color:#393A34\">}</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#393A34\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:#393A34\">}</span><br></span></code></pre></div></div>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"packages\">Packages<a href=\"https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#packages\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"packages-java\">Packages Java<a href=\"https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#packages-java\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<ul>\n<li>used to group related classes</li>\n<li>like folders containing files (classes)</li>\n<li>either Java defined or user-defined</li>\n<li>used to write maintainable and portable code and to avoid class name conflicts.</li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"modules-py\">Modules Py<a href=\"https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#modules-py\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<ul>\n<li>used to grou prelated functio nand classes together</li>\n<li>normal Python scripts that are used into other scripts</li>\n<li>either Python defined or user-defined</li>\n<li>used to write maintainable and portable code to improve reusability</li>\n</ul>",
            "url": "https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/",
            "title": "Fundamentals of software development @006",
            "summary": "Fundamentals of software development @006",
            "date_modified": "2025-09-01T00:09:15.360Z",
            "author": {
                "name": "Gracefullight",
                "url": "https://github.com/gracefullight"
            },
            "tags": [
                "fsd"
            ]
        },
        {
            "id": "https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"π0\">π0<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#%CF%800\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"problem--motivation\">Problem &amp; Motivation<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#problem--motivation\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Achieving real-world generality in robot learning is blocked by <strong>data scarcity, generalization, and robustness</strong> limits.</li>\n<li>Human intelligence most outpaces machines in <strong>versatility</strong>—solving diverse, physically situated tasks under constraints, language commands, and perturbations.</li>\n<li>In NLP/CV, <strong>foundation models</strong> pre-trained on diverse multi-task data, then <strong>fine-tuned (aligned)</strong> on curated datasets, outperform narrow specialists; the same paradigm is hypothesized for robotics.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"core-proposal\">Core Proposal<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#core-proposal\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>A <strong>novel flow-matching architecture</strong> built on a pre-trained <strong>Vision-Language Model (VLM)</strong> to inherit Internet-scale semantics.</li>\n<li>Further training adds <strong>robot actions</strong>, turning the model into a <strong>Vision-Language-Action (VLA)</strong> policy.</li>\n<li>Use <strong>cross-embodiment training</strong> to combine data from many robot types (single/dual-arm, mobile), despite differing configuration/action spaces.</li>\n<li>Employ <strong>action chunking</strong> + <strong>flow matching</strong> (diffusion variant) to model complex, continuous, high-frequency actions.</li>\n<li>Introduce an <strong>Action Expert</strong> (separate weights for action/state tokens), akin to a <strong>Mixture-of-Experts</strong>, augmenting the standard VLM.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"training-recipe-pre--vs-post-training\">Training Recipe (Pre- vs Post-Training)<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#training-recipe-pre--vs-post-training\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Pre-training</strong> on highly diverse data builds broad, general physical abilities.</li>\n<li><strong>Post-training</strong> on curated, task-specific data instills <strong>fluent, efficient strategies</strong>.</li>\n<li>Rationale: high-quality-only training lacks recovery behaviors; low-quality-only training lacks efficiency/robustness; <strong>combining both</strong> yields desired behavior.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"data--backbone\">Data &amp; Backbone<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#data--backbone\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>~<strong>10,000 hours</strong> of demonstrations + the <strong>OXE</strong> dataset; data spans <strong>7 robot configurations</strong> and <strong>68 tasks</strong>.</li>\n<li>VLM backbone initialized from <strong>PaliGemma (3B)</strong>; add <strong>~300M</strong> parameters for the action expert (total <strong>~3.3B</strong>).</li>\n<li>Pre-training mixture: weighted combination of internal datasets + full OXE; <strong>n^0.43</strong> weighting to down-weight overrepresented task-robot pairs.</li>\n<li>Unify interfaces: zero-pad <strong>qt/at</strong> to the largest robot dimension (18); mask missing image slots; late-fusion encoders map images/states to the same token space as language.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"modeling-details\">Modeling Details<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#modeling-details\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Conditional flow matching</strong> models the continuous distribution over action chunks.</li>\n<li>Train with a <strong>diffusion-style loss</strong> on individual sequence elements (instead of cross-entropy), with separate weights for diffusion-related tokens.</li>\n<li>Flow path uses a <strong>linear-Gaussian</strong> schedule; sample noisy actions with ε∼N(0, I); predict denoising vector field; <strong>Euler integration</strong> from τ=0→1 at inference.</li>\n<li>Efficient inference by <strong>caching</strong> K/V for the observation prefix; action tokens recomputed per integration step.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"high-level-language-policy\">High-Level Language Policy<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#high-level-language-policy\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Because the policy consumes language, a <strong>high-level VLM</strong> can decompose tasks (e.g., bussing) into intermediate language subgoals (SayCan-style planning), improving performance on complex, temporally extended tasks.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"evaluation-setup--baselines\">Evaluation Setup &amp; Baselines<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#evaluation-setup--baselines\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Out-of-box</strong> (direct prompting), <strong>fine-tuning</strong> on downstream tasks, and <strong>with high-level VLM</strong> providing intermediate commands.</li>\n<li>Compare against <strong>OpenVLA (7B, autoregressive discretization; no action chunks/high-frequency control)</strong> and <strong>Octo (93M; diffusion)</strong>, trained on the same mixture.</li>\n<li>Include a <strong>compute-parity</strong> π0 (160k steps vs 700k) and a <strong>π0-small</strong> variant (no VLM init).</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"key-results\">Key Results<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#key-results\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Out-of-box</strong>: π0 outperforms all baselines; even compute-parity π0 beats OpenVLA/Octo; π0-small still surpasses them—highlighting the benefits of <strong>expressive architectures + diffusion/flow matching + VLM pre-training</strong>.</li>\n<li><strong>Language following</strong>: π0 clearly exceeds π0-small across conditions:<!-- -->\n<ul>\n<li><strong>π0-flat</strong>: only overall task command.</li>\n<li><strong>π0-human</strong>: human-provided intermediate steps.</li>\n<li><strong>π0-HL</strong>: high-level VLM-provided steps (fully autonomous).</li>\n<li>Better language-following accuracy <strong>directly translates</strong> into stronger autonomous performance with high-level guidance.</li>\n</ul>\n</li>\n<li><strong>New dexterous tasks</strong> (e.g., bowls stacking, towel folding, microwave, drawer items, paper towel replacement):<!-- -->\n<ul>\n<li>Fine-tuned π0 generally outperforms <strong>OpenVLA</strong>, <strong>Octo</strong>, and small-data methods <strong>ACT</strong> / <strong>Diffusion Policy</strong>.</li>\n<li>Pre-training helps most when tasks resemble pre-training data; pretrained π0 often beats from-scratch by up to <strong>2×</strong>.</li>\n</ul>\n</li>\n<li><strong>Complex multi-stage tasks</strong> (laundry folding, table bussing, box building, to-go box, eggs):<!-- -->\n<ul>\n<li>π0 solves many tasks; <strong>full pre-training + fine-tuning</strong> performs best.</li>\n<li>Gains from pre-training are <strong>especially large</strong> on harder tasks; absolute performance varies with task difficulty and pre-training coverage.</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"takeaways--limitations\">Takeaways &amp; Limitations<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#takeaways--limitations\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>π0 mirrors LLM training: <strong>pre-train for knowledge</strong>, <strong>post-train for alignment</strong> (instruction-following and execution).</li>\n<li>Limitations/open questions:<!-- -->\n<ul>\n<li>Optimal <strong>composition/weighting</strong> of pre-training data remains unclear.</li>\n<li>Not all tasks work reliably; difficult to predict <strong>how much/what kind</strong> of data is needed for near-perfect performance.</li>\n<li>Uncertain <strong>positive transfer</strong> across very diverse tasks/robots and to distinct domains (e.g., driving, navigation, legged locomotion).</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"ref\">Ref<a href=\"https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#ref\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B., Jakubczak, S., Jones, T., Ke, L., Levine, S., Li‑Bell, A., Mothukuri, M., Nair, S., Pertsch, K., Shi, L. X, … Zhilinsky, U. (2025, June 21). π₀: A vision‑language‑action flow model for general robot control Robotics: Science and Systems (RSS), Los Angeles, CA, United States. <code>https://roboticsconference.org/program/papers/10/</code></li>\n</ul>",
            "url": "https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/",
            "title": "π0 Review",
            "summary": "π0 Review",
            "date_modified": "2025-08-31T14:24:33.716Z",
            "author": {
                "name": "Gracefullight",
                "url": "https://github.com/gracefullight"
            },
            "tags": [
                "vlm"
            ]
        },
        {
            "id": "https://gracefullight.dev/en/2025/08/31/vima-review/",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"vima\">VIMA<a href=\"https://gracefullight.dev/en/2025/08/31/vima-review/#vima\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Unified Multimodal Prompts</strong>: Reformulates diverse robot tasks (language, images, video) into a single sequence modeling problem.</li>\n<li><strong>Object-Centric Tokenization</strong>: Uses object-level tokens (Mask R-CNN + ViT) instead of raw pixels, improving data efficiency and semantic generalization.</li>\n<li><strong>Cross-Attention Conditioning</strong>: Conditions the policy on prompts via cross-attention, maintaining strong zero-shot performance even with small models or novel tasks.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"motivation\">Motivation<a href=\"https://gracefullight.dev/en/2025/08/31/vima-review/#motivation\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Robot task specification comes in many forms: one-shot demonstrations, language instructions, and visual goals.</li>\n<li>Traditionally, each task required distinct architectures and pipelines, leading to siloed systems with poor generalization.</li>\n</ul>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"VIMA Architecture\" src=\"https://gracefullight.dev/en/assets/images/vima-architecture-f4e3269ed85bc5b1c954f82ab1ef77cd.png\" width=\"1424\" height=\"988\" class=\"img_f7zd\"></p>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"key-contributions\">Key Contributions<a href=\"https://gracefullight.dev/en/2025/08/31/vima-review/#key-contributions\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ol>\n<li>\n<p><strong>Multimodal Prompting</strong></p>\n<ul>\n<li>A novel formulation that unifies diverse robot manipulation tasks into a <strong>sequence modeling problem</strong>.</li>\n<li>Prompts are defined as interleaved sequences of text and images, enabling flexibility across task formats.</li>\n</ul>\n</li>\n<li>\n<p><strong>VIMA-BENCH</strong></p>\n<ul>\n<li>A large-scale benchmark with <strong>17 tasks</strong> across six categories (object manipulation, goal reaching, novel concept grounding, video imitation, constraint satisfaction, visual reasoning).</li>\n<li>Provides <strong>650K expert trajectories</strong> and a <strong>four-level evaluation protocol</strong> for systematic generalization.</li>\n</ul>\n</li>\n<li>\n<p><strong>VIMA Agent</strong></p>\n<ul>\n<li>A transformer-based visuomotor agent with <strong>encoder-decoder architecture</strong> and <strong>object-centric design</strong>.</li>\n<li>Encodes prompts with a pre-trained <strong>T5 model</strong>, parses images into object tokens via <strong>Mask R-CNN + ViT</strong>, and decodes actions autoregressively using <strong>cross-attention</strong>.</li>\n</ul>\n</li>\n</ol>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"design-insights\">Design Insights<a href=\"https://gracefullight.dev/en/2025/08/31/vima-review/#design-insights\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Object-Centric Representation</strong>: Passing variable-length object token sequences directly to the controller is more effective than pixel-based tokenization.</li>\n<li><strong>Cross-Attention Conditioning</strong>: Stronger prompt focus and efficiency compared to simple concatenation (e.g., GPT-style).</li>\n<li><strong>Robustness</strong>: Minimal degradation under distractors or corrupted prompts, aided by T5 backbone and object augmentation.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"results\">Results<a href=\"https://gracefullight.dev/en/2025/08/31/vima-review/#results\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>\n<p><strong>Performance</strong>:</p>\n<ul>\n<li>Outperforms baselines (VIMA-Gato, VIMA-Flamingo, VIMA-GPT) by up to <strong>2.9× success rate</strong> in hardest zero-shot generalization.</li>\n<li>With <strong>10× less training data</strong>, still <strong>2.7× better</strong> than best competitor.</li>\n</ul>\n</li>\n<li>\n<p><strong>Scaling</strong>:</p>\n<ul>\n<li>Sample-efficient: with just <strong>1% of data</strong>, matches baselines trained with 10× more.</li>\n<li>Generalization holds across L1–L4 evaluation, with smaller regression than alternatives.</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"conclusion\">Conclusion<a href=\"https://gracefullight.dev/en/2025/08/31/vima-review/#conclusion\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<p>VIMA demonstrates that multimodal prompting is a powerful unifying framework for robot learning.<br>\n<!-- -->It achieves strong scalability, data efficiency, and generalization, establishing a <strong>solid starting point for future generalist robot agents</strong>.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"ref\">Ref<a href=\"https://gracefullight.dev/en/2025/08/31/vima-review/#ref\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A., Zhu, Y., &amp; Fan, L. (2023). VIMA: Robot Manipulation with Multimodal Prompts Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v202/jiang23b.html</code></li>\n</ul>",
            "url": "https://gracefullight.dev/en/2025/08/31/vima-review/",
            "title": "Vima Review",
            "summary": "Vima review",
            "date_modified": "2025-08-31T10:20:03.726Z",
            "author": {
                "name": "Gracefullight",
                "url": "https://github.com/gracefullight"
            },
            "tags": [
                "vlm"
            ]
        },
        {
            "id": "https://gracefullight.dev/en/2025/08/31/roboflamingo-review/",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"roboflamingo\">RoboFlamingo<a href=\"https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#roboflamingo\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>RoboFlamingo <strong>decouples vision-language understanding and control</strong>, using OpenFlamingo for perception and a lightweight policy head for sequential decision-making.</li>\n<li>Unlike prior VLM-based approaches, it requires only <strong>small-scale imitation fine-tuning</strong> on language-conditioned manipulation data, without large-scale co-fine-tuning.</li>\n<li>This design enables <strong>data-efficient, zero-shot generalizable, and deployable</strong> robot manipulation policies on modest compute resources.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"key-idea\">Key Idea<a href=\"https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#key-idea\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Proposes <strong>RoboFlamingo</strong>, a simple framework to adapt existing VLMs for robotic manipulation with lightweight fine-tuning.</li>\n<li>Built on <strong>OpenFlamingo</strong>, decoupling <strong>vision-language understanding</strong> from <strong>decision-making</strong>.</li>\n<li>Pre-trained VLM handles <strong>language and visual comprehension</strong>, while a dedicated <strong>policy head models sequential history</strong>.</li>\n<li>Fine-tuned only on <strong>language-conditioned manipulation datasets</strong> using imitation learning.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"advantages\">Advantages<a href=\"https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#advantages\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Requires only a <strong>small amount of demonstrations</strong> to adapt to downstream manipulation tasks.</li>\n<li>Provides <strong>open-loop control</strong> capability → deployable on low-performance platforms.</li>\n<li>Can be trained/evaluated on a <strong>single GPU server</strong>, making it a cost-effective and accessible solution.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"benchmarks\">Benchmarks<a href=\"https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#benchmarks\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Evaluated on <strong>CALVIN benchmark</strong> (34 tasks, 1000 instruction chains).</li>\n<li>RoboFlamingo achieves <strong>2× performance improvements</strong> over previous state-of-the-art methods.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"performance\">Performance<a href=\"https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#performance\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Imitation Learning</strong>: Outperforms all baselines across all metrics.</li>\n<li><strong>Zero-shot Generalization</strong>:<!-- -->\n<ul>\n<li><strong>Vision</strong>: Stronger generalization in ABC→D setting.</li>\n<li><strong>Language</strong>: Robust to GPT-4 generated synonymous instructions.</li>\n</ul>\n</li>\n<li><strong>Ablation Studies</strong>:<!-- -->\n<ul>\n<li>Ignoring history (MLP w/o hist) gives worst results.</li>\n<li>LSTM and GPT-based policy heads perform best (LSTM chosen as default).</li>\n<li><strong>VL pre-training</strong> is crucial for downstream manipulation.</li>\n<li><strong>Larger VLMs</strong> show better data efficiency.</li>\n<li><strong>Instruction fine-tuning</strong> improves both seen and unseen tasks.</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"flexibility-of-deployment\">Flexibility of Deployment<a href=\"https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#flexibility-of-deployment\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Supports <strong>open-loop control</strong> by predicting entire action sequences with a single inference → reduces latency and test-time compute.</li>\n<li>Direct open-loop use without retraining can degrade performance; mitigated with <strong>jump-step demonstrations</strong>.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"conclusion\">Conclusion<a href=\"https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#conclusion\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Demonstrates that pre-trained VLMs enable <strong>data efficiency</strong> and strong <strong>zero-shot generalization</strong> in robotic manipulation.</li>\n<li>RoboFlamingo is presented as an <strong>intuitive, efficient, and open solution</strong>, with high potential when combined with large-scale real robot data.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"ref\">Ref<a href=\"https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#ref\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Li, X., Liu, M., Zhang, H., Yu, C., Xu, J., Wu, H., Cheang, C., Jing, Y., Zhang, W., &amp; Liu, H. (2024). Vision-language foundation models as effective robot imitators. International Conference on Learning Representations (ICLR 2024), Vienna, Austria.</li>\n</ul>",
            "url": "https://gracefullight.dev/en/2025/08/31/roboflamingo-review/",
            "title": "RoboFlamingo Review",
            "summary": "RoboFlamingo Review",
            "date_modified": "2025-08-31T05:35:06.415Z",
            "author": {
                "name": "Gracefullight",
                "url": "https://github.com/gracefullight"
            },
            "tags": [
                "vlm"
            ]
        },
        {
            "id": "https://gracefullight.dev/en/2025/08/29/open-vla-review/",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"openvla\">OpenVLA<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#openvla\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>OpenVLA is a 7B open-source VLA model built on Llama2 + DINOv2 + SigLIP, trained on 970k demos, achieving stronger generalization and robustness than closed RT-2-X (55B) and outperforming Diffusion Policy.</li>\n<li>It introduces efficient adaptation via LoRA (1.4% params, 8× compute reduction) and 4-bit quantization (half memory, same accuracy), enabling fine-tuning and inference on consumer GPUs.</li>\n<li>Limitations remain (single-image input, <code>&lt;90%</code> reliability, limited throughput), but OpenVLA provides the first open, scalable framework for generalist robot policies.</li>\n</ul>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"OpenVLA Architecture\" src=\"https://gracefullight.dev/en/assets/images/open-vla-architecture-df4421cc82c1ebceca0ccf1061cd4593.png\" width=\"915\" height=\"366\" class=\"img_f7zd\"></p>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"motivation\">Motivation<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#motivation\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Training robot policies from scratch struggles with robustness and generalization.</li>\n<li>Fine-tuning <strong>vision-language-action (VLA)</strong> models offers reusable, generalizable visuomotor policies.</li>\n<li>Barriers: prior VLAs are <strong>closed-source</strong>, lack best practices for adaptation, and need server-class hardware.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"model--training\">Model &amp; Training<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#model--training\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>OpenVLA</strong>: 7B parameters, open-source.</li>\n<li>Built on <strong>Llama 2</strong> with fused <strong>DINOv2 + SigLIP</strong> vision encoders.</li>\n<li>Trained on <strong>970k robot demonstrations</strong> from Open-X Embodiment dataset.</li>\n<li>Represents robot actions as <strong>tokens</strong> (discretized into 256 bins, replacing unused Llama tokens).</li>\n<li>Standard <strong>next-token prediction</strong> objective.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"architecture--approach\">Architecture &amp; Approach<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#architecture--approach\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>End-to-end fine-tuning of VLM to generate robot actions as tokens.</li>\n<li>Differs from modular methods (e.g., Octo) that stitch separate encoders/decoders.</li>\n<li>Vision features are obtained by encoding the same input image with both SigLIP and DINOv2, then channel-wise concatenated and passed through an MLP projector. This preserves SigLIP’s semantic alignment with language and DINOv2's spatial reasoning, giving the VLM richer multimodal context for manipulation tasks.</li>\n<li>Uses Prismatic VLM backbone with multi-resolution features (spatial reasoning + semantics).</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"performance\">Performance<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#performance\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Outperforms closed <strong>RT-2-X (55B)</strong> by <strong>+16.5% task success</strong> with 7× fewer parameters.</li>\n<li>Beats <strong>Diffusion Policy</strong> (from-scratch imitation learning) by <strong>+20.4%</strong> on multi-task language-grounded settings.</li>\n<li>Demonstrates <strong>robust behaviors</strong> (distractor resistance, error recovery).</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"efficiency\">Efficiency<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#efficiency\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Introduces <strong>parameter-efficient fine-tuning</strong>:<!-- -->\n<ul>\n<li><strong>LoRA</strong> updates only 1.4% of parameters yet matches full fine-tuning.</li>\n<li>Can fine-tune on a single A100 GPU in ~10–15 hours (8× compute reduction).</li>\n</ul>\n</li>\n<li><strong>Quantization</strong>:<!-- -->\n<ul>\n<li>4-bit inference matches bfloat16 accuracy while halving memory footprint.</li>\n<li>Runs at 3Hz on consumer GPUs (e.g., A5000, 16GB).</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"evaluations\">Evaluations<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#evaluations\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Tested across <strong>29 tasks</strong> and multiple robots (WidowX, Google robot, Franka).</li>\n<li>Strong generalization on:<!-- -->\n<ul>\n<li><strong>Visual</strong> (unseen backgrounds/distractors).</li>\n<li><strong>Motion</strong> (new object positions/orientations).</li>\n<li><strong>Physical</strong> (new object shapes/sizes).</li>\n<li><strong>Semantic</strong> (unseen tasks, instructions).</li>\n</ul>\n</li>\n<li>First generalist open-source VLA achieving <strong>≥50% success rate across all tested tasks</strong>.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"design-insights\">Design Insights<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#design-insights\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Fine-tuning the vision encoder</strong> (vs. freezing) crucial for robotic control.</li>\n<li>Higher image resolution (384px vs. 224px) adds 3× compute without performance gains.</li>\n<li>Training required <strong>27 epochs</strong>, far more than typical VLM runs, to surpass 95% action token accuracy.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"limitations--future-work\">Limitations &amp; Future Work<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#limitations--future-work\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Supports only <strong>single-image observations</strong> (no proprioception, no history).</li>\n<li>Inference throughput (~6Hz on RTX 4090) insufficient for high-frequency control (e.g., ALOHA at 50Hz).</li>\n<li>Success rates remain below 90% in challenging tasks.</li>\n<li>Open questions:<!-- -->\n<ul>\n<li>Impact of base VLM size on performance.</li>\n<li>Benefits of co-training with Internet-scale data.</li>\n<li>Best visual features for VLAs.</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"contributions\">Contributions<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#contributions\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ol>\n<li>First <strong>open-source generalist VLA</strong> with strong performance.</li>\n<li>Scalable <strong>end-to-end training</strong> pipeline (action-as-token).</li>\n<li>Demonstrates <strong>LoRA + quantization</strong> for consumer-grade GPU adaptation.</li>\n<li>Provides <strong>code, checkpoints, and data curation recipes</strong> to support future research.</li>\n</ol>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"ref\">Ref<a href=\"https://gracefullight.dev/en/2025/08/29/open-vla-review/#ref\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E. P., Sanketi, P. R., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake, R., Sadigh, D., Levine, S., Liang, P., &amp; Finn, C. (2025). OpenVLA: An Open-Source Vision-Language-Action Model Proceedings of The 8th Conference on Robot Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v270/kim25c.html</code></li>\n</ul>",
            "url": "https://gracefullight.dev/en/2025/08/29/open-vla-review/",
            "title": "OpenVLA Review",
            "summary": "OpenVLA Review",
            "date_modified": "2025-08-29T07:43:25.796Z",
            "author": {
                "name": "Gracefullight",
                "url": "https://github.com/gracefullight"
            },
            "tags": [
                "vlm"
            ]
        },
        {
            "id": "https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"color-legend\">Color Legend<a href=\"https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#color-legend\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<table><thead><tr><th>Icon</th><th>Category</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td>🟡</td><td>Distribution</td><td>분포를 보여주고 싶을 때</td><td>Histogram, Density plot</td></tr><tr><td>⚫</td><td>Correlation</td><td>상관관계를 보여주고 싶을 때</td><td>Scatterplot, Correlogram</td></tr><tr><td>🟢</td><td>Ranking</td><td>순위를 보여주고 싶을 때</td><td>Bar chart, Lollipop chart</td></tr><tr><td>🔴</td><td>Part of a whole</td><td>전체 중 일부를 보여주고 싶을 때</td><td>Pie chart, Treemap</td></tr><tr><td>🔵</td><td>Evolution</td><td>시간에 따른 변화를 보여주고 싶을 때</td><td>Line chart, Area chart</td></tr><tr><td>🟣</td><td>Maps</td><td>지도를 활용해서 공간적 정보를 보여줄 때</td><td>Choropleth map, Bubble Map</td></tr><tr><td>🟤</td><td>Flow</td><td>흐름(흐름도, 이동 경로 등)을 보여줄 때</td><td>Flow map, Sankey-like</td></tr></tbody></table>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"categoric\">Categoric<a href=\"https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#categoric\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>One Variable<!-- -->\n<ul>\n<li>⚫ Waffle</li>\n<li>🟢 Bar Plot</li>\n<li>🟢 Lollipop</li>\n<li>🟢 Word Cloud</li>\n<li>🔴 Circular Packing</li>\n<li>🔴 Doughnut</li>\n<li>🔴 Pie</li>\n<li>🔴 Treemap</li>\n</ul>\n</li>\n<li>Two or More Variables<!-- -->\n<ul>\n<li>Two Independent Lists<!-- -->\n<ul>\n<li>🔴 Venn Diagram</li>\n</ul>\n</li>\n<li>Nested<!-- -->\n<ul>\n<li>🟢 Bar Plot</li>\n<li>🔴 Circular Packing</li>\n<li>🔴 Dendrogram</li>\n<li>🔴 Sunburst</li>\n<li>🔴 Treemap</li>\n</ul>\n</li>\n<li>Subgroup<!-- -->\n<ul>\n<li>⚫ Grouped Scatter</li>\n<li>⚫ Heatmap</li>\n<li>🟢 Lollipop</li>\n<li>🟢 Parallel Plot</li>\n<li>Spider</li>\n<li>🔴 Grouped Bar Plot</li>\n<li>🔴 Grouped Bar Plot</li>\n<li>🟤 Sankey Diagram</li>\n</ul>\n</li>\n<li>Adjacency<!-- -->\n<ul>\n<li>🟤 Arc</li>\n<li>🟤 Chord</li>\n<li>🟤 Network</li>\n<li>🟤 Sankey</li>\n<li>⚫ Heatmap</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"relational\">Relational<a href=\"https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#relational\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>Network<!-- -->\n<ul>\n<li>⚫ Heatmap</li>\n<li>🟢 Hive</li>\n<li>🟤 Arc</li>\n<li>🟤 Chord</li>\n<li>🟤 Network</li>\n<li>🟤 Sankey</li>\n</ul>\n</li>\n<li>Nested<!-- -->\n<ul>\n<li>No Value<!-- -->\n<ul>\n<li>🔴 Circular Packing</li>\n<li>🔴 Dendrogram</li>\n<li>🔴 Sunburst</li>\n<li>🔴 Treemap</li>\n<li>🟤 Sankey</li>\n</ul>\n</li>\n<li>Value for Leaf<!-- -->\n<ul>\n<li>🔴 Circular Packing</li>\n<li>🔴 Dendrogram</li>\n<li>🔴 Sunburst</li>\n<li>🔴 Treemap</li>\n<li>🟤 Sankey</li>\n</ul>\n</li>\n<li>Value for Edges<!-- -->\n<ul>\n<li>🔴 Dendrogram</li>\n<li>🟤 Chord</li>\n<li>🟤 Sankey</li>\n</ul>\n</li>\n<li>Value for Connection<!-- -->\n<ul>\n<li>Edge Bundling</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"map\">Map<a href=\"https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#map\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>🟣 Bubble Map</li>\n<li>🟣 Choropleth</li>\n<li>🟣 Connected Map</li>\n<li>🟣 Map</li>\n<li>🟣 Map Hexbin</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"time-series\">Time Series<a href=\"https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#time-series\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>One Series<!-- -->\n<ul>\n<li>🟡 Box Plot</li>\n<li>🟡 Violin</li>\n<li>🟡 Ridge Line</li>\n<li>🔵 Area</li>\n<li>🔵 Line Plot</li>\n<li>🟢 Bar Plot</li>\n<li>🟢 Lollipop</li>\n</ul>\n</li>\n<li>Several Series<!-- -->\n<ul>\n<li>🟡 Box Plot</li>\n<li>🟡 Violin</li>\n<li>🟡 Ridge Line</li>\n<li>⚫ Heatmap</li>\n<li>🔵 Line Plot</li>\n<li>🔵 Stacked Area</li>\n<li>🔵 Stream Graph</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"categoric-and-numeric\">Categoric and Numeric<a href=\"https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#categoric-and-numeric\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>One Numeric + One Categoric<!-- -->\n<ul>\n<li>One Observation, per Group<!-- -->\n<ul>\n<li>⚫ Waffle</li>\n<li>🟢 Bar Plot</li>\n<li>🟢 Lollipop</li>\n<li>🟢 Word Cloud</li>\n<li>🔴 Circular Packing</li>\n<li>🔴 Doughnut</li>\n<li>🔴 Pie</li>\n<li>🔴 Treemap</li>\n</ul>\n</li>\n<li>Several Observations, per Group<!-- -->\n<ul>\n<li>🟡 Box Plot</li>\n<li>🟡 Violin</li>\n<li>🟡 Ridge Line</li>\n<li>🟡 Density</li>\n<li>🟡 Histogram</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>One Category, Several Numeric<!-- -->\n<ul>\n<li>No Order<!-- -->\n<ul>\n<li>🟡 Box Plot</li>\n<li>🟡 Violin</li>\n<li>⚫ Grouped Scatter</li>\n<li>⚫ 2D Density</li>\n<li>⚫ PCA</li>\n<li>⚫ Correlogram</li>\n</ul>\n</li>\n<li>A Numeric is Ordered<!-- -->\n<ul>\n<li>⚫ Connected Scatter</li>\n<li>🔵 Area</li>\n<li>🔵 Line Plot</li>\n<li>🔵 Stacked Area</li>\n<li>🔵 Stream Graph</li>\n</ul>\n</li>\n<li>One Value Per Group<!-- -->\n<ul>\n<li>⚫ Grouped Scatter</li>\n<li>⚫ Heatmap</li>\n<li>🟢 Lollipop</li>\n<li>🟢 Parallel Plot</li>\n<li>🟢 Spider Plot</li>\n<li>🔴 Grouped Bar Plot</li>\n<li>🔴 Grouped Bar Plot</li>\n<li>🟤 Sankey Diagram</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Several Categories, One Numeric<!-- -->\n<ul>\n<li>Subgroup<!-- -->\n<ul>\n<li>One Observation. per Group<!-- -->\n<ul>\n<li>⚫ Grouped Scatter</li>\n<li>⚫ Heatmap</li>\n<li>🟢 Lollipop</li>\n<li>🟢 Parallel Plot</li>\n<li>🟢 Spider Plot</li>\n<li>🔴 Grouped Bar Plot</li>\n<li>🔴 Grouped Bar Plot</li>\n<li>🟤 Sankey Diagram</li>\n</ul>\n</li>\n<li>Several Observations, per Group<!-- -->\n<ul>\n<li>🟡 Box Plot</li>\n<li>🟡 Violin</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Nested<!-- -->\n<ul>\n<li>One Observation. per Group<!-- -->\n<ul>\n<li>🟢 Bar Plot</li>\n<li>🔴 Circular Packing</li>\n<li>🔴 Dendrogram</li>\n<li>🔴 Sunburst</li>\n<li>🔴 Treemap</li>\n</ul>\n</li>\n<li>Several Observations. per Group<!-- -->\n<ul>\n<li>🟡 Box Plot</li>\n<li>🟡 Violin</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Adjacency<!-- -->\n<ul>\n<li>⚫ Heatmap</li>\n<li>🟤 Arc</li>\n<li>🟤 Chord</li>\n<li>🟤 Network</li>\n<li>🟤 Sankey</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"numeric\">Numeric<a href=\"https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#numeric\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>One Numeric Variable<!-- -->\n<ul>\n<li>🟡 Density</li>\n<li>🟡 Histogram</li>\n</ul>\n</li>\n<li>Two Numeric Variables<!-- -->\n<ul>\n<li>Not Ordered<!-- -->\n<ul>\n<li>Few Points<!-- -->\n<ul>\n<li>🟡 Box Plot</li>\n<li>🟡 Histogram</li>\n<li>⚫ Scatter Plot</li>\n</ul>\n</li>\n<li>Many Points<!-- -->\n<ul>\n<li>🟡 Density</li>\n<li>🟡 Violin</li>\n<li>⚫ 2D Density</li>\n<li>🔵 Marginal Distribution</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Ordered<!-- -->\n<ul>\n<li>⚫ Connected Scatter</li>\n<li>🔵 Area Plot</li>\n<li>🔵 Line Plot</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Three Numeric Variables<!-- -->\n<ul>\n<li>Not Ordered<!-- -->\n<ul>\n<li>🟡 Box Plot</li>\n<li>🟡 Violin</li>\n<li>⚫ Bubble Plot</li>\n<li>⚫ 3d Scatter or Surface</li>\n</ul>\n</li>\n<li>Ordered<!-- -->\n<ul>\n<li>🔵 Area</li>\n<li>🔵 Line Plot</li>\n<li>🔵 Stacked Area</li>\n<li>🔵 Stream Graph</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Several Numeric Variables<!-- -->\n<ul>\n<li>Ordered<!-- -->\n<ul>\n<li>🔵 Area</li>\n<li>🔵 Line Plot</li>\n<li>🔵 Stacked Area</li>\n<li>🔵 Stream Graph</li>\n</ul>\n</li>\n<li>Not Ordered<!-- -->\n<ul>\n<li>🟡 Box Plot</li>\n<li>🟡 Ridge Line</li>\n<li>🟡 Violin</li>\n<li>⚫ Correlogram</li>\n<li>⚫ Heatmap</li>\n<li>⚫ PCA</li>\n<li>🔴 Dendrogram</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"ref\">Ref<a href=\"https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#ref\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><a href=\"https://www.data-to-viz.com/\" target=\"_blank\" rel=\"noopener noreferrer\">from Data to Viz</a></li>\n</ul>",
            "url": "https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/",
            "title": "데이터 시각화 의사 결정 트리",
            "summary": "데이터 시각화 의사 결정 트리",
            "date_modified": "2025-08-28T10:48:43.168Z",
            "author": {
                "name": "Gracefullight",
                "url": "https://github.com/gracefullight"
            },
            "tags": [
                "data"
            ]
        },
        {
            "id": "https://gracefullight.dev/en/vocab/vocab-ai-006/",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"vocabulary--expressions\">Vocabulary &amp; Expressions<a href=\"https://gracefullight.dev/en/vocab/vocab-ai-006/#vocabulary--expressions\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<table><thead><tr><th>Term/Expression</th><th>Definition</th><th>Simpler Paraphrase</th><th>Meaning</th></tr></thead><tbody><tr><td>prevalence</td><td>The state of being widespread or common</td><td>Commonness</td><td>유행, 널리 퍼짐</td></tr><tr><td>instantiation</td><td>The act of creating a specific instance of something</td><td>Creation of a specific example</td><td>구체적인 값의 생성</td></tr><tr><td>triviality</td><td>The quality of being trivial or unimportant</td><td>Unimportance</td><td>사소함, 하찮음</td></tr><tr><td>intermediary</td><td>A person or thing that acts as a link between two others</td><td>Middleman</td><td>중개자, 매개체</td></tr><tr><td>dreaded</td><td>Regarded with great fear or apprehension</td><td>Feared</td><td>두려운, 걱정되는</td></tr><tr><td>i.i.d.</td><td>Independent and identically distributed</td><td>Same distribution, no dependence</td><td>독립적이고 동일한 분포</td></tr><tr><td>analogous</td><td>Similar in some way</td><td>Comparable</td><td>유사한, 비슷한</td></tr><tr><td>posteriori</td><td>Relating to knowledge gained through experience or empirical evidence</td><td>Based on observation</td><td>경험적, 관찰에 기초한</td></tr><tr><td>posterior</td><td>Relating to the back or rear</td><td>Back</td><td>뒤쪽의, 후방의</td></tr><tr><td>resemblance</td><td>The state of resembling or being alike</td><td>Similarity</td><td>유사성, 닮음</td></tr><tr><td>stipulate</td><td>To demand or specify a requirement</td><td>Specify</td><td>규정하다, 명시하다</td></tr><tr><td>rectify</td><td>To correct or make right</td><td>Correct</td><td>수정하다, 바로잡다</td></tr><tr><td>schematic</td><td>Relating to a diagram or representation</td><td>Diagrammatic</td><td>도식적인, 다이어그램의</td></tr><tr><td>proposition</td><td>A statement or assertion that expresses a judgment or opinion</td><td>Proposal</td><td>제안, 명제</td></tr><tr><td>cavity</td><td>A cavity is a hollow place in a tooth caused by decay</td><td>Tooth decay</td><td>충치</td></tr><tr><td>tautological</td><td>Relating to or involving tautology (the saying of the same thing twice in different words)</td><td>Redundant</td><td>동의어 반복의, 중복적인</td></tr><tr><td>retrospectively</td><td>Looking back on or dealing with past events or situations</td><td>Looking back</td><td>회고적으로, 과거를 돌아보며</td></tr><tr><td>perturbations</td><td>Disturbances or deviations from a normal state</td><td>Disturbances</td><td>교란, 변동</td></tr><tr><td>deformable</td><td>Capable of being changed in shape or form</td><td>Changeable</td><td>변형 가능한</td></tr><tr><td>Consolidation</td><td>The process of combining multiple elements into a single, more effective whole</td><td>Integration</td><td>통합</td></tr><tr><td>Oscillation</td><td>Fluctuation or variation in a state or condition</td><td>Fluctuation</td><td>진동, 변동</td></tr></tbody></table>",
            "url": "https://gracefullight.dev/en/vocab/vocab-ai-006/",
            "title": "Vocabulary for AI @006",
            "summary": "Vocabulary for AI @006",
            "date_modified": "2025-08-28T06:13:52.660Z",
            "author": {
                "name": "Gracefullight",
                "url": "https://github.com/gracefullight"
            },
            "tags": [
                "vocab"
            ]
        },
        {
            "id": "https://gracefullight.dev/en/2025/08/28/developing-ml-systems/",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"problem-formulation-문제-정의\">Problem formulation (문제 정의)<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#problem-formulation-%EB%AC%B8%EC%A0%9C-%EC%A0%95%EC%9D%98\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>The first step is to figure out what problem you want to solve.</strong>\n<ol>\n<li>“사용자에게 어떤 문제를 해결해주고 싶은가?” → 모호하지 않고 구체적으로 정의해야 함.</li>\n<li>“그 문제 중 어떤 부분을 머신러닝으로 풀 수 있는가?” → 예: 사진을 라벨로 매핑하는 함수 학습.</li>\n</ol>\n</li>\n<li>이를 구체화하려면 ML 컴포넌트에 대해 <strong>loss function</strong> 을 지정해야 한다.</li>\n<li>문제를 쪼개보면 일부는 전통적 SW 엔지니어링으로 해결 가능하고, 일부만 ML로 다뤄야 할 수 있다.</li>\n<li>학습 유형은 지도·비지도·강화·준지도(semisupervised)까지 연속선상에 있음.<!-- -->\n<ul>\n<li><strong>Semisupervised learning</strong>: 일부 라벨만 활용해 비라벨 데이터에서 더 많은 정보 추출.</li>\n<li><strong>Weakly supervised learning</strong>: 부정확·노이즈 라벨을 사용.</li>\n</ul>\n</li>\n<li>결론: <strong>Noise와 label 부족은 “지도 ↔ 비지도” 사이의 연속체를 형성</strong>한다.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"data-collection--management-데이터-수집관리\">Data collection &amp; management (데이터 수집/관리)<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#data-collection--management-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91%EA%B4%80%EB%A6%AC\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>데이터는 직접 제작, 크라우드소싱, 사용자 행동에서 수집 가능.</li>\n<li>부족할 때는 <strong>transfer learning</strong> 활용.</li>\n<li><strong>Privacy 검토</strong>와 동의, 공정성, <strong>federated learning</strong> 등 고려 필요.</li>\n<li><strong>Data provenance</strong>(출처 관리): 데이터 정의, 값의 범위, 생성 주체, 중단 여부, 정의 변경 이력 등 추적 → 파이프라인 안정성이 알고리즘보다 중요.</li>\n<li>항상 자문: “이 데이터는 내 문제를 풀기에 적절한가? 입력과 출력 모두 충분히 담고 있는가?”</li>\n<li><strong>Learning curve</strong> 로 데이터 확장 효과/학습 plateau 확인.</li>\n<li>방어적 태도 필요: 입력 오류, 누락, 적대적 사용자, 철자 불일치 등 처리.</li>\n<li><strong>Data augmentation</strong> (회전, 이동, 노이즈 추가 등)으로 모델 강건성 향상.</li>\n<li>불균형 데이터는 <strong>undersampling, oversampling, SMOTE/ADASYN, boosting</strong> 등으로 완화.</li>\n<li>아웃라이어는 로그 변환 등으로 영향 축소, 트리 모델은 상대적으로 강건.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"feature-engineering-특징-엔지니어링\">Feature engineering (특징 엔지니어링)<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#feature-engineering-%ED%8A%B9%EC%A7%95-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li><strong>Quantization</strong>: 연속값을 구간(bin)으로 강제.</li>\n<li><strong>One-hot encoding</strong>: 범주형 속성을 다중 Boolean으로 변환.</li>\n<li>도메인 지식 기반 새 특성 추가 (예: 날짜 → 주말/공휴일 여부).</li>\n<li><strong>“At the end of the day, some ML projects succeed and some fail… the most important factor is the features used.” (Pedro Domingos)</strong></li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"exploratory-data-analysis-eda--visualization\">Exploratory data analysis (EDA) &amp; visualization<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#exploratory-data-analysis-eda--visualization\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>목표: 예측/검증이 아닌 <strong>데이터 이해</strong>.</li>\n<li><strong>Histograms, scatter plots</strong> 로 분포/결측/오류/이상치 확인.</li>\n<li>클러스터링 → 프로토타입 시각화, 이상치 탐지 (“고양이 vs 사자 옷 입은 고양이”).</li>\n<li>차원 축소 (예: <strong>t-SNE</strong>)로 고차 데이터를 2D/3D로 시각화.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"model-selection--training\">Model selection &amp; training<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#model-selection--training\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>데이터가 정리되면 모델 구축 단계.</li>\n<li><strong>Random forests</strong> → 범주형 특징 많고 일부 무관할 때.</li>\n<li><strong>Nonparametric methods</strong> → 데이터 많고 지식 부족, 특징 선택 고민 줄이고 싶을 때.</li>\n<li><strong>Logistic regression</strong> → 선형 분리 가능(또는 feature engineering 후).</li>\n<li><strong>SVM</strong> → 데이터 크기 작고 차원 높을 때.</li>\n<li><strong>Deep neural nets</strong> → 패턴 인식(이미지·음성).</li>\n<li>하이퍼파라미터는 경험 + 탐색으로 조율.</li>\n<li>검증 데이터 남용 시 <strong>validation overfitting</strong> 위험 → 여러 검증셋 필요.</li>\n<li>성능 평가: <strong>ROC curve, AUC, confusion matrix</strong>.</li>\n<li>중요한 건 <strong>아이디어–실험–검증 반복 사이클을 빠르게 하는 것</strong>.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"trust-interpretability-explainability\">Trust, interpretability, explainability<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#trust-interpretability-explainability\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>단순히 지표 성능만으로는 신뢰 부족 → 규제·언론·사용자도 신뢰성 원함.</li>\n<li><strong>Accountability:</strong> 오류 발생 시 책임 주체와 항소 절차 필요.</li>\n<li><strong>Interpretability:</strong> 모델 내부를 직접 이해 (트리, 선형회귀).<!-- -->\n<ul>\n<li>핵심 질문: <strong>“If I change x, how will the output change?”</strong></li>\n</ul>\n</li>\n<li><strong>Explainability:</strong> 블랙박스 모델 + 별도 모듈로 설명 (예: LIME).</li>\n<li>단순 설명이 잘못된 확신을 줄 수 있음. → 테스트와 실제 성능이 더 큰 신뢰를 준다.</li>\n<li>“안전하다고 설명만 있는 실험기 vs 100회 무사비행한 비행기” 비유.</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"operation-monitoring-maintenance\">Operation, monitoring, maintenance<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#operation-monitoring-maintenance\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<ul>\n<li>운영 단계에서는 <strong>롱테일 입력(long tail)</strong> 문제 등장 → 예상 못한 입력 지속 발생. → 실시간 모니터링과 사람 평가자 필요.</li>\n<li><strong>Nonstationarity:</strong> 세상과 사용자 행동 변화 → 최신 데이터 vs 안정적 모델 트레이드오프.</li>\n<li>신선도 요구 다름: 어떤 문제는 매일/매시간 새 모델, 어떤 문제는 수개월 동일 모델.</li>\n<li>배포 자동화 → 작은 변경은 자동 승인, 큰 변경은 리뷰.</li>\n<li><strong>Online vs Offline model</strong>: 기존 모델 점진적 수정 vs 매번 처음부터 재학습.</li>\n<li>데이터 자체가 바뀔 수도 있음 (스팸 이메일 → 스팸 문자, 음성, 영상 등).</li>\n</ul>\n<h2 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"checklist\">Checklist<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#checklist\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h2>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"tests-for-features-and-data\">Tests for Features and Data<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#tests-for-features-and-data\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<ul class=\"contains-task-list containsTaskList_cTFB\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Feature expectations are captured in a schema.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->All features are beneficial.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->No feature’s cost is too much.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Features adhere to meta-level requirements.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->The data pipeline has appropriate privacy controls.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->New features can be added quickly.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->All input feature code is tested.</li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"tests-for-model-development\">Tests for Model Development<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#tests-for-model-development\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<ul class=\"contains-task-list containsTaskList_cTFB\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Every model specification undergoes a code review.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Every model is checked in to a repository.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Offline proxy metrics correlate with actual metrics.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->All hyperparameters have been tuned.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->The impact of model staleness is known.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->A simpler model is not better.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Model quality is sufficient on all important data slices.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->The model has been tested for considerations of inclusion.</li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"tests-for-machine-learning-infrastructure\">Tests for Machine Learning Infrastructure<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#tests-for-machine-learning-infrastructure\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<ul class=\"contains-task-list containsTaskList_cTFB\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Training is reproducible.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Model specification code is unit tested.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->The full ML pipeline is integration tested.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Model quality is validated before attempting to serve it.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->The model allows debugging by observing the step-by-step computation of training or inference on a single example.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Models are tested via a canary process before they enter production serving environments.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Models can be quickly and safely rolled back to a previous serving version.</li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"monitoring-tests-for-machine-learning\">Monitoring Tests for Machine Learning<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#monitoring-tests-for-machine-learning\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<ul class=\"contains-task-list containsTaskList_cTFB\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Dependency changes result in notification.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Data invariants hold in training and serving inputs.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Training and serving features compute the same values.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->Models are not too stale.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->The model is numerically stable.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->The model has not experienced regressions in training speed, serving latency, throughput, or RAM usage.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled=\"\"> <!-- -->The model has not experienced a regression in prediction quality on served data.</li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_IhMp\" id=\"ref\">Ref<a href=\"https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#ref\" class=\"hash-link\" aria-label=\"Direct link to heading\" title=\"Direct link to heading\">​</a></h3>\n<ul>\n<li>Breck, E., Cai, S., Nielsen, E., Salib, M., &amp; Sculley, D. (2016). What’s your ML test score? A rubric for ML production systems. NIPS Workshop on Reliable Machine Learning in the Wild.</li>\n</ul>",
            "url": "https://gracefullight.dev/en/2025/08/28/developing-ml-systems/",
            "title": "Developing ML Systems",
            "summary": "Developing ML Systems",
            "date_modified": "2025-08-28T06:04:52.968Z",
            "author": {
                "name": "Gracefullight",
                "url": "https://github.com/gracefullight"
            },
            "tags": [
                "ai"
            ]
        }
    ]
}