<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="rss.xsl"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>gracefullight.dev Blog</title>
        <link>https://gracefullight.dev/en/</link>
        <description>gracefullight.dev Blog</description>
        <lastBuildDate>Tue, 02 Sep 2025 02:56:15 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Introduction to AI @005]]></title>
            <link>https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/</link>
            <guid>https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/</guid>
            <pubDate>Tue, 02 Sep 2025 02:56:15 GMT</pubDate>
            <description><![CDATA[Introduction to AI @005]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_IhMp" id="neural-network-development-history">Neural Network Development History<a href="https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#neural-network-development-history" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>1950s-1960s: Early Foundations<!-- -->
<ul>
<li>McCulloch &amp; Pitts (1943): mathematical neuron model</li>
<li>Rosenblatt’s Perceptron (1958): first trainable network</li>
<li>Minsky &amp; Papert (1969): limitations (XOR problem) → AI Winter</li>
</ul>
</li>
<li>1970s–1980s: First Revival<!-- -->
<ul>
<li>Werbos (1974); Rumelhart, Hinton, Williams (1986): Backpropagation</li>
<li>Hopfield Networks (1982): associative memory</li>
<li>Renewed optimism but limited by hardware</li>
</ul>
</li>
<li>1990s: Consolidation<!-- -->
<ul>
<li>LeCun’s CNN (LeNet, 1989): digit recognition</li>
<li>Elman, Jordan: Recurrent Neural Networks</li>
<li>Symbolic AI still dominated mainstream</li>
</ul>
</li>
<li>2000s: Deep Learning Foundations<!-- -->
<ul>
<li>Better hardware (GPUs) + large datasets</li>
<li>Hinton (2006): Deep Belief Networks (unsupervised pretraining)</li>
<li>Connectionism regains attention</li>
</ul>
</li>
<li>2010s: Deep Learning Boom<!-- -->
<ul>
<li>ImageNet (2012): AlexNet breakthrough</li>
<li>RNNs, LSTMs, GRUs → speech &amp; translation</li>
<li>Transformers (2017): revolutionized NLP</li>
</ul>
</li>
<li>2020s: Scaling &amp; Foundation Models<!-- -->
<ul>
<li>Large Language Models (GPT, BERT, etc.)</li>
<li>Multimodal AI: vision, text, speech integration</li>
<li>Connectionism dominates AI research &amp; industry</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="neural-network-models">Neural Network Models<a href="https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#neural-network-models" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>a collection of units (neurons) connected together</li>
<li>The properties of the network are determined by its topology and the properties of the neurons.</li>
<li>Roughly speaking, the neuron fires when a linear combination of its inputs exceeds some (hard or soft) threshold.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="Simple Neuron" src="https://gracefullight.dev/en/assets/images/simple-neuron-11bdb8ee67568f5375608d2d70af0bce.png" width="3010" height="1427" class="img_f7zd"></p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><msub><mi>n</mi><mi>j</mi></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></msubsup><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">in_j = \sum_{i=0}^{n} w_{ij}a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9456em;vertical-align:-0.2861em"></span><span class="mord mathnormal">i</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.104em;vertical-align:-0.2997em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mi>u</mi><msub><mi>t</mi><mi>j</mi></msub><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mi>i</mi><msub><mi>n</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">out_j = g(in_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9012em;vertical-align:-0.2861em"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>j</mi></msub><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></msubsup><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a_j = g(\sum_{i=0}^{n} w_{ij} a_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.104em;vertical-align:-0.2997em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="activation-function">Activation function<a href="https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#activation-function" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="relu-function">ReLU function<a href="https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#relu-function" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">ReLU(x) = max(0, x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10903em">LU</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></p>
<ul>
<li>an abbreviation for rectified linear unit</li>
<li>Commonly used</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="softplus-function">Softplus function<a href="https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#softplus-function" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>p</mi><mi>l</mi><mi>u</mi><mi>s</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mi>x</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Softplus(x) = \log(1 + e^x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord mathnormal" style="margin-right:0.01968em">tpl</span><span class="mord mathnormal">u</span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<ul>
<li>A smooth version of the ReLU function</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="logistic-or-sigmoid-function">Logistic or Sigmoid function<a href="https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#logistic-or-sigmoid-function" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>c</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">Logistic(x) = \frac{1}{1 + e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">c</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2484em;vertical-align:-0.4033em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7027em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<ul>
<li>Non-linear, can represent a nonlinear function</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="tanh-function">Tanh function<a href="https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#tanh-function" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><mn>2</mn><mi>x</mi></mrow></msup><mo>−</mo><mn>1</mn></mrow><mrow><msup><mi>e</mi><mrow><mn>2</mn><mi>x</mi></mrow></msup><mo>+</mo><mn>1</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">tanh(x) = \frac{e^{2x} -1}{e^{2x} + 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">anh</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.4213em;vertical-align:-0.4033em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0179em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="topology-of-a-neural-network">Topology of a neural network<a href="https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#topology-of-a-neural-network" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Feed-forward network (FFN):<!-- -->
<ul>
<li>Every node receives inputs from "upstream" nodes and delivers output to "downstream" nodes.</li>
<li>There are no loops.</li>
<li>FFN represents a function of its current inputs, thus it has no internal state other than the weights themselves.</li>
</ul>
</li>
<li>Recurrent Network (RNN):<!-- -->
<ul>
<li>A recurrent network feeds its outputs back into its own inputs.</li>
<li>In a recurrent network, the neuron values can eventually settle down, keep cycling, or behave unpredictably.</li>
<li>can support short-term memory</li>
</ul>
</li>
</ul>
<table><thead><tr><th>FFN</th><th>RNN</th></tr></thead><tbody><tr><td><img decoding="async" loading="lazy" alt="FFN" src="https://gracefullight.dev/en/assets/images/ffn-ff75c9f4654345c4c880e672b8384331.png" width="454" height="436" class="img_f7zd"></td><td><img decoding="async" loading="lazy" alt="RNN" src="https://gracefullight.dev/en/assets/images/rnn-be95c2621b75fdfbb5f8bce837be8e37.png" width="454" height="472" class="img_f7zd"></td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="training-process">Training Process<a href="https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#training-process" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Go through each training sample.</li>
<li>If correctly classified → do nothing.</li>
<li>If misclassified → update the weights:</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>←</mo><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mi>α</mi><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i \leftarrow w_i + \alpha(y - \hat{y})x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="perceptron-for-binary-classification">Perceptron for Binary Classification<a href="https://gracefullight.dev/en/2025/09/02/introduction-to-ai-005/#perceptron-for-binary-classification" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>A perceptron separates data into two classes with a hyperplane.</li>
<li>if <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>⋅</mo><mi>x</mi><mo>≥</mo><mn>0</mn><mo>→</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w \cdot x \geq 0 \rightarrow 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7719em;vertical-align:-0.136em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></li>
<li>if <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>⋅</mo><mi>x</mi><mo>≤</mo><mn>0</mn><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">w \cdot x \le 0 \rightarrow 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7719em;vertical-align:-0.136em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span></li>
</ul>]]></content:encoded>
            <category>iai</category>
        </item>
        <item>
            <title><![CDATA[Open X-Embodiment review]]></title>
            <link>https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/</link>
            <guid>https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/</guid>
            <pubDate>Mon, 01 Sep 2025 03:47:28 GMT</pubDate>
            <description><![CDATA[Open X-Embodiment review]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_IhMp" id="rt-x">RT-X<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#rt-x" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RT-X trains generalist robot policies by co-training RT-1/RT-2 on an X-embodiment mix of multi-robot, multi-task data, enabling efficient adaptation to new robots, tasks, and environments.</li>
<li>It standardizes 1M+ trajectories from 22 embodiments into the Open X-Embodiment (RLDS/tfrecord) repository, unifying observations and 7-DoF actions via coarse alignment.</li>
<li>Experiments show strong positive transfer and emergent skills (≈3× with RT-2-X on cross-robot tasks); performance scales with model capacity, short image histories, and web pretraining, while sensing/actuation diversity and frame alignment remain open problems.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="RT-X Architecture" src="https://gracefullight.dev/en/assets/images/rt-x-architecture-cc2128128460577bc8f720626e0d671d.png" width="1552" height="412" class="img_f7zd"></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="motivation">Motivation<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#motivation" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Seeks a <strong>generalist X-robot policy</strong> that can be efficiently adapted to new robots, tasks, and environments.</li>
<li>Mirrors a trend from CV/NLP where <strong>general-purpose, web-scale pretrained models</strong> outperform narrow, task-specific models.</li>
<li>Robotics lacks comparably large, diverse <strong>interaction datasets</strong>, making direct transfer of these lessons challenging.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="objectives">Objectives<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#objectives" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li><strong>Positive transfer:</strong> Test whether co-training on data from many robots improves performance on each training domain.</li>
<li><strong>Ecosystem building:</strong> Organize large robotic datasets to enable future X-embodiment research.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="core-approach">Core Approach<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#core-approach" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Train <strong>RT-1</strong> and <strong>RT-2</strong> on data from <strong>9 different manipulators</strong>, producing <strong>RT-X</strong> variants that outperform policies trained only on the evaluation domain and show <strong>better generalization</strong> and <strong>new capabilities</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="whats-different-from-prior-transfer-methods">What’s Different From Prior Transfer Methods<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#whats-different-from-prior-transfer-methods" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Many prior works reduce the <strong>embodiment gap</strong> via specialized mechanisms (shared action spaces, representation learning objectives, policy adaptation using embodiment metadata, decoupled robot/environment representations, domain translation).</li>
<li><strong>RT-X directly trains on X-embodiment data without explicit gap-reduction machinery</strong> and still observes <strong>positive transfer</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="dataset--format-open-x-embodiment">Dataset &amp; Format (Open X-Embodiment)<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#dataset--format-open-x-embodiment" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>1M+ real robot trajectories, 22 embodiments</strong> (single-arm, bimanual, quadrupeds), pooled from <strong>60 datasets / 34 labs</strong>, standardized for easy use.</li>
<li>Uses <a href="https://github.com/google-research/rlds" target="_blank" rel="noopener noreferrer"><strong>RLDS</strong></a> (serialized <code>tfrecord</code>), supporting varied action spaces and input modalities (RGB, depth, point clouds), and efficient parallel loading across major DL frameworks.</li>
<li>Language annotations are leveraged; <strong>PaLM</strong> is used to extract objects/behaviors from instructions.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="RLDS" src="https://gracefullight.dev/en/assets/images/rlds-5e68d1c660ef048892d5594530c62239.png" width="726" height="353" class="img_f7zd"></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="data-format-consolidation-coarse-alignment">Data Format Consolidation (Coarse Alignment)<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#data-format-consolidation-coarse-alignment" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Observations:</strong> History of recent images + language instruction. One <strong>canonical camera view</strong> per dataset is resized to a common resolution.</li>
<li><strong>Actions:</strong> Convert original controls to a <strong>7-DoF end-effector vector</strong> (x, y, z, roll, pitch, yaw, gripper or their rates). Actions are <strong>normalized before discretization</strong>; outputs are <strong>de-normalized per embodiment</strong>.</li>
<li><strong>Deliberate non-alignment:</strong> Camera poses/properties are <strong>not</strong> standardized; action frame alignment across datasets is <strong>not</strong> enforced. The same action vector may cause <strong>different motions</strong> on different robots (absolute/relative, position/velocity allowed).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="policy-architectures">Policy Architectures<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#policy-architectures" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>RT-1 (≈35M params):</strong> Transformer for control. Inputs: 15-frame image history + natural-language instruction.<!-- -->
<ul>
<li>Vision via ImageNet-pretrained <strong>EfficientNet</strong>; language via <strong>USE</strong> embedding.</li>
<li>Fuse via <strong>FiLM</strong> → 81 vision–language tokens → <strong>decoder-only Transformer</strong> outputs tokenized actions.</li>
</ul>
</li>
<li><strong>RT-2 (VLA family):</strong> Internet-scale VLM co-fine-tuned to output <strong>action as text tokens</strong> (e.g., <code>1 128 91 241 5 101 127</code>).<!-- -->
<ul>
<li>Any pretrained VLM can be adapted; this work uses <strong>RT-2–PaLI-X</strong> (ViT backbone + UL2 LM; primarily pretrained on WebLI).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="training-setup">Training Setup<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#training-setup" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Robotics data mixture:</strong> Data from <strong>9 manipulators</strong> (a union of multiple well-known robotics datasets).</li>
<li><strong>Loss:</strong> Standard <strong>categorical cross-entropy</strong> over tokenized actions.</li>
<li><strong>Regimes:</strong>
<ul>
<li><strong>RT-1-X:</strong> Trained solely on the robotics mixture.</li>
<li><strong>RT-2-X:</strong> <strong>Co-fine-tuned</strong> on a ~1:1 mix of original VLM data and the robotics mixture.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="experimental-questions">Experimental Questions<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#experimental-questions" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li>Does X-embodiment co-training improve in-domain performance (positive transfer)?</li>
<li>Does it improve <strong>generalization</strong> to <strong>unseen tasks</strong>?</li>
<li>How do <strong>model size</strong>, <strong>architecture</strong>, and <strong>dataset composition</strong> influence performance/generalization?</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="key-results">Key Results<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#key-results" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Small-scale domains:</strong> <strong>RT-1-X</strong> outperforms the <strong>Original Method</strong> (the authors’ per-dataset baselines) on <strong>4/5</strong> datasets with a large average gain → <strong>limited data domains</strong> benefit greatly from X-embodiment co-training.</li>
<li><strong>Large-scale domains:</strong>
<ul>
<li><strong>RT-1-X</strong> does <strong>not</strong> beat an RT-1 trained only on the embodiment-specific large dataset (suggests underfitting for this class).</li>
<li><strong>RT-2-X</strong> (larger capacity) <strong>outperforms both</strong> Original Method and RT-1 → X-robot training helps even in <strong>data-rich</strong> regimes when using <strong>sufficient capacity</strong>.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="generalization--emergent-skills">Generalization &amp; Emergent Skills<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#generalization--emergent-skills" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Unseen objects/backgrounds/environments:</strong> RT-2 and RT-2-X perform <strong>on par</strong> (VLM backbone already strong here).</li>
<li><strong>Emergent skills (transfer across robots):</strong> On Google Robot tasks that <strong>do not appear</strong> in RT-2’s dataset but exist in <strong>Bridge</strong> (for <strong>WidowX</strong>), <strong>RT-2-X ≈ 3×</strong> RT-2.<!-- -->
<ul>
<li>Removing <strong>Bridge</strong> from RT-2-X training <strong>significantly reduces</strong> hold-out performance → skills likely <strong>transferred</strong> from WidowX data.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="design-insights-ablations">Design Insights (Ablations)<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#design-insights-ablations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Short image history</strong> notably <strong>improves generalization</strong>.</li>
<li><strong>Web pretraining</strong> is <strong>critical</strong> for large models’ high performance.</li>
<li><strong>Model capacity matters:</strong> <strong>55B</strong> model succeeds more than <strong>5B</strong> on emergent skills → greater capacity ⇒ greater cross-dataset transfer.</li>
<li><strong>Co-fine-tuning vs. fine-tuning:</strong> Similar performance in this study (attributed to the <strong>greater diversity</strong> of robotics data in RT-2-X vs. prior works).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="limitations-open-problems">Limitations (Open Problems)<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#limitations-open-problems" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Does <strong>not</strong> cover robots with <strong>very different sensing/actuation modalities</strong>.</li>
<li>Does <strong>not</strong> study generalization to <strong>new robots</strong> nor define a <strong>decision criterion</strong> for when positive transfer will occur.</li>
<li>Camera pose/properties and control frame <strong>remain unaligned</strong>; a deliberate but still challenging domain gap to address in future work.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="ref">Ref<a href="https://gracefullight.dev/en/2025/09/01/open-x-embodiment-review/#ref" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>O’Neill, A., Rehman, A., Maddukuri, A., Gupta, A., Padalkar, A., Lee, A., Pooley, A., Gupta, A., Mandlekar, A., &amp; Jain, A. (2024). Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. 2024 IEEE International Conference on Robotics and Automation (ICRA).</li>
</ul>]]></content:encoded>
            <category>vlm</category>
        </item>
        <item>
            <title><![CDATA[Fundamentals of software development @006]]></title>
            <link>https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/</link>
            <guid>https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/</guid>
            <pubDate>Mon, 01 Sep 2025 00:09:15 GMT</pubDate>
            <description><![CDATA[Fundamentals of software development @006]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_IhMp" id="oop-vs-procedural-programming">OOP vs Procedural Programming<a href="https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#oop-vs-procedural-programming" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="oop">OOP<a href="https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#oop" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>a programming paradigm built around the concept of objects, which contain data and code to manipulate data.</li>
<li>The idea to model real-world entities and their interactions.</li>
<li>Global Data (fields) are enclosed in the objects.</li>
<li>Program components/tasks are easily divided across the development team / Requires more planning and design preparation</li>
<li>Easier to manage and maintain dependencies between objects / OOP programs are much larger and complex</li>
<li>Objects export the interface and hide the implementation and data / Tend to use more memory and GPU</li>
<li>Code is highly reusable and easy to scale and distribute / Making changes in one class potentially impact others, which can complicate the development of the code.</li>
</ul>
<!-- -->
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="procedural-programming">Procedural Programming<a href="https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#procedural-programming" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>the concept of procedure calls by structuring the program around procedures. (or functions/subroutines)</li>
<li>a sequential manner unless directed otherwise.</li>
<li>Global data (elements) is exposed to all the functions.</li>
<li>Easier to compile and interpret / Difficult to scale or extend</li>
<li>Straightforward and simpler to code / Dependencies between elements are unclear and not well-structured.</li>
<li>Less memory requirements / Data is exposed and insecure due to its exposure across the whole program</li>
<li>Easy to track the program flow / Hard to divide the work among programmers in a team.</li>
</ul>
<!-- -->
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="classes">Classes<a href="https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#classes" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>A class is a template/blueprint used to create objects</li>
</ul>
<table><thead><tr><th>java</th><th>python</th></tr></thead><tbody><tr><td>a pure OOP language</td><td>supports OOP</td></tr><tr><td>code must be written in classes</td><td>classes are optional</td></tr><tr><td>executable class must have <code>main()</code></td><td>scripts run without including a class</td></tr><tr><td>Encapsulation can be enforced by declaring fields as private</td><td>fields (global variables) are public by default</td></tr><tr><td>Visibility is managed through access modifiers</td><td>N/A ("_" to identify private data attributes, but still accessible)</td></tr></tbody></table>
<div class="language-py codeBlockContainer_QFtC theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_vdmG"><pre tabindex="0" class="prism-code language-py codeBlock_CpxK thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines__0Nl"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token keyword" style="color:#00009f">class</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">name</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">extend </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> superclass</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">variable</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">name</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">value</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic">#Class fields - data members</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">parameters</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic">#class constructor - object sbuilder</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">code</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">method</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">name</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">parameters</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic">#methods</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">code</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="classes-py">Classes Py<a href="https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#classes-py" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<table><thead><tr><th>Keywords</th><th>Functions</th></tr></thead><tbody><tr><td><code>class</code></td><td><code>__init__()</code></td></tr><tr><td><code>self</code>: keyword used to refer to object properties</td><td>del: the function is used to delete an object</td></tr><tr><td><code>pass</code>: keyword used to occupy no-code placement in a function</td><td><code>__str__()</code>: The function is used to return string representation of instances</td></tr><tr><td><code>cls</code>: keyword used to refer to class properties</td><td><code>super()</code>: the function is used call a parent method in a child class</td></tr></tbody></table>
<ul>
<li>Accessors: functions (with no parameters) in a Python class that provide access to the data attributes of an object.<!-- -->
<ul>
<li>known as getter methods, are named starting with the verb get, followed by the field name, which should start with an uppercase letter.</li>
</ul>
</li>
<li>Mutators: procedures (with parameter) in a Python class that enable the developer to modify the values of object attributes.<!-- -->
<ul>
<li>known as setter methods, are named starting with the verb set, followed by the field name, which should start with an uppercase letter.</li>
</ul>
</li>
</ul>
<div class="language-py codeBlockContainer_QFtC theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_vdmG"><pre tabindex="0" class="prism-code language-py codeBlock_CpxK thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines__0Nl"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> get</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">Variable</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">field</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token builtin">set</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">Variable</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> value</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">field</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> value</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="classes-java">Classes Java<a href="https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#classes-java" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<div class="language-java codeBlockContainer_QFtC theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_vdmG"><pre tabindex="0" class="prism-code language-java codeBlock_CpxK thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines__0Nl"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">public</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">Bank</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">private</span><span class="token plain"> </span><span class="token class-name">Customer</span><span class="token plain"> customer</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">private</span><span class="token plain"> </span><span class="token class-name">String</span><span class="token plain"> branch</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">public</span><span class="token plain"> </span><span class="token class-name">Bank</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    customer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">new</span><span class="token plain"> </span><span class="token class-name">Customer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">public</span><span class="token plain"> </span><span class="token class-name">Bank</span><span class="token punctuation" style="color:#393A34">(</span><span class="token class-name">String</span><span class="token plain"> name</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">this</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">this</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">branch </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> name</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">public</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">boolean</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">find</span><span class="token punctuation" style="color:#393A34">(</span><span class="token class-name">Bank</span><span class="token plain"> bank</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">this</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">branch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token function" style="color:#d73a49">equals</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">bank</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">branch</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="packages">Packages<a href="https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#packages" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="packages-java">Packages Java<a href="https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#packages-java" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>used to group related classes</li>
<li>like folders containing files (classes)</li>
<li>either Java defined or user-defined</li>
<li>used to write maintainable and portable code and to avoid class name conflicts.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="modules-py">Modules Py<a href="https://gracefullight.dev/en/2025/09/01/fundamentals-of-software-development-006/#modules-py" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>used to grou prelated functio nand classes together</li>
<li>normal Python scripts that are used into other scripts</li>
<li>either Python defined or user-defined</li>
<li>used to write maintainable and portable code to improve reusability</li>
</ul>]]></content:encoded>
            <category>fsd</category>
        </item>
        <item>
            <title><![CDATA[π0 Review]]></title>
            <link>https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/</link>
            <guid>https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/</guid>
            <pubDate>Sun, 31 Aug 2025 14:24:33 GMT</pubDate>
            <description><![CDATA[π0 Review]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_IhMp" id="π0">π0<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#%CF%800" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="problem--motivation">Problem &amp; Motivation<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#problem--motivation" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Achieving real-world generality in robot learning is blocked by <strong>data scarcity, generalization, and robustness</strong> limits.</li>
<li>Human intelligence most outpaces machines in <strong>versatility</strong>—solving diverse, physically situated tasks under constraints, language commands, and perturbations.</li>
<li>In NLP/CV, <strong>foundation models</strong> pre-trained on diverse multi-task data, then <strong>fine-tuned (aligned)</strong> on curated datasets, outperform narrow specialists; the same paradigm is hypothesized for robotics.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="core-proposal">Core Proposal<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#core-proposal" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>A <strong>novel flow-matching architecture</strong> built on a pre-trained <strong>Vision-Language Model (VLM)</strong> to inherit Internet-scale semantics.</li>
<li>Further training adds <strong>robot actions</strong>, turning the model into a <strong>Vision-Language-Action (VLA)</strong> policy.</li>
<li>Use <strong>cross-embodiment training</strong> to combine data from many robot types (single/dual-arm, mobile), despite differing configuration/action spaces.</li>
<li>Employ <strong>action chunking</strong> + <strong>flow matching</strong> (diffusion variant) to model complex, continuous, high-frequency actions.</li>
<li>Introduce an <strong>Action Expert</strong> (separate weights for action/state tokens), akin to a <strong>Mixture-of-Experts</strong>, augmenting the standard VLM.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="training-recipe-pre--vs-post-training">Training Recipe (Pre- vs Post-Training)<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#training-recipe-pre--vs-post-training" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Pre-training</strong> on highly diverse data builds broad, general physical abilities.</li>
<li><strong>Post-training</strong> on curated, task-specific data instills <strong>fluent, efficient strategies</strong>.</li>
<li>Rationale: high-quality-only training lacks recovery behaviors; low-quality-only training lacks efficiency/robustness; <strong>combining both</strong> yields desired behavior.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="data--backbone">Data &amp; Backbone<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#data--backbone" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>~<strong>10,000 hours</strong> of demonstrations + the <strong>OXE</strong> dataset; data spans <strong>7 robot configurations</strong> and <strong>68 tasks</strong>.</li>
<li>VLM backbone initialized from <strong>PaliGemma (3B)</strong>; add <strong>~300M</strong> parameters for the action expert (total <strong>~3.3B</strong>).</li>
<li>Pre-training mixture: weighted combination of internal datasets + full OXE; <strong>n^0.43</strong> weighting to down-weight overrepresented task-robot pairs.</li>
<li>Unify interfaces: zero-pad <strong>qt/at</strong> to the largest robot dimension (18); mask missing image slots; late-fusion encoders map images/states to the same token space as language.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="modeling-details">Modeling Details<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#modeling-details" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Conditional flow matching</strong> models the continuous distribution over action chunks.</li>
<li>Train with a <strong>diffusion-style loss</strong> on individual sequence elements (instead of cross-entropy), with separate weights for diffusion-related tokens.</li>
<li>Flow path uses a <strong>linear-Gaussian</strong> schedule; sample noisy actions with ε∼N(0, I); predict denoising vector field; <strong>Euler integration</strong> from τ=0→1 at inference.</li>
<li>Efficient inference by <strong>caching</strong> K/V for the observation prefix; action tokens recomputed per integration step.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="high-level-language-policy">High-Level Language Policy<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#high-level-language-policy" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Because the policy consumes language, a <strong>high-level VLM</strong> can decompose tasks (e.g., bussing) into intermediate language subgoals (SayCan-style planning), improving performance on complex, temporally extended tasks.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="evaluation-setup--baselines">Evaluation Setup &amp; Baselines<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#evaluation-setup--baselines" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Out-of-box</strong> (direct prompting), <strong>fine-tuning</strong> on downstream tasks, and <strong>with high-level VLM</strong> providing intermediate commands.</li>
<li>Compare against <strong>OpenVLA (7B, autoregressive discretization; no action chunks/high-frequency control)</strong> and <strong>Octo (93M; diffusion)</strong>, trained on the same mixture.</li>
<li>Include a <strong>compute-parity</strong> π0 (160k steps vs 700k) and a <strong>π0-small</strong> variant (no VLM init).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="key-results">Key Results<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#key-results" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Out-of-box</strong>: π0 outperforms all baselines; even compute-parity π0 beats OpenVLA/Octo; π0-small still surpasses them—highlighting the benefits of <strong>expressive architectures + diffusion/flow matching + VLM pre-training</strong>.</li>
<li><strong>Language following</strong>: π0 clearly exceeds π0-small across conditions:<!-- -->
<ul>
<li><strong>π0-flat</strong>: only overall task command.</li>
<li><strong>π0-human</strong>: human-provided intermediate steps.</li>
<li><strong>π0-HL</strong>: high-level VLM-provided steps (fully autonomous).</li>
<li>Better language-following accuracy <strong>directly translates</strong> into stronger autonomous performance with high-level guidance.</li>
</ul>
</li>
<li><strong>New dexterous tasks</strong> (e.g., bowls stacking, towel folding, microwave, drawer items, paper towel replacement):<!-- -->
<ul>
<li>Fine-tuned π0 generally outperforms <strong>OpenVLA</strong>, <strong>Octo</strong>, and small-data methods <strong>ACT</strong> / <strong>Diffusion Policy</strong>.</li>
<li>Pre-training helps most when tasks resemble pre-training data; pretrained π0 often beats from-scratch by up to <strong>2×</strong>.</li>
</ul>
</li>
<li><strong>Complex multi-stage tasks</strong> (laundry folding, table bussing, box building, to-go box, eggs):<!-- -->
<ul>
<li>π0 solves many tasks; <strong>full pre-training + fine-tuning</strong> performs best.</li>
<li>Gains from pre-training are <strong>especially large</strong> on harder tasks; absolute performance varies with task difficulty and pre-training coverage.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="takeaways--limitations">Takeaways &amp; Limitations<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#takeaways--limitations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>π0 mirrors LLM training: <strong>pre-train for knowledge</strong>, <strong>post-train for alignment</strong> (instruction-following and execution).</li>
<li>Limitations/open questions:<!-- -->
<ul>
<li>Optimal <strong>composition/weighting</strong> of pre-training data remains unclear.</li>
<li>Not all tasks work reliably; difficult to predict <strong>how much/what kind</strong> of data is needed for near-perfect performance.</li>
<li>Uncertain <strong>positive transfer</strong> across very diverse tasks/robots and to distinct domains (e.g., driving, navigation, legged locomotion).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="ref">Ref<a href="https://gracefullight.dev/en/2025/09/01/pi-zero-reivew/#ref" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B., Jakubczak, S., Jones, T., Ke, L., Levine, S., Li‑Bell, A., Mothukuri, M., Nair, S., Pertsch, K., Shi, L. X, … Zhilinsky, U. (2025, June 21). π₀: A vision‑language‑action flow model for general robot control Robotics: Science and Systems (RSS), Los Angeles, CA, United States. <code>https://roboticsconference.org/program/papers/10/</code></li>
</ul>]]></content:encoded>
            <category>vlm</category>
        </item>
        <item>
            <title><![CDATA[Vima Review]]></title>
            <link>https://gracefullight.dev/en/2025/08/31/vima-review/</link>
            <guid>https://gracefullight.dev/en/2025/08/31/vima-review/</guid>
            <pubDate>Sun, 31 Aug 2025 10:20:03 GMT</pubDate>
            <description><![CDATA[Vima review]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_IhMp" id="vima">VIMA<a href="https://gracefullight.dev/en/2025/08/31/vima-review/#vima" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Unified Multimodal Prompts</strong>: Reformulates diverse robot tasks (language, images, video) into a single sequence modeling problem.</li>
<li><strong>Object-Centric Tokenization</strong>: Uses object-level tokens (Mask R-CNN + ViT) instead of raw pixels, improving data efficiency and semantic generalization.</li>
<li><strong>Cross-Attention Conditioning</strong>: Conditions the policy on prompts via cross-attention, maintaining strong zero-shot performance even with small models or novel tasks.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="motivation">Motivation<a href="https://gracefullight.dev/en/2025/08/31/vima-review/#motivation" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Robot task specification comes in many forms: one-shot demonstrations, language instructions, and visual goals.</li>
<li>Traditionally, each task required distinct architectures and pipelines, leading to siloed systems with poor generalization.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="VIMA Architecture" src="https://gracefullight.dev/en/assets/images/vima-architecture-f4e3269ed85bc5b1c954f82ab1ef77cd.png" width="1424" height="988" class="img_f7zd"></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="key-contributions">Key Contributions<a href="https://gracefullight.dev/en/2025/08/31/vima-review/#key-contributions" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li>
<p><strong>Multimodal Prompting</strong></p>
<ul>
<li>A novel formulation that unifies diverse robot manipulation tasks into a <strong>sequence modeling problem</strong>.</li>
<li>Prompts are defined as interleaved sequences of text and images, enabling flexibility across task formats.</li>
</ul>
</li>
<li>
<p><strong>VIMA-BENCH</strong></p>
<ul>
<li>A large-scale benchmark with <strong>17 tasks</strong> across six categories (object manipulation, goal reaching, novel concept grounding, video imitation, constraint satisfaction, visual reasoning).</li>
<li>Provides <strong>650K expert trajectories</strong> and a <strong>four-level evaluation protocol</strong> for systematic generalization.</li>
</ul>
</li>
<li>
<p><strong>VIMA Agent</strong></p>
<ul>
<li>A transformer-based visuomotor agent with <strong>encoder-decoder architecture</strong> and <strong>object-centric design</strong>.</li>
<li>Encodes prompts with a pre-trained <strong>T5 model</strong>, parses images into object tokens via <strong>Mask R-CNN + ViT</strong>, and decodes actions autoregressively using <strong>cross-attention</strong>.</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="design-insights">Design Insights<a href="https://gracefullight.dev/en/2025/08/31/vima-review/#design-insights" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Object-Centric Representation</strong>: Passing variable-length object token sequences directly to the controller is more effective than pixel-based tokenization.</li>
<li><strong>Cross-Attention Conditioning</strong>: Stronger prompt focus and efficiency compared to simple concatenation (e.g., GPT-style).</li>
<li><strong>Robustness</strong>: Minimal degradation under distractors or corrupted prompts, aided by T5 backbone and object augmentation.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="results">Results<a href="https://gracefullight.dev/en/2025/08/31/vima-review/#results" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>
<p><strong>Performance</strong>:</p>
<ul>
<li>Outperforms baselines (VIMA-Gato, VIMA-Flamingo, VIMA-GPT) by up to <strong>2.9× success rate</strong> in hardest zero-shot generalization.</li>
<li>With <strong>10× less training data</strong>, still <strong>2.7× better</strong> than best competitor.</li>
</ul>
</li>
<li>
<p><strong>Scaling</strong>:</p>
<ul>
<li>Sample-efficient: with just <strong>1% of data</strong>, matches baselines trained with 10× more.</li>
<li>Generalization holds across L1–L4 evaluation, with smaller regression than alternatives.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="conclusion">Conclusion<a href="https://gracefullight.dev/en/2025/08/31/vima-review/#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>VIMA demonstrates that multimodal prompting is a powerful unifying framework for robot learning.<br>
<!-- -->It achieves strong scalability, data efficiency, and generalization, establishing a <strong>solid starting point for future generalist robot agents</strong>.</p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="ref">Ref<a href="https://gracefullight.dev/en/2025/08/31/vima-review/#ref" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A., Zhu, Y., &amp; Fan, L. (2023). VIMA: Robot Manipulation with Multimodal Prompts Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v202/jiang23b.html</code></li>
</ul>]]></content:encoded>
            <category>vlm</category>
        </item>
        <item>
            <title><![CDATA[RoboFlamingo Review]]></title>
            <link>https://gracefullight.dev/en/2025/08/31/roboflamingo-review/</link>
            <guid>https://gracefullight.dev/en/2025/08/31/roboflamingo-review/</guid>
            <pubDate>Sun, 31 Aug 2025 05:35:06 GMT</pubDate>
            <description><![CDATA[RoboFlamingo Review]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_IhMp" id="roboflamingo">RoboFlamingo<a href="https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#roboflamingo" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RoboFlamingo <strong>decouples vision-language understanding and control</strong>, using OpenFlamingo for perception and a lightweight policy head for sequential decision-making.</li>
<li>Unlike prior VLM-based approaches, it requires only <strong>small-scale imitation fine-tuning</strong> on language-conditioned manipulation data, without large-scale co-fine-tuning.</li>
<li>This design enables <strong>data-efficient, zero-shot generalizable, and deployable</strong> robot manipulation policies on modest compute resources.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="key-idea">Key Idea<a href="https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#key-idea" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Proposes <strong>RoboFlamingo</strong>, a simple framework to adapt existing VLMs for robotic manipulation with lightweight fine-tuning.</li>
<li>Built on <strong>OpenFlamingo</strong>, decoupling <strong>vision-language understanding</strong> from <strong>decision-making</strong>.</li>
<li>Pre-trained VLM handles <strong>language and visual comprehension</strong>, while a dedicated <strong>policy head models sequential history</strong>.</li>
<li>Fine-tuned only on <strong>language-conditioned manipulation datasets</strong> using imitation learning.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="advantages">Advantages<a href="https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#advantages" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Requires only a <strong>small amount of demonstrations</strong> to adapt to downstream manipulation tasks.</li>
<li>Provides <strong>open-loop control</strong> capability → deployable on low-performance platforms.</li>
<li>Can be trained/evaluated on a <strong>single GPU server</strong>, making it a cost-effective and accessible solution.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="benchmarks">Benchmarks<a href="https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#benchmarks" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Evaluated on <strong>CALVIN benchmark</strong> (34 tasks, 1000 instruction chains).</li>
<li>RoboFlamingo achieves <strong>2× performance improvements</strong> over previous state-of-the-art methods.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="performance">Performance<a href="https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#performance" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Imitation Learning</strong>: Outperforms all baselines across all metrics.</li>
<li><strong>Zero-shot Generalization</strong>:<!-- -->
<ul>
<li><strong>Vision</strong>: Stronger generalization in ABC→D setting.</li>
<li><strong>Language</strong>: Robust to GPT-4 generated synonymous instructions.</li>
</ul>
</li>
<li><strong>Ablation Studies</strong>:<!-- -->
<ul>
<li>Ignoring history (MLP w/o hist) gives worst results.</li>
<li>LSTM and GPT-based policy heads perform best (LSTM chosen as default).</li>
<li><strong>VL pre-training</strong> is crucial for downstream manipulation.</li>
<li><strong>Larger VLMs</strong> show better data efficiency.</li>
<li><strong>Instruction fine-tuning</strong> improves both seen and unseen tasks.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="flexibility-of-deployment">Flexibility of Deployment<a href="https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#flexibility-of-deployment" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Supports <strong>open-loop control</strong> by predicting entire action sequences with a single inference → reduces latency and test-time compute.</li>
<li>Direct open-loop use without retraining can degrade performance; mitigated with <strong>jump-step demonstrations</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="conclusion">Conclusion<a href="https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Demonstrates that pre-trained VLMs enable <strong>data efficiency</strong> and strong <strong>zero-shot generalization</strong> in robotic manipulation.</li>
<li>RoboFlamingo is presented as an <strong>intuitive, efficient, and open solution</strong>, with high potential when combined with large-scale real robot data.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="ref">Ref<a href="https://gracefullight.dev/en/2025/08/31/roboflamingo-review/#ref" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Li, X., Liu, M., Zhang, H., Yu, C., Xu, J., Wu, H., Cheang, C., Jing, Y., Zhang, W., &amp; Liu, H. (2024). Vision-language foundation models as effective robot imitators. International Conference on Learning Representations (ICLR 2024), Vienna, Austria.</li>
</ul>]]></content:encoded>
            <category>vlm</category>
        </item>
        <item>
            <title><![CDATA[OpenVLA Review]]></title>
            <link>https://gracefullight.dev/en/2025/08/29/open-vla-review/</link>
            <guid>https://gracefullight.dev/en/2025/08/29/open-vla-review/</guid>
            <pubDate>Fri, 29 Aug 2025 07:43:25 GMT</pubDate>
            <description><![CDATA[OpenVLA Review]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_IhMp" id="openvla">OpenVLA<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#openvla" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>OpenVLA is a 7B open-source VLA model built on Llama2 + DINOv2 + SigLIP, trained on 970k demos, achieving stronger generalization and robustness than closed RT-2-X (55B) and outperforming Diffusion Policy.</li>
<li>It introduces efficient adaptation via LoRA (1.4% params, 8× compute reduction) and 4-bit quantization (half memory, same accuracy), enabling fine-tuning and inference on consumer GPUs.</li>
<li>Limitations remain (single-image input, <code>&lt;90%</code> reliability, limited throughput), but OpenVLA provides the first open, scalable framework for generalist robot policies.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="OpenVLA Architecture" src="https://gracefullight.dev/en/assets/images/open-vla-architecture-df4421cc82c1ebceca0ccf1061cd4593.png" width="915" height="366" class="img_f7zd"></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="motivation">Motivation<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#motivation" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Training robot policies from scratch struggles with robustness and generalization.</li>
<li>Fine-tuning <strong>vision-language-action (VLA)</strong> models offers reusable, generalizable visuomotor policies.</li>
<li>Barriers: prior VLAs are <strong>closed-source</strong>, lack best practices for adaptation, and need server-class hardware.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="model--training">Model &amp; Training<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#model--training" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>OpenVLA</strong>: 7B parameters, open-source.</li>
<li>Built on <strong>Llama 2</strong> with fused <strong>DINOv2 + SigLIP</strong> vision encoders.</li>
<li>Trained on <strong>970k robot demonstrations</strong> from Open-X Embodiment dataset.</li>
<li>Represents robot actions as <strong>tokens</strong> (discretized into 256 bins, replacing unused Llama tokens).</li>
<li>Standard <strong>next-token prediction</strong> objective.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="architecture--approach">Architecture &amp; Approach<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#architecture--approach" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>End-to-end fine-tuning of VLM to generate robot actions as tokens.</li>
<li>Differs from modular methods (e.g., Octo) that stitch separate encoders/decoders.</li>
<li>Vision features are obtained by encoding the same input image with both SigLIP and DINOv2, then channel-wise concatenated and passed through an MLP projector. This preserves SigLIP’s semantic alignment with language and DINOv2's spatial reasoning, giving the VLM richer multimodal context for manipulation tasks.</li>
<li>Uses Prismatic VLM backbone with multi-resolution features (spatial reasoning + semantics).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="performance">Performance<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#performance" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Outperforms closed <strong>RT-2-X (55B)</strong> by <strong>+16.5% task success</strong> with 7× fewer parameters.</li>
<li>Beats <strong>Diffusion Policy</strong> (from-scratch imitation learning) by <strong>+20.4%</strong> on multi-task language-grounded settings.</li>
<li>Demonstrates <strong>robust behaviors</strong> (distractor resistance, error recovery).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="efficiency">Efficiency<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#efficiency" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Introduces <strong>parameter-efficient fine-tuning</strong>:<!-- -->
<ul>
<li><strong>LoRA</strong> updates only 1.4% of parameters yet matches full fine-tuning.</li>
<li>Can fine-tune on a single A100 GPU in ~10–15 hours (8× compute reduction).</li>
</ul>
</li>
<li><strong>Quantization</strong>:<!-- -->
<ul>
<li>4-bit inference matches bfloat16 accuracy while halving memory footprint.</li>
<li>Runs at 3Hz on consumer GPUs (e.g., A5000, 16GB).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="evaluations">Evaluations<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#evaluations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Tested across <strong>29 tasks</strong> and multiple robots (WidowX, Google robot, Franka).</li>
<li>Strong generalization on:<!-- -->
<ul>
<li><strong>Visual</strong> (unseen backgrounds/distractors).</li>
<li><strong>Motion</strong> (new object positions/orientations).</li>
<li><strong>Physical</strong> (new object shapes/sizes).</li>
<li><strong>Semantic</strong> (unseen tasks, instructions).</li>
</ul>
</li>
<li>First generalist open-source VLA achieving <strong>≥50% success rate across all tested tasks</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="design-insights">Design Insights<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#design-insights" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Fine-tuning the vision encoder</strong> (vs. freezing) crucial for robotic control.</li>
<li>Higher image resolution (384px vs. 224px) adds 3× compute without performance gains.</li>
<li>Training required <strong>27 epochs</strong>, far more than typical VLM runs, to surpass 95% action token accuracy.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="limitations--future-work">Limitations &amp; Future Work<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#limitations--future-work" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Supports only <strong>single-image observations</strong> (no proprioception, no history).</li>
<li>Inference throughput (~6Hz on RTX 4090) insufficient for high-frequency control (e.g., ALOHA at 50Hz).</li>
<li>Success rates remain below 90% in challenging tasks.</li>
<li>Open questions:<!-- -->
<ul>
<li>Impact of base VLM size on performance.</li>
<li>Benefits of co-training with Internet-scale data.</li>
<li>Best visual features for VLAs.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="contributions">Contributions<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#contributions" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li>First <strong>open-source generalist VLA</strong> with strong performance.</li>
<li>Scalable <strong>end-to-end training</strong> pipeline (action-as-token).</li>
<li>Demonstrates <strong>LoRA + quantization</strong> for consumer-grade GPU adaptation.</li>
<li>Provides <strong>code, checkpoints, and data curation recipes</strong> to support future research.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="ref">Ref<a href="https://gracefullight.dev/en/2025/08/29/open-vla-review/#ref" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E. P., Sanketi, P. R., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake, R., Sadigh, D., Levine, S., Liang, P., &amp; Finn, C. (2025). OpenVLA: An Open-Source Vision-Language-Action Model Proceedings of The 8th Conference on Robot Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v270/kim25c.html</code></li>
</ul>]]></content:encoded>
            <category>vlm</category>
        </item>
        <item>
            <title><![CDATA[데이터 시각화 의사 결정 트리]]></title>
            <link>https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/</link>
            <guid>https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/</guid>
            <pubDate>Thu, 28 Aug 2025 10:48:43 GMT</pubDate>
            <description><![CDATA[데이터 시각화 의사 결정 트리]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_IhMp" id="color-legend">Color Legend<a href="https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#color-legend" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<table><thead><tr><th>Icon</th><th>Category</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td>🟡</td><td>Distribution</td><td>분포를 보여주고 싶을 때</td><td>Histogram, Density plot</td></tr><tr><td>⚫</td><td>Correlation</td><td>상관관계를 보여주고 싶을 때</td><td>Scatterplot, Correlogram</td></tr><tr><td>🟢</td><td>Ranking</td><td>순위를 보여주고 싶을 때</td><td>Bar chart, Lollipop chart</td></tr><tr><td>🔴</td><td>Part of a whole</td><td>전체 중 일부를 보여주고 싶을 때</td><td>Pie chart, Treemap</td></tr><tr><td>🔵</td><td>Evolution</td><td>시간에 따른 변화를 보여주고 싶을 때</td><td>Line chart, Area chart</td></tr><tr><td>🟣</td><td>Maps</td><td>지도를 활용해서 공간적 정보를 보여줄 때</td><td>Choropleth map, Bubble Map</td></tr><tr><td>🟤</td><td>Flow</td><td>흐름(흐름도, 이동 경로 등)을 보여줄 때</td><td>Flow map, Sankey-like</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="categoric">Categoric<a href="https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#categoric" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>One Variable<!-- -->
<ul>
<li>⚫ Waffle</li>
<li>🟢 Bar Plot</li>
<li>🟢 Lollipop</li>
<li>🟢 Word Cloud</li>
<li>🔴 Circular Packing</li>
<li>🔴 Doughnut</li>
<li>🔴 Pie</li>
<li>🔴 Treemap</li>
</ul>
</li>
<li>Two or More Variables<!-- -->
<ul>
<li>Two Independent Lists<!-- -->
<ul>
<li>🔴 Venn Diagram</li>
</ul>
</li>
<li>Nested<!-- -->
<ul>
<li>🟢 Bar Plot</li>
<li>🔴 Circular Packing</li>
<li>🔴 Dendrogram</li>
<li>🔴 Sunburst</li>
<li>🔴 Treemap</li>
</ul>
</li>
<li>Subgroup<!-- -->
<ul>
<li>⚫ Grouped Scatter</li>
<li>⚫ Heatmap</li>
<li>🟢 Lollipop</li>
<li>🟢 Parallel Plot</li>
<li>Spider</li>
<li>🔴 Grouped Bar Plot</li>
<li>🔴 Grouped Bar Plot</li>
<li>🟤 Sankey Diagram</li>
</ul>
</li>
<li>Adjacency<!-- -->
<ul>
<li>🟤 Arc</li>
<li>🟤 Chord</li>
<li>🟤 Network</li>
<li>🟤 Sankey</li>
<li>⚫ Heatmap</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="relational">Relational<a href="https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#relational" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Network<!-- -->
<ul>
<li>⚫ Heatmap</li>
<li>🟢 Hive</li>
<li>🟤 Arc</li>
<li>🟤 Chord</li>
<li>🟤 Network</li>
<li>🟤 Sankey</li>
</ul>
</li>
<li>Nested<!-- -->
<ul>
<li>No Value<!-- -->
<ul>
<li>🔴 Circular Packing</li>
<li>🔴 Dendrogram</li>
<li>🔴 Sunburst</li>
<li>🔴 Treemap</li>
<li>🟤 Sankey</li>
</ul>
</li>
<li>Value for Leaf<!-- -->
<ul>
<li>🔴 Circular Packing</li>
<li>🔴 Dendrogram</li>
<li>🔴 Sunburst</li>
<li>🔴 Treemap</li>
<li>🟤 Sankey</li>
</ul>
</li>
<li>Value for Edges<!-- -->
<ul>
<li>🔴 Dendrogram</li>
<li>🟤 Chord</li>
<li>🟤 Sankey</li>
</ul>
</li>
<li>Value for Connection<!-- -->
<ul>
<li>Edge Bundling</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="map">Map<a href="https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#map" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>🟣 Bubble Map</li>
<li>🟣 Choropleth</li>
<li>🟣 Connected Map</li>
<li>🟣 Map</li>
<li>🟣 Map Hexbin</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="time-series">Time Series<a href="https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#time-series" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>One Series<!-- -->
<ul>
<li>🟡 Box Plot</li>
<li>🟡 Violin</li>
<li>🟡 Ridge Line</li>
<li>🔵 Area</li>
<li>🔵 Line Plot</li>
<li>🟢 Bar Plot</li>
<li>🟢 Lollipop</li>
</ul>
</li>
<li>Several Series<!-- -->
<ul>
<li>🟡 Box Plot</li>
<li>🟡 Violin</li>
<li>🟡 Ridge Line</li>
<li>⚫ Heatmap</li>
<li>🔵 Line Plot</li>
<li>🔵 Stacked Area</li>
<li>🔵 Stream Graph</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="categoric-and-numeric">Categoric and Numeric<a href="https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#categoric-and-numeric" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>One Numeric + One Categoric<!-- -->
<ul>
<li>One Observation, per Group<!-- -->
<ul>
<li>⚫ Waffle</li>
<li>🟢 Bar Plot</li>
<li>🟢 Lollipop</li>
<li>🟢 Word Cloud</li>
<li>🔴 Circular Packing</li>
<li>🔴 Doughnut</li>
<li>🔴 Pie</li>
<li>🔴 Treemap</li>
</ul>
</li>
<li>Several Observations, per Group<!-- -->
<ul>
<li>🟡 Box Plot</li>
<li>🟡 Violin</li>
<li>🟡 Ridge Line</li>
<li>🟡 Density</li>
<li>🟡 Histogram</li>
</ul>
</li>
</ul>
</li>
<li>One Category, Several Numeric<!-- -->
<ul>
<li>No Order<!-- -->
<ul>
<li>🟡 Box Plot</li>
<li>🟡 Violin</li>
<li>⚫ Grouped Scatter</li>
<li>⚫ 2D Density</li>
<li>⚫ PCA</li>
<li>⚫ Correlogram</li>
</ul>
</li>
<li>A Numeric is Ordered<!-- -->
<ul>
<li>⚫ Connected Scatter</li>
<li>🔵 Area</li>
<li>🔵 Line Plot</li>
<li>🔵 Stacked Area</li>
<li>🔵 Stream Graph</li>
</ul>
</li>
<li>One Value Per Group<!-- -->
<ul>
<li>⚫ Grouped Scatter</li>
<li>⚫ Heatmap</li>
<li>🟢 Lollipop</li>
<li>🟢 Parallel Plot</li>
<li>🟢 Spider Plot</li>
<li>🔴 Grouped Bar Plot</li>
<li>🔴 Grouped Bar Plot</li>
<li>🟤 Sankey Diagram</li>
</ul>
</li>
</ul>
</li>
<li>Several Categories, One Numeric<!-- -->
<ul>
<li>Subgroup<!-- -->
<ul>
<li>One Observation. per Group<!-- -->
<ul>
<li>⚫ Grouped Scatter</li>
<li>⚫ Heatmap</li>
<li>🟢 Lollipop</li>
<li>🟢 Parallel Plot</li>
<li>🟢 Spider Plot</li>
<li>🔴 Grouped Bar Plot</li>
<li>🔴 Grouped Bar Plot</li>
<li>🟤 Sankey Diagram</li>
</ul>
</li>
<li>Several Observations, per Group<!-- -->
<ul>
<li>🟡 Box Plot</li>
<li>🟡 Violin</li>
</ul>
</li>
</ul>
</li>
<li>Nested<!-- -->
<ul>
<li>One Observation. per Group<!-- -->
<ul>
<li>🟢 Bar Plot</li>
<li>🔴 Circular Packing</li>
<li>🔴 Dendrogram</li>
<li>🔴 Sunburst</li>
<li>🔴 Treemap</li>
</ul>
</li>
<li>Several Observations. per Group<!-- -->
<ul>
<li>🟡 Box Plot</li>
<li>🟡 Violin</li>
</ul>
</li>
</ul>
</li>
<li>Adjacency<!-- -->
<ul>
<li>⚫ Heatmap</li>
<li>🟤 Arc</li>
<li>🟤 Chord</li>
<li>🟤 Network</li>
<li>🟤 Sankey</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="numeric">Numeric<a href="https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#numeric" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>One Numeric Variable<!-- -->
<ul>
<li>🟡 Density</li>
<li>🟡 Histogram</li>
</ul>
</li>
<li>Two Numeric Variables<!-- -->
<ul>
<li>Not Ordered<!-- -->
<ul>
<li>Few Points<!-- -->
<ul>
<li>🟡 Box Plot</li>
<li>🟡 Histogram</li>
<li>⚫ Scatter Plot</li>
</ul>
</li>
<li>Many Points<!-- -->
<ul>
<li>🟡 Density</li>
<li>🟡 Violin</li>
<li>⚫ 2D Density</li>
<li>🔵 Marginal Distribution</li>
</ul>
</li>
</ul>
</li>
<li>Ordered<!-- -->
<ul>
<li>⚫ Connected Scatter</li>
<li>🔵 Area Plot</li>
<li>🔵 Line Plot</li>
</ul>
</li>
</ul>
</li>
<li>Three Numeric Variables<!-- -->
<ul>
<li>Not Ordered<!-- -->
<ul>
<li>🟡 Box Plot</li>
<li>🟡 Violin</li>
<li>⚫ Bubble Plot</li>
<li>⚫ 3d Scatter or Surface</li>
</ul>
</li>
<li>Ordered<!-- -->
<ul>
<li>🔵 Area</li>
<li>🔵 Line Plot</li>
<li>🔵 Stacked Area</li>
<li>🔵 Stream Graph</li>
</ul>
</li>
</ul>
</li>
<li>Several Numeric Variables<!-- -->
<ul>
<li>Ordered<!-- -->
<ul>
<li>🔵 Area</li>
<li>🔵 Line Plot</li>
<li>🔵 Stacked Area</li>
<li>🔵 Stream Graph</li>
</ul>
</li>
<li>Not Ordered<!-- -->
<ul>
<li>🟡 Box Plot</li>
<li>🟡 Ridge Line</li>
<li>🟡 Violin</li>
<li>⚫ Correlogram</li>
<li>⚫ Heatmap</li>
<li>⚫ PCA</li>
<li>🔴 Dendrogram</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="ref">Ref<a href="https://gracefullight.dev/en/2025/08/28/data-visualization-desicion-tree/#ref" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><a href="https://www.data-to-viz.com/" target="_blank" rel="noopener noreferrer">from Data to Viz</a></li>
</ul>]]></content:encoded>
            <category>data</category>
        </item>
        <item>
            <title><![CDATA[Vocabulary for AI @006]]></title>
            <link>https://gracefullight.dev/en/vocab/vocab-ai-006/</link>
            <guid>https://gracefullight.dev/en/vocab/vocab-ai-006/</guid>
            <pubDate>Thu, 28 Aug 2025 06:13:52 GMT</pubDate>
            <description><![CDATA[Vocabulary for AI @006]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_IhMp" id="vocabulary--expressions">Vocabulary &amp; Expressions<a href="https://gracefullight.dev/en/vocab/vocab-ai-006/#vocabulary--expressions" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<table><thead><tr><th>Term/Expression</th><th>Definition</th><th>Simpler Paraphrase</th><th>Meaning</th></tr></thead><tbody><tr><td>prevalence</td><td>The state of being widespread or common</td><td>Commonness</td><td>유행, 널리 퍼짐</td></tr><tr><td>instantiation</td><td>The act of creating a specific instance of something</td><td>Creation of a specific example</td><td>구체적인 값의 생성</td></tr><tr><td>triviality</td><td>The quality of being trivial or unimportant</td><td>Unimportance</td><td>사소함, 하찮음</td></tr><tr><td>intermediary</td><td>A person or thing that acts as a link between two others</td><td>Middleman</td><td>중개자, 매개체</td></tr><tr><td>dreaded</td><td>Regarded with great fear or apprehension</td><td>Feared</td><td>두려운, 걱정되는</td></tr><tr><td>i.i.d.</td><td>Independent and identically distributed</td><td>Same distribution, no dependence</td><td>독립적이고 동일한 분포</td></tr><tr><td>analogous</td><td>Similar in some way</td><td>Comparable</td><td>유사한, 비슷한</td></tr><tr><td>posteriori</td><td>Relating to knowledge gained through experience or empirical evidence</td><td>Based on observation</td><td>경험적, 관찰에 기초한</td></tr><tr><td>posterior</td><td>Relating to the back or rear</td><td>Back</td><td>뒤쪽의, 후방의</td></tr><tr><td>resemblance</td><td>The state of resembling or being alike</td><td>Similarity</td><td>유사성, 닮음</td></tr><tr><td>stipulate</td><td>To demand or specify a requirement</td><td>Specify</td><td>규정하다, 명시하다</td></tr><tr><td>rectify</td><td>To correct or make right</td><td>Correct</td><td>수정하다, 바로잡다</td></tr><tr><td>schematic</td><td>Relating to a diagram or representation</td><td>Diagrammatic</td><td>도식적인, 다이어그램의</td></tr><tr><td>proposition</td><td>A statement or assertion that expresses a judgment or opinion</td><td>Proposal</td><td>제안, 명제</td></tr><tr><td>cavity</td><td>A cavity is a hollow place in a tooth caused by decay</td><td>Tooth decay</td><td>충치</td></tr><tr><td>tautological</td><td>Relating to or involving tautology (the saying of the same thing twice in different words)</td><td>Redundant</td><td>동의어 반복의, 중복적인</td></tr><tr><td>retrospectively</td><td>Looking back on or dealing with past events or situations</td><td>Looking back</td><td>회고적으로, 과거를 돌아보며</td></tr><tr><td>perturbations</td><td>Disturbances or deviations from a normal state</td><td>Disturbances</td><td>교란, 변동</td></tr><tr><td>deformable</td><td>Capable of being changed in shape or form</td><td>Changeable</td><td>변형 가능한</td></tr><tr><td>Consolidation</td><td>The process of combining multiple elements into a single, more effective whole</td><td>Integration</td><td>통합</td></tr><tr><td>Oscillation</td><td>Fluctuation or variation in a state or condition</td><td>Fluctuation</td><td>진동, 변동</td></tr></tbody></table>]]></content:encoded>
            <category>vocab</category>
        </item>
        <item>
            <title><![CDATA[Developing ML Systems]]></title>
            <link>https://gracefullight.dev/en/2025/08/28/developing-ml-systems/</link>
            <guid>https://gracefullight.dev/en/2025/08/28/developing-ml-systems/</guid>
            <pubDate>Thu, 28 Aug 2025 06:04:52 GMT</pubDate>
            <description><![CDATA[Developing ML Systems]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_IhMp" id="problem-formulation-문제-정의">Problem formulation (문제 정의)<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#problem-formulation-%EB%AC%B8%EC%A0%9C-%EC%A0%95%EC%9D%98" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>The first step is to figure out what problem you want to solve.</strong>
<ol>
<li>“사용자에게 어떤 문제를 해결해주고 싶은가?” → 모호하지 않고 구체적으로 정의해야 함.</li>
<li>“그 문제 중 어떤 부분을 머신러닝으로 풀 수 있는가?” → 예: 사진을 라벨로 매핑하는 함수 학습.</li>
</ol>
</li>
<li>이를 구체화하려면 ML 컴포넌트에 대해 <strong>loss function</strong> 을 지정해야 한다.</li>
<li>문제를 쪼개보면 일부는 전통적 SW 엔지니어링으로 해결 가능하고, 일부만 ML로 다뤄야 할 수 있다.</li>
<li>학습 유형은 지도·비지도·강화·준지도(semisupervised)까지 연속선상에 있음.<!-- -->
<ul>
<li><strong>Semisupervised learning</strong>: 일부 라벨만 활용해 비라벨 데이터에서 더 많은 정보 추출.</li>
<li><strong>Weakly supervised learning</strong>: 부정확·노이즈 라벨을 사용.</li>
</ul>
</li>
<li>결론: <strong>Noise와 label 부족은 “지도 ↔ 비지도” 사이의 연속체를 형성</strong>한다.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="data-collection--management-데이터-수집관리">Data collection &amp; management (데이터 수집/관리)<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#data-collection--management-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91%EA%B4%80%EB%A6%AC" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>데이터는 직접 제작, 크라우드소싱, 사용자 행동에서 수집 가능.</li>
<li>부족할 때는 <strong>transfer learning</strong> 활용.</li>
<li><strong>Privacy 검토</strong>와 동의, 공정성, <strong>federated learning</strong> 등 고려 필요.</li>
<li><strong>Data provenance</strong>(출처 관리): 데이터 정의, 값의 범위, 생성 주체, 중단 여부, 정의 변경 이력 등 추적 → 파이프라인 안정성이 알고리즘보다 중요.</li>
<li>항상 자문: “이 데이터는 내 문제를 풀기에 적절한가? 입력과 출력 모두 충분히 담고 있는가?”</li>
<li><strong>Learning curve</strong> 로 데이터 확장 효과/학습 plateau 확인.</li>
<li>방어적 태도 필요: 입력 오류, 누락, 적대적 사용자, 철자 불일치 등 처리.</li>
<li><strong>Data augmentation</strong> (회전, 이동, 노이즈 추가 등)으로 모델 강건성 향상.</li>
<li>불균형 데이터는 <strong>undersampling, oversampling, SMOTE/ADASYN, boosting</strong> 등으로 완화.</li>
<li>아웃라이어는 로그 변환 등으로 영향 축소, 트리 모델은 상대적으로 강건.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="feature-engineering-특징-엔지니어링">Feature engineering (특징 엔지니어링)<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#feature-engineering-%ED%8A%B9%EC%A7%95-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Quantization</strong>: 연속값을 구간(bin)으로 강제.</li>
<li><strong>One-hot encoding</strong>: 범주형 속성을 다중 Boolean으로 변환.</li>
<li>도메인 지식 기반 새 특성 추가 (예: 날짜 → 주말/공휴일 여부).</li>
<li><strong>“At the end of the day, some ML projects succeed and some fail… the most important factor is the features used.” (Pedro Domingos)</strong></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="exploratory-data-analysis-eda--visualization">Exploratory data analysis (EDA) &amp; visualization<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#exploratory-data-analysis-eda--visualization" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>목표: 예측/검증이 아닌 <strong>데이터 이해</strong>.</li>
<li><strong>Histograms, scatter plots</strong> 로 분포/결측/오류/이상치 확인.</li>
<li>클러스터링 → 프로토타입 시각화, 이상치 탐지 (“고양이 vs 사자 옷 입은 고양이”).</li>
<li>차원 축소 (예: <strong>t-SNE</strong>)로 고차 데이터를 2D/3D로 시각화.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="model-selection--training">Model selection &amp; training<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#model-selection--training" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>데이터가 정리되면 모델 구축 단계.</li>
<li><strong>Random forests</strong> → 범주형 특징 많고 일부 무관할 때.</li>
<li><strong>Nonparametric methods</strong> → 데이터 많고 지식 부족, 특징 선택 고민 줄이고 싶을 때.</li>
<li><strong>Logistic regression</strong> → 선형 분리 가능(또는 feature engineering 후).</li>
<li><strong>SVM</strong> → 데이터 크기 작고 차원 높을 때.</li>
<li><strong>Deep neural nets</strong> → 패턴 인식(이미지·음성).</li>
<li>하이퍼파라미터는 경험 + 탐색으로 조율.</li>
<li>검증 데이터 남용 시 <strong>validation overfitting</strong> 위험 → 여러 검증셋 필요.</li>
<li>성능 평가: <strong>ROC curve, AUC, confusion matrix</strong>.</li>
<li>중요한 건 <strong>아이디어–실험–검증 반복 사이클을 빠르게 하는 것</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="trust-interpretability-explainability">Trust, interpretability, explainability<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#trust-interpretability-explainability" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>단순히 지표 성능만으로는 신뢰 부족 → 규제·언론·사용자도 신뢰성 원함.</li>
<li><strong>Accountability:</strong> 오류 발생 시 책임 주체와 항소 절차 필요.</li>
<li><strong>Interpretability:</strong> 모델 내부를 직접 이해 (트리, 선형회귀).<!-- -->
<ul>
<li>핵심 질문: <strong>“If I change x, how will the output change?”</strong></li>
</ul>
</li>
<li><strong>Explainability:</strong> 블랙박스 모델 + 별도 모듈로 설명 (예: LIME).</li>
<li>단순 설명이 잘못된 확신을 줄 수 있음. → 테스트와 실제 성능이 더 큰 신뢰를 준다.</li>
<li>“안전하다고 설명만 있는 실험기 vs 100회 무사비행한 비행기” 비유.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="operation-monitoring-maintenance">Operation, monitoring, maintenance<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#operation-monitoring-maintenance" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>운영 단계에서는 <strong>롱테일 입력(long tail)</strong> 문제 등장 → 예상 못한 입력 지속 발생. → 실시간 모니터링과 사람 평가자 필요.</li>
<li><strong>Nonstationarity:</strong> 세상과 사용자 행동 변화 → 최신 데이터 vs 안정적 모델 트레이드오프.</li>
<li>신선도 요구 다름: 어떤 문제는 매일/매시간 새 모델, 어떤 문제는 수개월 동일 모델.</li>
<li>배포 자동화 → 작은 변경은 자동 승인, 큰 변경은 리뷰.</li>
<li><strong>Online vs Offline model</strong>: 기존 모델 점진적 수정 vs 매번 처음부터 재학습.</li>
<li>데이터 자체가 바뀔 수도 있음 (스팸 이메일 → 스팸 문자, 음성, 영상 등).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id="checklist">Checklist<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#checklist" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="tests-for-features-and-data">Tests for Features and Data<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#tests-for-features-and-data" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul class="contains-task-list containsTaskList_cTFB">
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Feature expectations are captured in a schema.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->All features are beneficial.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->No feature’s cost is too much.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Features adhere to meta-level requirements.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->The data pipeline has appropriate privacy controls.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->New features can be added quickly.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->All input feature code is tested.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="tests-for-model-development">Tests for Model Development<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#tests-for-model-development" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul class="contains-task-list containsTaskList_cTFB">
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Every model specification undergoes a code review.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Every model is checked in to a repository.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Offline proxy metrics correlate with actual metrics.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->All hyperparameters have been tuned.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->The impact of model staleness is known.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->A simpler model is not better.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Model quality is sufficient on all important data slices.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->The model has been tested for considerations of inclusion.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="tests-for-machine-learning-infrastructure">Tests for Machine Learning Infrastructure<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#tests-for-machine-learning-infrastructure" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul class="contains-task-list containsTaskList_cTFB">
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Training is reproducible.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Model specification code is unit tested.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->The full ML pipeline is integration tested.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Model quality is validated before attempting to serve it.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->The model allows debugging by observing the step-by-step computation of training or inference on a single example.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Models are tested via a canary process before they enter production serving environments.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Models can be quickly and safely rolled back to a previous serving version.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="monitoring-tests-for-machine-learning">Monitoring Tests for Machine Learning<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#monitoring-tests-for-machine-learning" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul class="contains-task-list containsTaskList_cTFB">
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Dependency changes result in notification.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Data invariants hold in training and serving inputs.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Training and serving features compute the same values.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Models are not too stale.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->The model is numerically stable.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->The model has not experienced regressions in training speed, serving latency, throughput, or RAM usage.</li>
<li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->The model has not experienced a regression in prediction quality on served data.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id="ref">Ref<a href="https://gracefullight.dev/en/2025/08/28/developing-ml-systems/#ref" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Breck, E., Cai, S., Nielsen, E., Salib, M., &amp; Sculley, D. (2016). What’s your ML test score? A rubric for ML production systems. NIPS Workshop on Reliable Machine Learning in the Wild.</li>
</ul>]]></content:encoded>
            <category>ai</category>
        </item>
    </channel>
</rss>