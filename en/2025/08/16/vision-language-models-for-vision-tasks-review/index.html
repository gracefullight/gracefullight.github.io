<!doctype html><html lang=en dir=ltr class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>vision-language models for vision tasks review | gracefullight.dev</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=ko><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docusaurus_tag content=default><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docsearch:docusaurus_tag content=default><meta data-rh=true content=gcY9SiftHgQoJjBZ7IgwNNN5_atLPAX6kWb1nFVfa6E name=google-site-verification><meta data-rh=true content=65AD1E28C0D057CEB3C68FBC0293E55B name=msvalidate.01><meta data-rh=true content=d024c2837887f72dc7b3792b958be74d69ba9593 name=naver-site-verification><meta data-rh=true content=6672f93d837354fb name=yandex-verification><meta data-rh=true content=yZEdU1ABcR name=baidu-site-verification><meta data-rh=true content=uelupjqqsm5egzlhy1aev2rfxow5yt name=facebook-domain-verification><meta data-rh=true property=og:title content="vision-language models for vision tasks review | gracefullight.dev"><meta data-rh=true name=description content="vision-language models for vision tasks review"><meta data-rh=true property=og:description content="vision-language models for vision tasks review"><meta data-rh=true property=og:type content=article><meta data-rh=true property=article:published_time content=2025-08-16T07:40:55.588Z><meta data-rh=true property=article:author content=https://github.com/gracefullight><meta data-rh=true property=article:tag content=vlm><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review/><link data-rh=true rel=alternate href=https://gracefullight.dev/2025/08/16/vision-language-models-for-vision-tasks-review/ hreflang=ko><link data-rh=true rel=alternate href=https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review/ hreflang=en><link data-rh=true rel=alternate href=https://gracefullight.dev/2025/08/16/vision-language-models-for-vision-tasks-review/ hreflang=x-default><link data-rh=true rel=preconnect href=https://RFS69RSYOJ-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@id":"https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review","@type":"BlogPosting","author":{"@type":"Person","description":"Owner","image":"https://avatars.githubusercontent.com/u/11773683?v=4","name":"Gracefullight","url":"https://github.com/gracefullight"},"datePublished":"2025-08-16T07:40:55.588Z","description":"vision-language models for vision tasks review","headline":"vision-language models for vision tasks review","isPartOf":{"@id":"https://gracefullight.dev/en/","@type":"Blog","name":"Blog"},"keywords":[],"mainEntityOfPage":"https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review","name":"vision-language models for vision tasks review","url":"https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review"}</script><link rel=alternate type=application/rss+xml href=/en/rss.xml title="gracefullight.dev RSS Feed"><link rel=alternate type=application/atom+xml href=/en/atom.xml title="gracefullight.dev Atom Feed"><link rel=alternate type=application/json href=/en/feed.json title="gracefullight.dev JSON Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-E99DNE7S05"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-E99DNE7S05",{})</script><link rel=search type=application/opensearchdescription+xml title=gracefullight.dev href=/en/opensearch.xml><link href=/en/img/favicon-32x32.png rel=icon><link href=/en/manifest.json rel=manifest><meta content=#f28913 name=theme-color><meta content=yes name=mobile-web-app-capable><meta content=#f28913 name=apple-mobile-web-app-status-bar-style><link href=/en/img/apple-touch-icon.png rel=apple-touch-icon><link rel=preconnect href=https://pagead2.googlesyndication.com><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3004788392777865" async crossorigin=anonymous></script><link rel=preconnect href=https://www.clarity.ms><script>!function(t,e,n,a,c,i,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(i=e.createElement(a)).async=1,i.src="https://www.clarity.ms/tag/"+c,(r=e.getElementsByTagName(a)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script","aongv9xgi6")</script><link rel=preconnect href=https://wcs.naver.net><script src=https://wcs.naver.net/wcslog.js async></script><script>if(!wcs_add)var wcs_add={};wcs_add.wa="156bc73a81e3bd0",window.wcs&&wcs_do()</script><link rel=preconnect href=https://cdn.channel.io><script>!function(){var n=window;if(n.ChannelIO)return(window.console.error||window.console.log||function(){})("ChannelIO script included twice.");var e=function(){e.c(arguments)};function t(){if(!n.ChannelIOInitialized){n.ChannelIOInitialized=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cdn.channel.io/plugin/ch-plugin-web.js",e.charset="UTF-8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}}e.q=[],e.c=function(n){e.q.push(n)},n.ChannelIO=e,"complete"===document.readyState?t():window.attachEvent?window.attachEvent("onload",t):(window.addEventListener("DOMContentLoaded",t,!1),window.addEventListener("load",t,!1))}(),ChannelIO("boot",{pluginKey:"0fd130ba-a1a6-4b7e-802a-e82a885a7fd8"})</script><link rel=preconnect href=https://static.cloudflareinsights.com><script src=https://static.cloudflareinsights.com/beacon.min.js defer data-cf-beacon='{"token":"c0899829e72b45e98dff77241127252c"}'></script><link href=https://cdn.jsdelivr.net rel=preconnect><script type=application/ld+json>{"@context":"http://schema.org","@type":"Person","email":"mailto:gracefullight.dev@gmail.com","image":"https://avatars.githubusercontent.com/u/11773683?v=4","jobTitle":"FullStack JavaScript Developer","logo":"https://gracefullight.dev/img/apple-touch-icon.png","name":"Eunkwang Shin","nationality":"Korean","sameAs":["https://github.com/gracefullight","https://linkedin.com/in/gracefullight"],"url":"https://gracefullight.dev"}</script><link rel=stylesheet crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css type=text/css><script src=/en/assets/js/runtime~main.07ab8dd4.js defer></script><script src=/en/assets/js/main.f25fd7e2.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/favicon-32x32.png><link rel=preload as=image href="https://avatars.githubusercontent.com/u/11773683?v=4"><div role=region aria-label="Skip to main content"><a class=skipToContent_soTP href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent__Vw_ themedComponent--light_S0y_"><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent__Vw_ themedComponent--dark_IFCp"></div><b class="navbar__title text--truncate">gracefullight.dev</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_LGSY><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/2025/08/16/vision-language-models-for-vision-tasks-review/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ko>한국어</a><li><a href=/en/2025/08/16/vision-language-models-for-vision-tasks-review/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a></ul></div><a class="navbar__item navbar__link" href=/en/archive/>Archives</a><a class="navbar__item navbar__link" href=/en/tags/>Tags</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_YAYV colorModeToggle_T40I"><button class="clean-btn toggleButton_Qe_r toggleButtonDisabled_AVvq" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_T_L2 lightToggleIcon_FRTA"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_T_L2 darkToggleIcon_zlMh"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_T_L2 systemToggleIcon_h91M"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_SXBB><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_bGoB"><div class="container margin-vert--lg"><div class=row><aside class="col col--3"><nav class="sidebar_Pgmn thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_euex margin-bottom--md">Recent posts</div><div role=group><h3 class=yearGroupHeading_cosT>2025</h3><ul class="sidebarItemList_jela clean-list"><li class=sidebarItem_drnv><a aria-current=page class="sidebarItemLink_hMRR sidebarItemLinkActive_molF" href=/en/2025/08/16/vision-language-models-for-vision-tasks-review/>vision-language models for vision tasks review</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/16/introduction-to-ai-003/>Introduction to AI @003</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/14/fundamentals-of-data-analytics-003/>Fundamentals of data analytics @003</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/vocab/vocab-ai-004/>Vocabulary for AI @004</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/11/fundamentals-of-software-development-003/>Fundamentals of software development @003</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/09/introduction-to-ai-002/>Introduction to AI @002</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/06/fundamentals-of-data-analytics-002/>Fundamentals of Data Analytics @002</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/vocab/vocab-ai-003/>Vocabulary for AI @003</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/05/introduction-to-ai-001/>Introduction to Artificial Intelligence @001</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/04/fundamentals-of-software-development-002/>Fundamentals of software development @002</a></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class=title_lZLA>vision-language models for vision tasks review</h1><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-16T07:40:55.588Z>August 16, 2025</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div id=__blog-post-container class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=overview>Overview<a href=#overview class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<blockquote>
<p>Most visual recognition studies rely heavily on crowdlabelled data in DNN</p>
</blockquote>
<ul>
<li>Background development of visual recognition paradigms</li>
<li>Foundations its architecture</li>
<li>Datasets in VLM pre-training and evaluations</li>
<li>Review and categorization of existing pre-training methods</li>
<li>Benchmarking analysis discussion</li>
<li>Reach challenges & potential research direction</li>
<li>Training hard<!-- -->
<ul>
<li>New learning paradigm</li>
</ul>
</li>
<li>Vision-Language Model Pre-training and Zero-shot Prediction<!-- -->
<ul>
<li>Increasing attention</li>
</ul>
</li>
<li>VLMs with transfer learning<!-- -->
<ul>
<li>Prompt tuning</li>
<li>Visual adaption</li>
</ul>
</li>
<li>VLMs with knowledge distillation<!-- -->
<ul>
<li>distill knowledge from VLMs to downstream tasks</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=the-development-of-visual-recognition-paradigms>The development of visual recognition paradigms<a href=#the-development-of-visual-recognition-paradigms class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<!-- -->
<ul>
<li>Traditional ML: Hand-crafted features for prediction.</li>
<li>Deep Learning: Deep networks (e.g., ResNet) with large-scale labeled data.</li>
<li>Supervised Pre-training + Fine-tuning: Learned representations transferred to downstream tasks.</li>
<li>Unsupervised / Self-supervised Pre-training + Fine-tuning: Objectives like masked modeling and contrastive learning to learn representations.</li>
<li>Vision-Language Models & Zero-shot: Leverage large-scale web data, enabling zero-shot prediction without task-specific fine-tuning.<!-- -->
<ul>
<li>Collecting large-scale informative image-text data</li>
<li>Designing high-capacity models for effective learning from Bigdata.</li>
<li>Designing new pre-training objectives for learning effective VLMs.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Illustration of development of VLMs for visual recognition" src=/en/assets/images/vlm-paradigm-46050660982130f887307cc0442975c0.png width=701 height=347 class=img_f7zd></p>
<ul>
<li>CLIP: Image-text contrastive objective and learns by pulling the paired images and texts close and pushing others faraway in the embedding space.<!-- -->
<ul>
<li>enables effective usage of web data and allows zero-shot predictions without task-specific finetuning.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=vlm-overview>VLM Overview<a href=#vlm-overview class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p><img decoding=async loading=lazy alt="VLM Overview" src=/en/assets/images/vlm-overview-2abb2d65aeef690c49e399a7ca3ac86c.png width=1400 height=728 class=img_f7zd></p>
<ul>
<li>Given Image-text pairs.</li>
<li>Employs a text encoder and an image encoder to extract image and text features.</li>
<li>Learns the vision-language correlation with certain pre-training objectives.</li>
<li>GAP: Global Average Pooling, a technique used to reduce the spatial dimensions of feature maps while retaining important information.</li>
<li>ViT: Vision Transformer: Transformers for image recognition at scale.</li>
<li>CNN Based: VGG, <strong>ResNet</strong>, EfficientNet<!-- -->
<ul>
<li>ResNet: Adopts skip connections between convolutional blocks which mitigates gradient vanishing and explosion and enables DNN training.</li>
<li>ResNet-D: Replace global average pooling with transformer multi-head attention.</li>
</ul>
</li>
<li>Transformer Based: ViT<!-- -->
<ul>
<li>Adding a normalization layer before the transformer encoder.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=vlm-pre-training-objectives>VLM pre-training Objectives<a href=#vlm-pre-training-objectives class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=contrastive-objectives>Contrastive Objectives<a href=#contrastive-objectives class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Pros<!-- -->
<ul>
<li>Enforce positive pairs to have similar embeddings in contrast to negative pairs.</li>
<li>Encourages VLMs to learn discriminative vision and language features, where more discriminative features lead to more confident and accurate zero-shot predictions.</li>
</ul>
</li>
<li>Cons<!-- -->
<ul>
<li>Joint optimizing positive and negative pairs is complicated and challenging.</li>
<li>Involves a heuristic temperature hyper-parameter for controlling the feature discriminability.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_IhMp" id=image-contrastive-learning>Image Contrastive Learning<a href=#image-contrastive-learning class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Forcing a query image to be close with its positive keys (its data augmentations)</li>
<li>Faraway from its negative keys (other images)</li>
<li><strong>Learn discriminative features</strong> in image modality, which often serves as an auxiliary objective for fully exploiting the image data potential.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_IhMp" id=image-text-contrastive-learning>Image-Text Contrastive Learning<a href=#image-text-contrastive-learning class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Pulling the embeddings of paired images and texts close while pushing others away.</li>
<li>Minimizing a symmetrical image-text infoNCE loss</li>
<li><strong>Learn vision-language correlation</strong> by contrasting image-text pairs.<!-- -->
<ul>
<li>CLIP: A symmetrical image-text infoNCE loss</li>
<li>ALIGN: scales up the VLM pre-training with large-scale (but noisy image-text pair with noise-robust contrastive learning)</li>
<li>DeCLIP: Nearest-neighbor supervision to utilize the information from similar pairs, enabling effective pre-training on limited data.</li>
<li>OTTER: Optimal transport to pseudo-pair images and texts reducing the required training data.</li>
<li>ZeroVL: Limited data resource via debiased data sampling and data augmentation with coin flipping mixup.</li>
<li>FILIP: Region-word alignment into contrastive learning, enabling to learn fine-grained vision-language corresponding knowledge.</li>
<li>Pyramid-CLIP: Multiple semantic levels and performs both cross-level and peer-level contrastive learning for effective VLM pre-training.</li>
<li>LA-CLIP, ALIP: LLM to augment synthetic captions for given images while RA-CLIP retrieves relevant image-text pairs for image-text pair augmentation.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt=CLIP src=/en/assets/images/vlm-clip-002c83bc065d184a2350741cacc71908.png width=848 height=528 class=img_f7zd></p>
<h4 class="anchor anchorWithStickyNavbar_IhMp" id=image-text-label-contrastive-learning>Image-Text-Label Contrastive Learning<a href=#image-text-label-contrastive-learning class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Supervised Contrastive Learning into image-text contrastive learning.</li>
<li><strong>Learn discriminative and task-specific features</strong> by exploiting both supervised labels and unsupervised image-text pairs.<!-- -->
<ul>
<li>UniCL: pre-training allows learning both discriminative and task-specific (image classification) features simultaneously with around 900M image-text pairs.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Image-Text-Label Contrastive Learning" src=/en/assets/images/vlm-image-text-label-83c01a33a07520e54ac28a0ffbb1ceaa.png width=846 height=558 class=img_f7zd></p>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=generative-objectives>Generative Objectives<a href=#generative-objectives class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Encouraging VLMs to learn rich vision, language and vision-language contexts for better zero-shot predictions.</li>
<li>Generally adopted as additional objectives above other VLM pre-training objectives for learning rich context information.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_IhMp" id=masked-image-modelling>Masked Image Modelling<a href=#masked-image-modelling class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Cross-patch correlation by masking and reconstructing images.</li>
<li><strong>Learn image context information by masking and reconstructing images</strong>
<ul>
<li>MAE, BeiT: certain patches in an image are masked and the encoder is trained to reconstruct them conditioned on unmasked patches.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Masked Image Modelling" src=/en/assets/images/vlm-masked-image-modelling-0b2757747dbb1e20d94920022ed416e1.png width=936 height=424 class=img_f7zd></p>
<h4 class="anchor anchorWithStickyNavbar_IhMp" id=masked-language-modelling>Masked Language Modelling<a href=#masked-language-modelling class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Adopted pre-training objectives in NLP.</li>
<li>Randomly masking a certain percentage of input tokens and predicting them. (15% in BERT)</li>
<li><strong>Learn by masking a fraction of tokens</strong> in each input text and training networks to predict the masked tokens.<!-- -->
<ul>
<li>FLAVA: masks out 15% text tokens and reconstructs them from the rest tokens for modelling cross-word correlation.</li>
<li>FIBER: adopts masked language modelling as one of the VLM pre-training objectives to extract better language features.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Masked Language Modelling" src=/en/assets/images/vlm-masked-language-modelling-4673252375ddfbf207081514c51e2eae.png width=868 height=454 class=img_f7zd></p>
<h4 class="anchor anchorWithStickyNavbar_IhMp" id=masked-cross-modal-modelling>Masked Cross-Modal Modelling<a href=#masked-cross-modal-modelling class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Integrates masked image modelling and masked language modelling.</li>
<li>Given an image-text pair, it randomly masks a subset of image patches and a subset of text tokens and then learns to reconstruct them.</li>
<li><strong>Learn by masking a certain percentage of image patches and text tokens</strong> and training VLMs to reconstruct them based on the embeddings of unmasked image patches and text tokens.<!-- -->
<ul>
<li>FLAVA: 40% image patches and 15% text tokens as in, and employs a MLP to predict masked patched and tokens, capturing rich vision-language correspondence information.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_IhMp" id=image-to-text-generation>Image-to-Text Generation<a href=#image-to-text-generation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li><strong>Generate descriptive texts for a given image</strong> for capturing fine-grained vision-language correlation by training VLMs to predict tokenized texts.<!-- -->
<ul>
<li>COCA, NLP, PaLI: train VLMs with the standard encoder-decoder architecture and image captioning objectives.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Image to caption" src=/en/assets/images/vlm-image-to-caption-4348d37878d43e85e49b73006131f2c2.png width=924 height=712 class=img_f7zd></p>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=alignment-objectives>Alignment Objectives<a href=#alignment-objectives class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<blockquote>
<p>Align image–text pairs in the embedding space.</p>
</blockquote>
<ul>
<li>Image-Text Matching<!-- -->
<ul>
<li>models the <strong>overall correlation</strong> between an entire image and an entire sentence. (전역적 상관관계)</li>
</ul>
</li>
<li>captures <strong>fine-grained correlations</strong> between image regions and specific words. (지역적 상관관계)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=vlm-pre-training-frameworks>VLM Pre-Training Frameworks<a href=#vlm-pre-training-frameworks class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p><img decoding=async loading=lazy alt="VLM pre-training frameworks" src=/en/assets/images/vlm-pretraining-frameworks-39991946607c915ff2a7126f16d59c30.png width=715 height=273 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=evaluation>Evaluation<a href=#evaluation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=zero-shot-prediction>Zero-shot Prediction<a href=#zero-shot-prediction class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Image Classification: classify images into pre-defined categories like "prompt engineering".</li>
<li>Semantic Segmentation: by comparing the embeddings of the given image pixels and texts.</li>
<li>Object Detection: localize and classify objects in images with the object locating ability learned from auxiliary datasets.</li>
<li>Image-Text Retrieval<!-- -->
<ul>
<li>Text-to-image retrieval that retrieves images based on texts</li>
<li>Image-to-text retrieval that retrieves texts based on images.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=linear-probing>Linear Probing<a href=#linear-probing class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>freezes the pre-trained VLM</li>
<li>trains a linear classifier to classify the VLM-encoded embeddings to assess the VLM representations.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=datasets>Datasets<a href=#datasets class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>For Pre-training VLMs<!-- -->
<ul>
<li>CLIP, 2021, 400M, English</li>
<li>ALIGN, 2021, 1.8B, English</li>
<li>FILIP, 2021, 300M, English</li>
<li>WebLi, 2022, 12B, 129 Languages</li>
</ul>
</li>
<li>For VLM Evaluation<!-- -->
<ul>
<li>Image Classification<!-- -->
<ul>
<li>PSACAL VOC 2007 Classification, 11-point mAP</li>
<li>Oxford-IIIT PETS, Mean Per Class</li>
<li>EuroSAT, Accuracy</li>
<li>Hateful Memes, ROC AUC</li>
<li>Country211, Accuracy</li>
</ul>
</li>
<li>Image-Text Retrieval<!-- -->
<ul>
<li>Flickr30k, Recall</li>
<li>COCO Caption, Recall</li>
</ul>
</li>
<li>Action Recognition<!-- -->
<ul>
<li>UCF101, Accuracy</li>
<li>Kinetics700, Mean(top1, top5)</li>
<li>RareAct, mWAP, mSAP</li>
</ul>
</li>
<li>Object Detection<!-- -->
<ul>
<li>COCO 2017 Detection, box mAP</li>
<li>LVIS, box mAP</li>
<li>ODinW, box mAP</li>
</ul>
</li>
<li>Semantic Segmentation<!-- -->
<ul>
<li>Cityscapes, Mean IoU</li>
<li>ADE20K, Mean IoU</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Zhang, J., Huang, J., Jin, S., & Lu, S. (2024). Vision-Language Models for Vision Tasks: A Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(8), 5625–5644. <code>&lt;https://doi.org/10.1109/TPAMI.2024.3369699></code></li>
</ul></div><footer class=docusaurus-mt-lg><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href=/en/2025/08/16/introduction-to-ai-003/><div class=pagination-nav__sublabel>Older Post</div><div class=pagination-nav__label>Introduction to AI @003</div></a></nav></main><div class="col col--2"><div class="tableOfContents_qoed thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#overview class="table-of-contents__link toc-highlight">Overview</a><li><a href=#the-development-of-visual-recognition-paradigms class="table-of-contents__link toc-highlight">The development of visual recognition paradigms</a><li><a href=#vlm-overview class="table-of-contents__link toc-highlight">VLM Overview</a><li><a href=#vlm-pre-training-objectives class="table-of-contents__link toc-highlight">VLM pre-training Objectives</a><ul><li><a href=#contrastive-objectives class="table-of-contents__link toc-highlight">Contrastive Objectives</a><li><a href=#generative-objectives class="table-of-contents__link toc-highlight">Generative Objectives</a><li><a href=#alignment-objectives class="table-of-contents__link toc-highlight">Alignment Objectives</a><li><a href=#vlm-pre-training-frameworks class="table-of-contents__link toc-highlight">VLM Pre-Training Frameworks</a></ul><li><a href=#evaluation class="table-of-contents__link toc-highlight">Evaluation</a><ul><li><a href=#zero-shot-prediction class="table-of-contents__link toc-highlight">Zero-shot Prediction</a><li><a href=#linear-probing class="table-of-contents__link toc-highlight">Linear Probing</a></ul><li><a href=#datasets class="table-of-contents__link toc-highlight">Datasets</a><li><a href=#ref class="table-of-contents__link toc-highlight">Ref</a></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Support Me</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://www.buymeacoffee.com/LOUB2kN target=_blank rel="noopener noreferrer" style="cursor: pointer;">
                <img src=https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png style="height: auto !important;width: auto !important;">
              </a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Feeds</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://gracefullight.dev/rss.xml target=_blank rel="noopener noreferrer" class=footer__link-item>RSS<svg width=13.5 height=13.5 aria-hidden=true class=iconExternalLink_E7SL><use href=#theme-svg-external-link /></svg></a><li class=footer__item><a href=https://gracefullight.dev/atom.xml target=_blank rel="noopener noreferrer" class=footer__link-item>Atom<svg width=13.5 height=13.5 aria-hidden=true class=iconExternalLink_E7SL><use href=#theme-svg-external-link /></svg></a></ul></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2016 - 2023 Gracefullight. Built with Docusaurus.</div></div></div></footer></div>