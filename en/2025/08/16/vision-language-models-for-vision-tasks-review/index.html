<!doctype html><html lang=en dir=ltr class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated=false><head><meta charset=UTF-8><meta name=generator content="Docusaurus v3.9.2"><title data-rh=true>Vision-Language Models for Vision Tasks Review | gracefullight.dev</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"/><meta data-rh=true name=twitter:card content=summary_large_image /><meta data-rh=true property=og:url content=https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review/ /><meta data-rh=true property=og:locale content=en /><meta data-rh=true property=og:locale:alternate content=ko /><meta data-rh=true name=docusaurus_locale content=en /><meta data-rh=true name=docusaurus_tag content=default /><meta data-rh=true name=docsearch:language content=en /><meta data-rh=true name=docsearch:docusaurus_tag content=default /><meta data-rh=true content=gcY9SiftHgQoJjBZ7IgwNNN5_atLPAX6kWb1nFVfa6E name=google-site-verification /><meta data-rh=true content=65AD1E28C0D057CEB3C68FBC0293E55B name=msvalidate.01 /><meta data-rh=true content=d024c2837887f72dc7b3792b958be74d69ba9593 name=naver-site-verification /><meta data-rh=true content=f7c93483a6f87c79 name=yandex-verification /><meta data-rh=true content=yZEdU1ABcR name=baidu-site-verification /><meta data-rh=true content=uelupjqqsm5egzlhy1aev2rfxow5yt name=facebook-domain-verification /><meta data-rh=true property=og:title content="Vision-Language Models for Vision Tasks Review | gracefullight.dev"/><meta data-rh=true name=description content="Vision-Language Models for Vision Tasks Review"/><meta data-rh=true property=og:description content="Vision-Language Models for Vision Tasks Review"/><meta data-rh=true property=og:type content=article /><meta data-rh=true property=article:published_time content=2025-08-16T07:40:55.588Z /><meta data-rh=true property=article:author content=https://github.com/gracefullight /><meta data-rh=true property=article:tag content=vlm /><link data-rh=true rel=icon href=/en/img/favicon.ico /><link data-rh=true rel=canonical href=https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review/ /><link data-rh=true rel=alternate href=https://gracefullight.dev/2025/08/16/vision-language-models-for-vision-tasks-review/ hreflang=ko /><link data-rh=true rel=alternate href=https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review/ hreflang=en /><link data-rh=true rel=alternate href=https://gracefullight.dev/2025/08/16/vision-language-models-for-vision-tasks-review/ hreflang=x-default /><link data-rh=true rel=preconnect href=https://RFS69RSYOJ-dsn.algolia.net crossorigin=anonymous /><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@id":"https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review","@type":"BlogPosting","author":{"@type":"Person","description":"Owner","image":"https://avatars.githubusercontent.com/u/11773683?v=4","name":"Gracefullight","url":"https://github.com/gracefullight"},"datePublished":"2025-08-16T07:40:55.588Z","description":"Vision-Language Models for Vision Tasks Review","headline":"Vision-Language Models for Vision Tasks Review","isPartOf":{"@id":"https://gracefullight.dev/en/","@type":"Blog","name":"Blog"},"keywords":[],"mainEntityOfPage":"https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review","name":"Vision-Language Models for Vision Tasks Review","url":"https://gracefullight.dev/en/2025/08/16/vision-language-models-for-vision-tasks-review"}</script><link rel=alternate type=application/rss+xml href=/en/rss.xml title="gracefullight.dev RSS Feed"><link rel=alternate type=application/atom+xml href=/en/atom.xml title="gracefullight.dev Atom Feed"><link rel=alternate type=application/json href=/en/feed.json title="gracefullight.dev JSON Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-E99DNE7S05"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-E99DNE7S05",{})</script><link rel=search type=application/opensearchdescription+xml title=gracefullight.dev href=/en/opensearch.xml><link href=/en/img/favicon-32x32.png rel=icon><link href=/en/manifest.json rel=manifest><meta content=#f28913 name=theme-color><meta content=yes name=mobile-web-app-capable><meta content=#f28913 name=apple-mobile-web-app-status-bar-style><link href=/en/img/apple-touch-icon.png rel=apple-touch-icon><link rel=preconnect href=https://pagead2.googlesyndication.com><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3004788392777865" async crossorigin=anonymous></script><link rel=preconnect href=https://www.clarity.ms><script>!function(t,e,n,a,c,i,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(i=e.createElement(a)).async=1,i.src="https://www.clarity.ms/tag/"+c,(r=e.getElementsByTagName(a)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script","aongv9xgi6")</script><link rel=preconnect href=https://wcs.naver.net><script src=https://wcs.naver.net/wcslog.js async></script><script>if(!wcs_add)var wcs_add={};wcs_add.wa="156bc73a81e3bd0",window.wcs&&wcs_do()</script><link rel=preconnect href=https://cdn.channel.io><script>!function(){var n=window;if(n.ChannelIO)return(window.console.error||window.console.log||function(){})("ChannelIO script included twice.");var e=function(){e.c(arguments)};function t(){if(!n.ChannelIOInitialized){n.ChannelIOInitialized=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cdn.channel.io/plugin/ch-plugin-web.js",e.charset="UTF-8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}}e.q=[],e.c=function(n){e.q.push(n)},n.ChannelIO=e,"complete"===document.readyState?t():window.attachEvent?window.attachEvent("onload",t):(window.addEventListener("DOMContentLoaded",t,!1),window.addEventListener("load",t,!1))}(),ChannelIO("boot",{pluginKey:"0fd130ba-a1a6-4b7e-802a-e82a885a7fd8"})</script><link rel=preconnect href=https://static.cloudflareinsights.com><script src=https://static.cloudflareinsights.com/beacon.min.js defer data-cf-beacon='{"token":"c0899829e72b45e98dff77241127252c"}'></script><link href=https://mc.yandex.ru rel=preconnect><script>!function(e,t,c,n,r,a,s){e[r]=e[r]||function(){(e[r].a=e[r].a||[]).push(arguments)},e[r].l=+new Date;for(var i=0;i<document.scripts.length;i++)if(document.scripts[i].src===n)return;a=t.createElement(c),s=t.getElementsByTagName(c)[0],a.async=1,a.src=n,s.parentNode.insertBefore(a,s)}(window,document,"script","https://mc.yandex.ru/metrika/tag.js?id=104072655","ym"),ym(0x63405cf,"init",{ssr:!0,clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!1})</script><link href=https://cdn.jsdelivr.net rel=preconnect><script type=application/ld+json>{"@context":"http://schema.org","@type":"Person","email":"mailto:gracefullight.dev@gmail.com","image":"https://avatars.githubusercontent.com/u/11773683?v=4","jobTitle":"FullStack JavaScript Developer","logo":"https://gracefullight.dev/img/apple-touch-icon.png","name":"Eunkwang Shin","nationality":"Korean","sameAs":["https://github.com/gracefullight","https://linkedin.com/in/gracefullight"],"url":"https://gracefullight.dev"}</script><link rel=stylesheet crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css type=text/css><link rel=stylesheet href=/en/assets/css/styles.60923ef7.css /><script src=/en/assets/js/runtime~main.1b51a719.js defer></script><script src=/en/assets/js/main.bdbba33d.js defer></script></head><body class=navigation-with-keyboard><svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/favicon-32x32.png /><link rel=preload as=image href="https://avatars.githubusercontent.com/u/11773683?v=4"/><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_mlkZ themedComponent--light_NVdE"/><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"/></div><b class="navbar__title text--truncate">gracefullight.dev</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/2025/08/16/vision-language-models-for-vision-tasks-review/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ko>한국어</a><li><a href=/en/2025/08/16/vision-language-models-for-vision-tasks-review/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a></ul></div><a class="navbar__item navbar__link" href=/en/archive/>Archives</a><a class="navbar__item navbar__link" href=/en/tags/>Tags</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts=Meta+k><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 24 24" aria-hidden=true><circle cx=11 cy=11 r=8 stroke=currentColor fill=none stroke-width=1.4 /><path d="m21 21-4.3-4.3" stroke=currentColor fill=none stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class=row><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role=group><h3 class=yearGroupHeading_rMGB>2026</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/27/free-up-storage-space-on-mac/>Free up storage space on mac</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/27/promoting-an-opensource-project/>Promoting an opensource project</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/24/iqc-002/>IQC 002</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/23/tim-002/>TIM 002</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/17/innovation-tactics/>Innovation Tactics</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/01/31/agentic-sdlc/>Agentic SDLC</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/01/29/local-docker-env/>로컬 도커 환경 툴 비교</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-01/>Phrasal Verbs 01</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-014/>Phrasal Verbs 014</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-013/>Phrasal Verbs 013</a></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class=title_f1Hy>Vision-Language Models for Vision Tasks Review</h1><div class="container_mt6G margin-vert--md"><time datetime=2025-08-16T07:40:55.588Z>August 16, 2025</time> · <!-- -->16 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div id=__blog-post-container class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=overview>Overview<a href=#overview class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<blockquote>
<p>Most visual recognition studies rely heavily on crowdlabelled data in DNN</p>
</blockquote>
<ul>
<li class="">Background development of visual recognition paradigms</li>
<li class="">Foundations its architecture</li>
<li class="">Datasets in VLM pre-training and evaluations</li>
<li class="">Review and categorization of existing pre-training methods</li>
<li class="">Benchmarking analysis discussion</li>
<li class="">Reach challenges & potential research direction</li>
<li class="">Training hard<!-- -->
<ul>
<li class="">New learning paradigm</li>
</ul>
</li>
<li class="">Vision-Language Model Pre-training and Zero-shot Prediction<!-- -->
<ul>
<li class="">Increasing attention</li>
</ul>
</li>
<li class="">VLMs with transfer learning<!-- -->
<ul>
<li class="">Prompt tuning</li>
<li class="">Visual adaption</li>
</ul>
</li>
<li class="">VLMs with knowledge distillation<!-- -->
<ul>
<li class="">distill knowledge from VLMs to downstream tasks</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=the-development-of-visual-recognition-paradigms>The development of visual recognition paradigms<a href=#the-development-of-visual-recognition-paradigms class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<!-- -->
<ul>
<li class="">Traditional ML: Hand-crafted features for prediction.</li>
<li class="">Deep Learning: Deep networks (e.g., ResNet) with large-scale labeled data.</li>
<li class="">Supervised Pre-training + Fine-tuning: Learned representations transferred to downstream tasks.</li>
<li class="">Unsupervised / Self-supervised Pre-training + Fine-tuning: Objectives like masked modeling and contrastive learning to learn representations.</li>
<li class="">Vision-Language Models & Zero-shot: Leverage large-scale web data, enabling zero-shot prediction without task-specific fine-tuning.<!-- -->
<ul>
<li class="">Collecting large-scale informative image-text data</li>
<li class="">Designing high-capacity models for effective learning from Bigdata.</li>
<li class="">Designing new pre-training objectives for learning effective VLMs.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Illustration of development of VLMs for visual recognition" src=/en/assets/images/vlm-paradigm-46050660982130f887307cc0442975c0.png width=701 height=347 class=img_ev3q /></p>
<ul>
<li class="">CLIP: Image-text contrastive objective and learns by pulling the paired images and texts close and pushing others faraway in the embedding space.<!-- -->
<ul>
<li class="">enables effective usage of web data and allows zero-shot predictions without task-specific finetuning.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=vlm-overview>VLM Overview<a href=#vlm-overview class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<p><img decoding=async loading=lazy alt="VLM Overview" src=/en/assets/images/vlm-overview-2abb2d65aeef690c49e399a7ca3ac86c.png width=1400 height=728 class=img_ev3q /></p>
<ul>
<li class="">Given Image-text pairs.</li>
<li class="">Employs a text encoder and an image encoder to extract image and text features.</li>
<li class="">Learns the vision-language correlation with certain pre-training objectives.</li>
<li class="">GAP: Global Average Pooling, a technique used to reduce the spatial dimensions of feature maps while retaining important information.</li>
<li class="">ViT: Vision Transformer: Transformers for image recognition at scale.</li>
<li class="">CNN Based: VGG, <strong>ResNet</strong>, EfficientNet<!-- -->
<ul>
<li class="">ResNet: Adopts skip connections between convolutional blocks which mitigates gradient vanishing and explosion and enables DNN training.</li>
<li class="">ResNet-D: Replace global average pooling with transformer multi-head attention.</li>
</ul>
</li>
<li class="">Transformer Based: ViT<!-- -->
<ul>
<li class="">Adding a normalization layer before the transformer encoder.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=vlm-pre-training-objectives>VLM pre-training Objectives<a href=#vlm-pre-training-objectives class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=contrastive-objectives>Contrastive Objectives<a href=#contrastive-objectives class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Pros<!-- -->
<ul>
<li class="">Enforce positive pairs to have similar embeddings in contrast to negative pairs.</li>
<li class="">Encourages VLMs to learn discriminative vision and language features, where more discriminative features lead to more confident and accurate zero-shot predictions.</li>
</ul>
</li>
<li class="">Cons<!-- -->
<ul>
<li class="">Joint optimizing positive and negative pairs is complicated and challenging.</li>
<li class="">Involves a heuristic temperature hyper-parameter for controlling the feature discriminability.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=image-contrastive-learning>Image Contrastive Learning<a href=#image-contrastive-learning class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">Forcing a query image to be close with its positive keys (its data augmentations)</li>
<li class="">Faraway from its negative keys (other images)</li>
<li class=""><strong>Learn discriminative features</strong> in image modality, which often serves as an auxiliary objective for fully exploiting the image data potential.</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=image-text-contrastive-learning>Image-Text Contrastive Learning<a href=#image-text-contrastive-learning class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">Pulling the embeddings of paired images and texts close while pushing others away.</li>
<li class="">Minimizing a symmetrical image-text infoNCE loss</li>
<li class=""><strong>Learn vision-language correlation</strong> by contrasting image-text pairs.<!-- -->
<ul>
<li class="">CLIP: A symmetrical image-text infoNCE loss</li>
<li class="">ALIGN: scales up the VLM pre-training with large-scale (but noisy image-text pair with noise-robust contrastive learning)</li>
<li class="">DeCLIP: Nearest-neighbor supervision to utilize the information from similar pairs, enabling effective pre-training on limited data.</li>
<li class="">OTTER: Optimal transport to pseudo-pair images and texts reducing the required training data.</li>
<li class="">ZeroVL: Limited data resource via debiased data sampling and data augmentation with coin flipping mixup.</li>
<li class="">FILIP: Region-word alignment into contrastive learning, enabling to learn fine-grained vision-language corresponding knowledge.</li>
<li class="">Pyramid-CLIP: Multiple semantic levels and performs both cross-level and peer-level contrastive learning for effective VLM pre-training.</li>
<li class="">LA-CLIP, ALIP: LLM to augment synthetic captions for given images while RA-CLIP retrieves relevant image-text pairs for image-text pair augmentation.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt=CLIP src=/en/assets/images/vlm-clip-002c83bc065d184a2350741cacc71908.png width=848 height=528 class=img_ev3q /></p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=image-text-label-contrastive-learning>Image-Text-Label Contrastive Learning<a href=#image-text-label-contrastive-learning class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">Supervised Contrastive Learning into image-text contrastive learning.</li>
<li class=""><strong>Learn discriminative and task-specific features</strong> by exploiting both supervised labels and unsupervised image-text pairs.<!-- -->
<ul>
<li class="">UniCL: pre-training allows learning both discriminative and task-specific (image classification) features simultaneously with around 900M image-text pairs.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Image-Text-Label Contrastive Learning" src=/en/assets/images/vlm-image-text-label-83c01a33a07520e54ac28a0ffbb1ceaa.png width=846 height=558 class=img_ev3q /></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=generative-objectives>Generative Objectives<a href=#generative-objectives class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Encouraging VLMs to learn rich vision, language and vision-language contexts for better zero-shot predictions.</li>
<li class="">Generally adopted as additional objectives above other VLM pre-training objectives for learning rich context information.</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=masked-image-modelling>Masked Image Modelling<a href=#masked-image-modelling class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">Cross-patch correlation by masking and reconstructing images.</li>
<li class=""><strong>Learn image context information by masking and reconstructing images</strong>
<ul>
<li class="">MAE, BeiT: certain patches in an image are masked and the encoder is trained to reconstruct them conditioned on unmasked patches.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Masked Image Modelling" src=/en/assets/images/vlm-masked-image-modelling-0b2757747dbb1e20d94920022ed416e1.png width=936 height=424 class=img_ev3q /></p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=masked-language-modelling>Masked Language Modelling<a href=#masked-language-modelling class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">Adopted pre-training objectives in NLP.</li>
<li class="">Randomly masking a certain percentage of input tokens and predicting them. (15% in BERT)</li>
<li class=""><strong>Learn by masking a fraction of tokens</strong> in each input text and training networks to predict the masked tokens.<!-- -->
<ul>
<li class="">FLAVA: masks out 15% text tokens and reconstructs them from the rest tokens for modelling cross-word correlation.</li>
<li class="">FIBER: adopts masked language modelling as one of the VLM pre-training objectives to extract better language features.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Masked Language Modelling" src=/en/assets/images/vlm-masked-language-modelling-4673252375ddfbf207081514c51e2eae.png width=868 height=454 class=img_ev3q /></p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=masked-cross-modal-modelling>Masked Cross-Modal Modelling<a href=#masked-cross-modal-modelling class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">Integrates masked image modelling and masked language modelling.</li>
<li class="">Given an image-text pair, it randomly masks a subset of image patches and a subset of text tokens and then learns to reconstruct them.</li>
<li class=""><strong>Learn by masking a certain percentage of image patches and text tokens</strong> and training VLMs to reconstruct them based on the embeddings of unmasked image patches and text tokens.<!-- -->
<ul>
<li class="">FLAVA: 40% image patches and 15% text tokens as in, and employs a MLP to predict masked patched and tokens, capturing rich vision-language correspondence information.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=image-to-text-generation>Image-to-Text Generation<a href=#image-to-text-generation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class=""><strong>Generate descriptive texts for a given image</strong> for capturing fine-grained vision-language correlation by training VLMs to predict tokenized texts.<!-- -->
<ul>
<li class="">COCA, NLP, PaLI: train VLMs with the standard encoder-decoder architecture and image captioning objectives.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Image to caption" src=/en/assets/images/vlm-image-to-caption-4348d37878d43e85e49b73006131f2c2.png width=924 height=712 class=img_ev3q /></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=alignment-objectives>Alignment Objectives<a href=#alignment-objectives class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<blockquote>
<p>Align image–text pairs in the embedding space.</p>
</blockquote>
<ul>
<li class="">pros<!-- -->
<ul>
<li class="">simple, easy to optimize</li>
<li class="">can be easily extended to model fine-grained vision-language correlation</li>
</ul>
</li>
<li class="">cons<!-- -->
<ul>
<li class="">little correlation information within vision or language modality.</li>
</ul>
</li>
<li class="">adopted as auxiliary losses to other VLM pre-training objectives for enhancing modelling the correlation across vision and language modalities.</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=image-text-matching>Image-Text Matching<a href=#image-text-matching class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">models the <strong>overall correlation</strong> between an entire image and an entire sentence. (전역적 상관관계)</li>
<li class="">Image-text matching models global image-text correlation by directly aligning paired images and texts<!-- -->
<ul>
<li class="">FLAVA: matches the given image with its paired text via a classifier and a binary classification loss.</li>
<li class="">FIBER: follows to mine hard negatives with pair-wise similarities for better alignment between image and text.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=region-word-matching>Region-Word Matching<a href=#region-word-matching class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">captures <strong>fine-grained correlations</strong> between image regions and specific words. (지역적 상관관계)</li>
<li class="">models local fine-grained vision-language correlation by aligning paired image regions and word tokens.</li>
<li class="">benefiting <strong>zero-shot dense predictions</strong> in object detection and semantic segmentation.<!-- -->
<ul>
<li class="">GLIP, FIBER, DetCLIP: replace object classification logits by region-word alignment scores.<!-- -->
<ul>
<li class="">the dot-product similarity between regional visual features and token-wise features.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Region-Word Matching, GLIP" src=/en/assets/images/vlm-region-word-578b649828e666c5d4d0a9ca7af22e51.png width=942 height=538 class=img_ev3q /></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=vlm-pre-training-frameworks>VLM Pre-Training Frameworks<a href=#vlm-pre-training-frameworks class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<p><img decoding=async loading=lazy alt="VLM pre-training frameworks" src=/en/assets/images/vlm-pretraining-frameworks-39991946607c915ff2a7126f16d59c30.png width=715 height=273 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=evaluation>Evaluation<a href=#evaluation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=zero-shot-prediction>Zero-shot Prediction<a href=#zero-shot-prediction class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Image Classification: classify images into pre-defined categories like "prompt engineering".</li>
<li class="">Semantic Segmentation: by comparing the embeddings of the given image pixels and texts.</li>
<li class="">Object Detection: localize and classify objects in images with the object locating ability learned from auxiliary datasets.</li>
<li class="">Image-Text Retrieval<!-- -->
<ul>
<li class="">Text-to-image retrieval that retrieves images based on texts</li>
<li class="">Image-to-text retrieval that retrieves texts based on images.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=linear-probing>Linear Probing<a href=#linear-probing class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">freezes the pre-trained VLM</li>
<li class="">trains a linear classifier to classify the VLM-encoded embeddings to assess the VLM representations.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=datasets>Datasets<a href=#datasets class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">For Pre-training VLMs<!-- -->
<ul>
<li class="">CLIP, 2021, 400M, English</li>
<li class="">ALIGN, 2021, 1.8B, English</li>
<li class="">FILIP, 2021, 300M, English</li>
<li class="">WebLi, 2022, 12B, 129 Languages</li>
</ul>
</li>
<li class="">For VLM Evaluation<!-- -->
<ul>
<li class="">Image Classification<!-- -->
<ul>
<li class="">PSACAL VOC 2007 Classification, 11-point mAP</li>
<li class="">Oxford-IIIT PETS, Mean Per Class</li>
<li class="">EuroSAT, Accuracy</li>
<li class="">Hateful Memes, ROC AUC</li>
<li class="">Country211, Accuracy</li>
</ul>
</li>
<li class="">Image-Text Retrieval<!-- -->
<ul>
<li class="">Flickr30k, Recall</li>
<li class="">COCO Caption, Recall</li>
</ul>
</li>
<li class="">Action Recognition<!-- -->
<ul>
<li class="">UCF101, Accuracy</li>
<li class="">Kinetics700, Mean(top1, top5)</li>
<li class="">RareAct, mWAP, mSAP</li>
</ul>
</li>
<li class="">Object Detection<!-- -->
<ul>
<li class="">COCO 2017 Detection, box mAP</li>
<li class="">LVIS, box mAP</li>
<li class="">ODinW, box mAP</li>
</ul>
</li>
<li class="">Semantic Segmentation<!-- -->
<ul>
<li class="">Cityscapes, Mean IoU</li>
<li class="">ADE20K, Mean IoU</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=vlm-transfer-learning>VLM Transfer learning<a href=#vlm-transfer-learning class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<blockquote>
<p>which adapts VLMs to fit downstream tasks via prompt tuning, feature adapter.</p>
</blockquote>
<ul>
<li class="">image and text distributions gap: downstream dataset may have task-specific image styles and text formats</li>
<li class="">training objectives gap: VLMs are generally trained with task-agnostic objectives, while downstream tasks often involve task-specific objectives. (coarse or fine-grained classification, region or pixel-level recognition)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=transfer-via-prompt-tuning>Transfer via Prompt Tuning<a href=#transfer-via-prompt-tuning class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<blockquote>
<p>Inspired by the "prompt learning" in NLP</p>
</blockquote>
<ul>
<li class="">pros<!-- -->
<ul>
<li class="">simple, easy-to-implement</li>
<li class="">requires little extra network layer or complex network modifications</li>
<li class="">adapting VLMs in a black-box manner, which has clear advantages in transferring VLMs that involve concerns in intellectual property.</li>
</ul>
</li>
<li class="">cons<!-- -->
<ul>
<li class="">low flexibility by following the manifold (잠재 공간) of the original VLMs in prompting.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=transfer-with-text-prompt-tuning>Transfer with Text Prompt Tuning<a href=#transfer-with-text-prompt-tuning class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">Exploring more effective and efficient learnable text prompts with several labelled downstream samples for each class.<!-- -->
<ul>
<li class=""><strong>supervised and few-shot supervised</strong>
<ul>
<li class="">CoOp: Exploring context optimization to learn context words for a single class name with learnable word vectors.</li>
<li class="">CoCoOp: Exploring conditional context optimization that generates a specific prompt for each image.</li>
<li class="">SubPT: designs subspace prompt tuning to improve the generalization of learned prompts.</li>
<li class="">LASP: regularizes learnable prompts with hand-engineered prompts.</li>
<li class="">VPT: models text prompts with instance-specific distribution with better generalization on downstream tasks.</li>
<li class="">KgCoOp: enhances the generalization of unseen class by mitigating the forgetting of textual knowledge.</li>
<li class="">SoftCPT: fine-tunes VLMs on multiple few-shot tasks simultaneously for benefiting from multi-task learning.</li>
<li class="">PLOT: employs optimal transport to learn multiple prompts to describe the diverse characteristics of a category.</li>
<li class="">DualCoOp, TaI-DP: transport VLMs to multi-label classification tasks.<!-- -->
<ul>
<li class="">DualCoOp: adopts both positive and negative prompts for multi-label classification</li>
<li class="">TaI-DP: double-grained prompt tuning for capturing both coarse-grained and fine-grained embeddings.</li>
</ul>
</li>
<li class="">DenseCLIP: explores language-guided fine-tuning that employs visual features to tune text prompts for dense prediction.</li>
<li class="">ProTeCt: improves the consistency of model predictions for hierarchical classification task.</li>
</ul>
</li>
<li class=""><strong>unsupervised</strong>
<ul>
<li class="">UPL: optimizes learnable prompts with self-training on selected pseudo-labeled samples.</li>
<li class="">TPT: explores test-time prompt tuning to learn adaptive prompts from a single downstream sample.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Text Prompt Tuning" src=/en/assets/images/vlm-text-prompt-tuning-90bb346d2a9f42bf7769b169254da344.png width=600 height=476 class=img_ev3q /></p>
<ul>
<li class=""><code>V</code> is learnable word vectors that are optimized by minimizing the classification loss with the downstream samples.</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=transfer-with-visual-prompt-tuning>Transfer with Visual Prompt Tuning<a href=#transfer-with-visual-prompt-tuning class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">Transfers VLMs by modulating the input of image encoder.<!-- -->
<ul>
<li class="">VP: adopts learnable image perturbations <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>v</mi></mrow><annotation encoding=application/x-tex>v</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.03588em>v</span></span></span></span> to modify the input image <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mi>x</mi><mi>I</mi></msup></mrow><annotation encoding=application/x-tex>x^I</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8413em></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8413em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.07847em>I</span></span></span></span></span></span></span></span></span></span></span> by <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mi>x</mi><mi>I</mi></msup><mo>+</mo><mi>v</mi></mrow><annotation encoding=application/x-tex>x^I + v</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9247em;vertical-align:-0.0833em></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8413em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.07847em>I</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.03588em>v</span></span></span></span>, aiming to adjust <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>v</mi></mrow><annotation encoding=application/x-tex>v</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.03588em>v</span></span></span></span> to minimize a recognition loss.</li>
<li class="">RePrompt: integrates retrieval mechanisms into visual prompt tuning, allowing leveraging the knowledge from downstream tasks.</li>
</ul>
</li>
<li class="">enables pixel-level adaptation to downstream tasks, benefiting them greatly especially for dense prediction tasks.</li>
</ul>
<p><img decoding=async loading=lazy alt="Visual Prompt Tuning" src=/en/assets/images/vlm-visual-prompt-tuning-36a78e501672cf89040235b31cd0c448.png width=632 height=478 class=img_ev3q /></p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=transfer-with-text-visual-prompt-tuning>Transfer with Text-Visual Prompt Tuning<a href=#transfer-with-text-visual-prompt-tuning class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">modulate the text and image inputs simultaneously, benefiting from joint prompt optimization on multiple modalities.<!-- -->
<ul>
<li class="">UPT: unifies prompt tuning to jointly optimize text and image prompts, demonstrating the complementary nature of the two prompt tuning tasks.</li>
<li class="">MVLPT: explores multi-task vision-language prompt tuning to incorporate cross-task knowledge into text and image prompt tuning.</li>
<li class="">MAPLE: conducts multi-modal prompt tuning by aligning visual prompts with their corresponding language prompts, enabling a mutual promotion between text prompts and image prompts.</li>
<li class="">CAVPT: introduces a cross attention between class-aware visual prompts and text prompts, encouraging the visual prompts to concentrate more on visual concepts.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=transfer-via-feature-adaptation>Transfer via Feature Adaptation<a href=#transfer-via-feature-adaptation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">adapt image or text features with an additional light-weight feature adapter<!-- -->
<ul>
<li class="">Clip-Adapter: inserts several trainable linear layers after CLIP's language and image encoders and optimized them while keeping CLIP architecture and parameters frozen.</li>
<li class="">Tip-adapter: a training-free adapter that directly employs the embeddings of few-shot labelled images as the adapter weights.</li>
<li class="">SVL-Adapter: a self-supervised adapter which employs an additional encoder for self-supervised learning on input images.</li>
</ul>
</li>
<li class="">flexible and effective as its architecture and the insertion manner allow tailoring flexibly for different and complex downstream tasks.</li>
<li class="">requires modifying network architecture and thus can not handle VLMs that have concerns in intellectual property.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=other-transfer-methods>Other Transfer Methods<a href=#other-transfer-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Direct fine-tuning, architecture modification, cross attention<!-- -->
<ul>
<li class="">Wise-FT: combines the weights of a fine-tuned VLM and the original VLM for learning new information from downstream tasks.</li>
<li class="">MaskCLIP: extracts dense image features by modifying the architecture of the CLIP image encoder.</li>
<li class="">VT-CLIP: introduces visual-guided attention to semantically correlate text features with downstream images, leading to a better transfer performance.</li>
<li class="">CALIP: introduces parameter-free attention for effective interaction and communication between visual-guided text features.</li>
<li class="">TaskRes: directly tunes text-based classifier to exploit the old knowledge in the pre-trained VLM.</li>
<li class="">CuPL, VCD: employ large language models like GPT-3 to augment text prompts for learning rich discriminative text information.</li>
</ul>
</li>
</ul>
<p><img decoding=async loading=lazy alt="Feature Adaptation" src=/en/assets/images/vlm-feature-adaptation-de7cf8b41fcfa8659cc658c703b4dbc5.png width=876 height=498 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=vlm-knowledge-distillation>VLM Knowledge Distillation<a href=#vlm-knowledge-distillation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">distils general and robust VLM knowledge to task-specific models without the restriction of VLM architecture, benefiting task-specific designs while tackling various dense prediction tasks.</li>
<li class="">most VLM knowledge distillation methods focus on transferring image-level knowledge to region- or pixel-level tasks such as object detection and semantic segmentation.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=knowledge-distillation-for-object-detection>Knowledge Distillation for Object Detection<a href=#knowledge-distillation-for-object-detection class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">To distill VLM knowledge to enlarge the detector vocabulary</li>
<li class="">To better align image-level and object-level representations<!-- -->
<ul>
<li class="">ViLD: distills VLM knowledge to a two-stage detector whose embedding space is enforced to be consistent with that of CLIP image encoder.</li>
<li class="">HierKD: hierarchical global-local knowledge distillation.</li>
<li class="">RKD: region-based knowledge distillation for better aligning region-level and image-level embeddings.</li>
<li class="">ZSD-YOLO: self-labeling data augmentation for exploiting CLIP for better object detection.</li>
<li class="">OADP: proposal features while transferring contextual knowledge.</li>
<li class="">BARON: uses neighborhood sampling to distill a bag of regions instead of individual regions.</li>
<li class="">RO-ViT: distills information from VLMs for open-vocabulary detection.</li>
</ul>
</li>
<li class="">VLM distillation via prompt learning<!-- -->
<ul>
<li class="">DetPro: a detection prompt technique for learning continuous prompt representations for open-vocabulary object detection.</li>
<li class="">PrompDet: regional prompt learning for aligning word embeddings with regional image embeddings.</li>
<li class="">PB-OVD: trains object detectors with VLM-predicted pseudo bounding boxes.</li>
<li class="">XPM: a robust cross-modal pseudo-labeling strategy that employs VLM-generated pseudo masks for open-vocabulary instance segmentation.</li>
<li class="">P3OVD: prompt-driven self-training that refines the VLM-generated pseudo labels with fine-grained prompt tuning.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=knowledge-distillation-for-semantic-segmentation>Knowledge Distillation for Semantic Segmentation<a href=#knowledge-distillation-for-semantic-segmentation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Leverage VLMs to enlarge the vocabulary of segmentation models, aim to segment pixels described by arbitrary texts. (i.e., any categories of pixels beyond base classes)</li>
<li class="">Tackling the mismatch between image-level and pixel-level representations.<!-- -->
<ul>
<li class="">CLIPSeg: a lightweight transformer decoder to extend CLIP for semantic segmentation.</li>
<li class="">LSeg: maximizes the correlation between CLIP text embeddings and pixel-wise image embedding encoded by segmentation models.</li>
<li class="">ZegCLIP: employs CLIP to generate semantic masks and introduces a relationship descriptor to mitigate overfitting on base classes.</li>
<li class="">MaskCLIP+, SSIW: distill knowledge with VLM-predicted pixel-level pseudo labels.</li>
<li class="">FreeSeg: generates mask proposals first and then performs zero-shot classification for them.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id=knowledge-distillation-for-weakly-supervised-semantic-segmentation>Knowledge distillation for weakly-supervised semantic segmentation<a href=#knowledge-distillation-for-weakly-supervised-semantic-segmentation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h4>
<ul>
<li class="">Leverage both VLMs and weak supervision (e.g., image-level labels) for semantic segmentation.</li>
<li class="">CLIP-ES: employs CLIP to refine the class activation map by designing a softmax function and a class-aware attention-based affinity module for mitigating the category confusion issue.</li>
<li class="">CLIMS: employs CLIP knowledge to generate high-quality class activation maps for better weakly-supervised semantic segmentation.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=performance>Performance<a href=#performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">VLM is largely attributed to three factors: Big data, Big Model, and Task-agnostic learning.</li>
<li class="">Limitations<!-- -->
<ul>
<li class="">When data/model size keeps increasing, the performance saturates and further scaling up won’t improve performance</li>
<li class="">Adopting large-scale data in VLM pre-training necessitates extensive computation resources</li>
<li class="">Adopting large models introduces excessive computation and memory overheads in both training and inference</li>
</ul>
</li>
<li class="">Transfer Learning<!-- -->
<ul>
<li class="">can mitigate the domain gaps by learning from task-specific data, being labelled or unlabelled.</li>
<li class="">Supervised <code>></code> few-shot supervised <code>=</code> unsupervised transfer (overfitting but challenging)</li>
</ul>
</li>
<li class="">Knowledge Distillation<!-- -->
<ul>
<li class="">brings clear performance improvement on detection and segmentation tasks</li>
<li class="">introduces general and robust VLM knowledge while benefiting from task-specific designs</li>
</ul>
</li>
<li class="">the development of VLM pre-training for dense visual recognition tasks (on region or pixel-level detection and segmentation) lag far behind.</li>
<li class="">require certain norms in term of training data, networks and downstream tasks.<!-- -->
<ul>
<li class="">VLM transfer: release their codes and do not require intensive computation resources, easing reproduction and benchmarking.</li>
<li class="">VLM pre-training: studied with different data and networks, making benchmarking a very challenging task. also use non-public training data, or require intensive computation resources.</li>
<li class="">VLM knowledge distillation: adopt different task-specific backbones, which complicates benchmarking.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=challenges>Challenges<a href=#challenges class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">VLM pre-training<!-- -->
<ul>
<li class=""><strong>Fine-grained vision-language correlation modelling</strong>: can better recognize patches and pixels beyond images, greatly benefiting dense prediction tasks</li>
<li class=""><strong>Unification of vision and language learning</strong>: enables efficient communications across data modalities which can benefit both training effectiveness and training efficiency.</li>
<li class=""><strong>Pre-training VLMs with multiple languages</strong>: could introduce bias in term of cultures and regions and hinder VLM applications in other language areas.</li>
<li class=""><strong>Data-efficient VLMs</strong>: instead of merely learning from each image-text pair, more useful information could be learned with the supervision among image-text pairs.</li>
<li class=""><strong>Pre-training VLMs with LLMs</strong>: employ LLMs to augment the texts in the raw image-text pairs, which provides richer language knowledge and helps better learn vision-language correlation.</li>
</ul>
</li>
<li class="">VLM Transfer Learning<!-- -->
<ul>
<li class=""><strong>Unsupervised VLM transfer</strong>: much lower risk of overfitting than few-shot supervised transfer.</li>
<li class=""><strong>VLM transfer with visual prompt/adapter</strong>: Existing studies focus on text prompt learning. Visual prompt learning or visual adapter, which is complementary to text prompting and can enable pixel-level adaptation in various dense prediction tasks.</li>
<li class=""><strong>Test-time VLM transfer</strong>: Existing studies conduct transfer by fine-tuning VLMs on each downstream task (i.e., prompt learning), leading to repetitive efforts while facing many downstream tasks. Adapting prompts on the fly during inference can circumvent the repetitive training in existing VLM transfer.</li>
<li class=""><strong>VLM transfer with LLMs</strong>: Different from prompt engineering and prompt learning, exploit LLMs to generate text prompts that better describe downstream tasks. This approach is automatic and requires little labelled data.</li>
</ul>
</li>
<li class="">VLM knowledge distillation<!-- -->
<ul>
<li class=""><strong>Knowledge distillation from multiple VLMs</strong>: harvest their synergistic effect by coordinating knowledge distillation from multiple VLMs.</li>
<li class=""><strong>Knowledge distillation for other visual recognition tasks</strong>: leverage the knowledge distilled from VLMs to improve performance on other visual recognition tasks. (instance segmentation, panoptic segmentation, person reidentification)</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Zhang, J., Huang, J., Jin, S., & Lu, S. (2024). Vision-Language Models for Vision Tasks: A Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(8), 5625–5644. <code>https://doi.org/10.1109/TPAMI.2024.3369699</code></li>
</ul></div><footer class=docusaurus-mt-lg><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/2025/08/17/trustworthiness-in-vision-language-models-review/><div class=pagination-nav__sublabel>Newer Post</div><div class=pagination-nav__label>Trustworthiness in Vision-Language Models Review</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/2025/08/16/introduction-to-ai-003/><div class=pagination-nav__sublabel>Older Post</div><div class=pagination-nav__label>IAI +003</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#overview class="table-of-contents__link toc-highlight">Overview</a><li><a href=#the-development-of-visual-recognition-paradigms class="table-of-contents__link toc-highlight">The development of visual recognition paradigms</a><li><a href=#vlm-overview class="table-of-contents__link toc-highlight">VLM Overview</a><li><a href=#vlm-pre-training-objectives class="table-of-contents__link toc-highlight">VLM pre-training Objectives</a><ul><li><a href=#contrastive-objectives class="table-of-contents__link toc-highlight">Contrastive Objectives</a><li><a href=#generative-objectives class="table-of-contents__link toc-highlight">Generative Objectives</a><li><a href=#alignment-objectives class="table-of-contents__link toc-highlight">Alignment Objectives</a><li><a href=#vlm-pre-training-frameworks class="table-of-contents__link toc-highlight">VLM Pre-Training Frameworks</a></ul><li><a href=#evaluation class="table-of-contents__link toc-highlight">Evaluation</a><ul><li><a href=#zero-shot-prediction class="table-of-contents__link toc-highlight">Zero-shot Prediction</a><li><a href=#linear-probing class="table-of-contents__link toc-highlight">Linear Probing</a></ul><li><a href=#datasets class="table-of-contents__link toc-highlight">Datasets</a><li><a href=#vlm-transfer-learning class="table-of-contents__link toc-highlight">VLM Transfer learning</a><ul><li><a href=#transfer-via-prompt-tuning class="table-of-contents__link toc-highlight">Transfer via Prompt Tuning</a><li><a href=#transfer-via-feature-adaptation class="table-of-contents__link toc-highlight">Transfer via Feature Adaptation</a><li><a href=#other-transfer-methods class="table-of-contents__link toc-highlight">Other Transfer Methods</a></ul><li><a href=#vlm-knowledge-distillation class="table-of-contents__link toc-highlight">VLM Knowledge Distillation</a><ul><li><a href=#knowledge-distillation-for-object-detection class="table-of-contents__link toc-highlight">Knowledge Distillation for Object Detection</a><li><a href=#knowledge-distillation-for-semantic-segmentation class="table-of-contents__link toc-highlight">Knowledge Distillation for Semantic Segmentation</a></ul><li><a href=#performance class="table-of-contents__link toc-highlight">Performance</a><li><a href=#challenges class="table-of-contents__link toc-highlight">Challenges</a><li><a href=#ref class="table-of-contents__link toc-highlight">Ref</a></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Support Me</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://www.buymeacoffee.com/LOUB2kN target=_blank rel="noopener noreferrer" style="cursor: pointer;">
                <img src=https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png style="height: auto !important;width: auto !important;">
              </a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Feeds</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://gracefullight.dev/rss.xml target=_blank rel="noopener noreferrer" class=footer__link-item>RSS<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a><li class=footer__item><a href=https://gracefullight.dev/atom.xml target=_blank rel="noopener noreferrer" class=footer__link-item>Atom<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a></ul></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2016 - 2023 Gracefullight. Built with Docusaurus.</div></div></div></footer></div></body>