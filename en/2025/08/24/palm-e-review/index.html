<!doctype html><html lang=en dir=ltr class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated=false><head><meta charset=UTF-8><meta name=generator content="Docusaurus v3.9.2"><title data-rh=true>PaLM-E An Embodied Multimodal Language Model Review | gracefullight.dev</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"/><meta data-rh=true name=twitter:card content=summary_large_image /><meta data-rh=true property=og:url content=https://gracefullight.dev/en/2025/08/24/palm-e-review/ /><meta data-rh=true property=og:locale content=en /><meta data-rh=true property=og:locale:alternate content=ko /><meta data-rh=true name=docusaurus_locale content=en /><meta data-rh=true name=docusaurus_tag content=default /><meta data-rh=true name=docsearch:language content=en /><meta data-rh=true name=docsearch:docusaurus_tag content=default /><meta data-rh=true content=gcY9SiftHgQoJjBZ7IgwNNN5_atLPAX6kWb1nFVfa6E name=google-site-verification /><meta data-rh=true content=65AD1E28C0D057CEB3C68FBC0293E55B name=msvalidate.01 /><meta data-rh=true content=d024c2837887f72dc7b3792b958be74d69ba9593 name=naver-site-verification /><meta data-rh=true content=f7c93483a6f87c79 name=yandex-verification /><meta data-rh=true content=yZEdU1ABcR name=baidu-site-verification /><meta data-rh=true content=uelupjqqsm5egzlhy1aev2rfxow5yt name=facebook-domain-verification /><meta data-rh=true property=og:title content="PaLM-E An Embodied Multimodal Language Model Review | gracefullight.dev"/><meta data-rh=true name=description content="PaLM-E An Embodied Multimodal Language Model Review"/><meta data-rh=true property=og:description content="PaLM-E An Embodied Multimodal Language Model Review"/><meta data-rh=true property=og:type content=article /><meta data-rh=true property=article:published_time content=2025-08-24T10:33:27.131Z /><meta data-rh=true property=article:author content=https://github.com/gracefullight /><meta data-rh=true property=article:tag content=vlm /><link data-rh=true rel=icon href=/en/img/favicon.ico /><link data-rh=true rel=canonical href=https://gracefullight.dev/en/2025/08/24/palm-e-review/ /><link data-rh=true rel=alternate href=https://gracefullight.dev/2025/08/24/palm-e-review/ hreflang=ko /><link data-rh=true rel=alternate href=https://gracefullight.dev/en/2025/08/24/palm-e-review/ hreflang=en /><link data-rh=true rel=alternate href=https://gracefullight.dev/2025/08/24/palm-e-review/ hreflang=x-default /><link data-rh=true rel=preconnect href=https://RFS69RSYOJ-dsn.algolia.net crossorigin=anonymous /><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@id":"https://gracefullight.dev/en/2025/08/24/palm-e-review","@type":"BlogPosting","author":{"@type":"Person","description":"Owner","image":"https://avatars.githubusercontent.com/u/11773683?v=4","name":"Gracefullight","url":"https://github.com/gracefullight"},"datePublished":"2025-08-24T10:33:27.131Z","description":"PaLM-E An Embodied Multimodal Language Model Review","headline":"PaLM-E An Embodied Multimodal Language Model Review","isPartOf":{"@id":"https://gracefullight.dev/en/","@type":"Blog","name":"Blog"},"keywords":[],"mainEntityOfPage":"https://gracefullight.dev/en/2025/08/24/palm-e-review","name":"PaLM-E An Embodied Multimodal Language Model Review","url":"https://gracefullight.dev/en/2025/08/24/palm-e-review"}</script><link rel=alternate type=application/rss+xml href=/en/rss.xml title="gracefullight.dev RSS Feed"><link rel=alternate type=application/atom+xml href=/en/atom.xml title="gracefullight.dev Atom Feed"><link rel=alternate type=application/json href=/en/feed.json title="gracefullight.dev JSON Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-E99DNE7S05"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-E99DNE7S05",{})</script><link rel=search type=application/opensearchdescription+xml title=gracefullight.dev href=/en/opensearch.xml><link href=/en/img/favicon-32x32.png rel=icon><link href=/en/manifest.json rel=manifest><meta content=#f28913 name=theme-color><meta content=yes name=mobile-web-app-capable><meta content=#f28913 name=apple-mobile-web-app-status-bar-style><link href=/en/img/apple-touch-icon.png rel=apple-touch-icon><link rel=preconnect href=https://pagead2.googlesyndication.com><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3004788392777865" async crossorigin=anonymous></script><link rel=preconnect href=https://www.clarity.ms><script>!function(t,e,n,a,c,i,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(i=e.createElement(a)).async=1,i.src="https://www.clarity.ms/tag/"+c,(r=e.getElementsByTagName(a)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script","aongv9xgi6")</script><link rel=preconnect href=https://wcs.naver.net><script src=https://wcs.naver.net/wcslog.js async></script><script>if(!wcs_add)var wcs_add={};wcs_add.wa="156bc73a81e3bd0",window.wcs&&wcs_do()</script><link rel=preconnect href=https://cdn.channel.io><script>!function(){var n=window;if(n.ChannelIO)return(window.console.error||window.console.log||function(){})("ChannelIO script included twice.");var e=function(){e.c(arguments)};function t(){if(!n.ChannelIOInitialized){n.ChannelIOInitialized=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cdn.channel.io/plugin/ch-plugin-web.js",e.charset="UTF-8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}}e.q=[],e.c=function(n){e.q.push(n)},n.ChannelIO=e,"complete"===document.readyState?t():window.attachEvent?window.attachEvent("onload",t):(window.addEventListener("DOMContentLoaded",t,!1),window.addEventListener("load",t,!1))}(),ChannelIO("boot",{pluginKey:"0fd130ba-a1a6-4b7e-802a-e82a885a7fd8"})</script><link rel=preconnect href=https://static.cloudflareinsights.com><script src=https://static.cloudflareinsights.com/beacon.min.js defer data-cf-beacon='{"token":"c0899829e72b45e98dff77241127252c"}'></script><link href=https://mc.yandex.ru rel=preconnect><script>!function(e,t,c,n,r,a,s){e[r]=e[r]||function(){(e[r].a=e[r].a||[]).push(arguments)},e[r].l=+new Date;for(var i=0;i<document.scripts.length;i++)if(document.scripts[i].src===n)return;a=t.createElement(c),s=t.getElementsByTagName(c)[0],a.async=1,a.src=n,s.parentNode.insertBefore(a,s)}(window,document,"script","https://mc.yandex.ru/metrika/tag.js?id=104072655","ym"),ym(0x63405cf,"init",{ssr:!0,clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!1})</script><link href=https://cdn.jsdelivr.net rel=preconnect><script type=application/ld+json>{"@context":"http://schema.org","@type":"Person","email":"mailto:gracefullight.dev@gmail.com","image":"https://avatars.githubusercontent.com/u/11773683?v=4","jobTitle":"FullStack JavaScript Developer","logo":"https://gracefullight.dev/img/apple-touch-icon.png","name":"Eunkwang Shin","nationality":"Korean","sameAs":["https://github.com/gracefullight","https://linkedin.com/in/gracefullight"],"url":"https://gracefullight.dev"}</script><link rel=stylesheet crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css type=text/css><link rel=stylesheet href=/en/assets/css/styles.60923ef7.css /><script src=/en/assets/js/runtime~main.1b51a719.js defer></script><script src=/en/assets/js/main.bdbba33d.js defer></script></head><body class=navigation-with-keyboard><svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/favicon-32x32.png /><link rel=preload as=image href="https://avatars.githubusercontent.com/u/11773683?v=4"/><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_mlkZ themedComponent--light_NVdE"/><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"/></div><b class="navbar__title text--truncate">gracefullight.dev</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/2025/08/24/palm-e-review/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ko>한국어</a><li><a href=/en/2025/08/24/palm-e-review/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a></ul></div><a class="navbar__item navbar__link" href=/en/archive/>Archives</a><a class="navbar__item navbar__link" href=/en/tags/>Tags</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts=Meta+k><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 24 24" aria-hidden=true><circle cx=11 cy=11 r=8 stroke=currentColor fill=none stroke-width=1.4 /><path d="m21 21-4.3-4.3" stroke=currentColor fill=none stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class=row><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role=group><h3 class=yearGroupHeading_rMGB>2026</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/27/free-up-storage-space-on-mac/>Free up storage space on mac</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/27/promoting-an-opensource-project/>Promoting an opensource project</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/24/iqc-002/>IQC 002</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/23/tim-002/>TIM 002</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/17/innovation-tactics/>Innovation Tactics</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/01/31/agentic-sdlc/>Agentic SDLC</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/01/29/local-docker-env/>로컬 도커 환경 툴 비교</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-01/>Phrasal Verbs 01</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-014/>Phrasal Verbs 014</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-013/>Phrasal Verbs 013</a></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class=title_f1Hy>PaLM-E An Embodied Multimodal Language Model Review</h1><div class="container_mt6G margin-vert--md"><time datetime=2025-08-24T10:33:27.131Z>August 24, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div id=__blog-post-container class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=palm-e>PaLM-E<a href=#palm-e class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">ViT (e.g., ViT-4B, ViT-22B) extracts image embeddings.</li>
<li class="">OSRT builds object-centric slot representations.</li>
<li class="">These are injected into the LLM embedding space (PaLM variants: 8B, 62B, 540B) for high-level abstraction and planning, with execution delegated to low-level policies (e.g., RT-1).</li>
</ul>
<p><img decoding=async loading=lazy alt="PaLM-E Architecture" src=/en/assets/images/palm-e-architecture-ca3f220adc8e114bc89a0589703866fb.png width=1284 height=650 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=core-idea>Core idea<a href=#core-idea class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Build <strong>embodied language models</strong> by <strong>injecting continuous sensor inputs</strong> (images, states, other modalities) directly into a <strong>pretrained LLM’s embedding space</strong>, linking <strong>words ↔ percepts</strong>.</li>
<li class="">Inputs are <strong>multimodal sentences</strong> that <strong>interleave</strong> text tokens with encoded visual/state tokens; outputs are <strong>text</strong> (answers or high-level plans).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=architecture--representations>Architecture & representations<a href=#architecture--representations class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Start from a <strong>decoder-only, autoregressive LLM</strong> (PaLM) and <strong>condition on a prefix</strong> that mixes text and <strong>encoder-produced vectors</strong>.</li>
<li class="">Provide multiple encoder options:<!-- -->
<ul>
<li class=""><strong>State vectors</strong> (simplest).</li>
<li class=""><strong>ViT</strong> features with a learned <strong>projector ψ</strong> to match LLM embedding dimensionality.</li>
<li class=""><strong>Object-centric, 3D-aware OSRT</strong> (neural scene representations). Supports <strong>entity-label tokens</strong> (<code>&lt;obj j></code>) so the model can refer to specific objects in generated plans.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=training-setup>Training setup<a href=#training-setup class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Train <strong>end-to-end</strong> (encoders + projector + optionally the LLM) to output <strong>sequential decisions as natural text</strong> or answers (VQA, captioning).</li>
<li class="">Dataset items contain <strong>(continuous observations, text sequence, prefix index)</strong>; loss is <strong>cross-entropy on non-prefix tokens</strong>.</li>
<li class="">Explore <strong>freezing the LLM</strong> (train encoders/projection only), and <strong>co-training</strong> across diverse tasks ("full mixture"; only ~9% is embodied data).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=planning--control-loop>Planning & control loop<a href=#planning--control-loop class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">For <strong>planning/control</strong>, PaLM-E emits <strong>textual subgoals/skills</strong> drawn from a small skill vocabulary; a separate <strong>low-level policy</strong> executes them.</li>
<li class="">The system runs <strong>closed-loop</strong>: execute → observe → (re)plan; PaLM-E acts as a <strong>high-level policy</strong> sequencing low-level skills.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=why-not-text-only-llms-or-affordance-only-grounding>Why not text-only LLMs or affordance-only grounding?<a href=#why-not-text-only-llms-or-affordance-only-grounding class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Prior work that feeds <strong>only text to the LLM</strong> (and uses external affordance models) is <strong>insufficient</strong> when <strong>spatial layout</strong> matters.</li>
<li class="">PaLM-E instead <strong>grounds inside the LLM</strong> by <strong>injecting continuous observations</strong>, enabling <strong>direct plan generation</strong> while leveraging the LLM’s world knowledge.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=environments--use-cases>Environments & use cases<a href=#environments--use-cases class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Three domains</strong>: <strong>TAMP</strong> (grasp/stack planning), <strong>Language-Table</strong> (multi-object tabletop pushing), <strong>Mobile manipulation</strong> (kitchen tasks).</li>
<li class=""><strong>Use cases</strong> to test embodied reasoning: <strong>affordance prediction</strong>, <strong>failure detection</strong>, <strong>long-horizon planning</strong> (low-level policies from <strong>RT-1</strong>).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=results-high-level>Results (high level)<a href=#results-high-level class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Transfer via co-training</strong>: One model trained on mixed tasks/embodiments achieves <strong>higher performance</strong> than task-specialists; "full mixture" yields <code>>2×</code> gains (Fig. 3).</li>
<li class=""><strong>Few-shot/data efficiency</strong>: Solves robotics tasks with <strong>very few examples</strong> (e.g., <strong>10–80</strong> for Language-Table, <strong>320</strong> for TAMP). <strong>OSRT</strong> further improves data efficiency.</li>
<li class=""><strong>Mobile manipulation</strong>: End-to-end embodied planning works in real kitchens, robust to disturbances; PaLM-E beats <strong>PaLI (zero-shot)</strong> and <strong>QT-OPT/CLIP baselines</strong> on affordance/failure detection.</li>
<li class=""><strong>General V+L</strong>: The <strong>562B</strong> generalist achieves <strong>state-of-the-art on OK-VQA</strong> and strong VQAv2/COCO without task-specific finetuning.</li>
<li class=""><strong>Language retention & scaling</strong>: <strong>Freezing LLM</strong> preserves language ability but can struggle on some robotics tasks; <strong>unfrozen + scale up</strong> significantly reduces <strong>catastrophic forgetting</strong>.</li>
<li class=""><strong>Emergent behaviors</strong>: <strong>Multimodal chain-of-thought</strong> and <strong>multi-image reasoning</strong> emerge in <strong>PaLM-E-562B</strong>, despite training on <strong>single-image prompts</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=takeaways>Takeaways<a href=#takeaways class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Injecting <strong>neural scene representations (OSRT)</strong> and <strong>entity-labeled multimodal tokens</strong> is <strong>effective</strong> even without massive embodied data.</li>
<li class=""><strong>Diverse, joint training</strong> transfers vision-language knowledge <strong>into embodied decision-making</strong>, enabling <strong>data-efficient robot planning</strong>.</li>
<li class="">Two viable paths to retain language skills during multimodal finetuning:<!-- -->
<ol>
<li class=""><strong>Freeze the LLM</strong>, train encoders (max language retention, sometimes weaker robotics),</li>
<li class=""><strong>Unfreeze and scale</strong> the LLM (much less forgetting, strong embodied performance).</li>
</ol>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., & Florence, P. (2023). PaLM-E: An Embodied Multimodal Language Model Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v202/driess23a.html</code></li>
</ul></div><footer class=docusaurus-mt-lg><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/2025/08/24/rt-2-review/><div class=pagination-nav__sublabel>Newer Post</div><div class=pagination-nav__label>RT-2, Robotic Transformer 2 Review</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/2025/08/24/rt-1-review/><div class=pagination-nav__sublabel>Older Post</div><div class=pagination-nav__label>RT-1, Robot Transformer 1 Review</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#palm-e class="table-of-contents__link toc-highlight">PaLM-E</a><li><a href=#core-idea class="table-of-contents__link toc-highlight">Core idea</a><li><a href=#architecture--representations class="table-of-contents__link toc-highlight">Architecture & representations</a><li><a href=#training-setup class="table-of-contents__link toc-highlight">Training setup</a><li><a href=#planning--control-loop class="table-of-contents__link toc-highlight">Planning & control loop</a><li><a href=#why-not-text-only-llms-or-affordance-only-grounding class="table-of-contents__link toc-highlight">Why not text-only LLMs or affordance-only grounding?</a><li><a href=#environments--use-cases class="table-of-contents__link toc-highlight">Environments & use cases</a><li><a href=#results-high-level class="table-of-contents__link toc-highlight">Results (high level)</a><li><a href=#takeaways class="table-of-contents__link toc-highlight">Takeaways</a><li><a href=#ref class="table-of-contents__link toc-highlight">Ref</a></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Support Me</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://www.buymeacoffee.com/LOUB2kN target=_blank rel="noopener noreferrer" style="cursor: pointer;">
                <img src=https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png style="height: auto !important;width: auto !important;">
              </a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Feeds</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://gracefullight.dev/rss.xml target=_blank rel="noopener noreferrer" class=footer__link-item>RSS<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a><li class=footer__item><a href=https://gracefullight.dev/atom.xml target=_blank rel="noopener noreferrer" class=footer__link-item>Atom<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a></ul></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2016 - 2023 Gracefullight. Built with Docusaurus.</div></div></div></footer></div></body>