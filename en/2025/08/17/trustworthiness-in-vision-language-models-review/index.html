<!doctype html><html lang=en dir=ltr class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated=false><head><meta charset=UTF-8><meta name=generator content="Docusaurus v3.9.2"><title data-rh=true>Trustworthiness in Vision-Language Models Review | gracefullight.dev</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"/><meta data-rh=true name=twitter:card content=summary_large_image /><meta data-rh=true property=og:url content=https://gracefullight.dev/en/2025/08/17/trustworthiness-in-vision-language-models-review/ /><meta data-rh=true property=og:locale content=en /><meta data-rh=true property=og:locale:alternate content=ko /><meta data-rh=true name=docusaurus_locale content=en /><meta data-rh=true name=docusaurus_tag content=default /><meta data-rh=true name=docsearch:language content=en /><meta data-rh=true name=docsearch:docusaurus_tag content=default /><meta data-rh=true content=gcY9SiftHgQoJjBZ7IgwNNN5_atLPAX6kWb1nFVfa6E name=google-site-verification /><meta data-rh=true content=65AD1E28C0D057CEB3C68FBC0293E55B name=msvalidate.01 /><meta data-rh=true content=d024c2837887f72dc7b3792b958be74d69ba9593 name=naver-site-verification /><meta data-rh=true content=f7c93483a6f87c79 name=yandex-verification /><meta data-rh=true content=yZEdU1ABcR name=baidu-site-verification /><meta data-rh=true content=uelupjqqsm5egzlhy1aev2rfxow5yt name=facebook-domain-verification /><meta data-rh=true property=og:title content="Trustworthiness in Vision-Language Models Review | gracefullight.dev"/><meta data-rh=true name=description content="Trustworthiness in Vision-Language Models Review"/><meta data-rh=true property=og:description content="Trustworthiness in Vision-Language Models Review"/><meta data-rh=true property=og:type content=article /><meta data-rh=true property=article:published_time content=2025-08-17T00:02:41.350Z /><meta data-rh=true property=article:author content=https://github.com/gracefullight /><meta data-rh=true property=article:tag content=vlm /><link data-rh=true rel=icon href=/en/img/favicon.ico /><link data-rh=true rel=canonical href=https://gracefullight.dev/en/2025/08/17/trustworthiness-in-vision-language-models-review/ /><link data-rh=true rel=alternate href=https://gracefullight.dev/2025/08/17/trustworthiness-in-vision-language-models-review/ hreflang=ko /><link data-rh=true rel=alternate href=https://gracefullight.dev/en/2025/08/17/trustworthiness-in-vision-language-models-review/ hreflang=en /><link data-rh=true rel=alternate href=https://gracefullight.dev/2025/08/17/trustworthiness-in-vision-language-models-review/ hreflang=x-default /><link data-rh=true rel=preconnect href=https://RFS69RSYOJ-dsn.algolia.net crossorigin=anonymous /><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@id":"https://gracefullight.dev/en/2025/08/17/trustworthiness-in-vision-language-models-review","@type":"BlogPosting","author":{"@type":"Person","description":"Owner","image":"https://avatars.githubusercontent.com/u/11773683?v=4","name":"Gracefullight","url":"https://github.com/gracefullight"},"datePublished":"2025-08-17T00:02:41.350Z","description":"Trustworthiness in Vision-Language Models Review","headline":"Trustworthiness in Vision-Language Models Review","isPartOf":{"@id":"https://gracefullight.dev/en/","@type":"Blog","name":"Blog"},"keywords":[],"mainEntityOfPage":"https://gracefullight.dev/en/2025/08/17/trustworthiness-in-vision-language-models-review","name":"Trustworthiness in Vision-Language Models Review","url":"https://gracefullight.dev/en/2025/08/17/trustworthiness-in-vision-language-models-review"}</script><link rel=alternate type=application/rss+xml href=/en/rss.xml title="gracefullight.dev RSS Feed"><link rel=alternate type=application/atom+xml href=/en/atom.xml title="gracefullight.dev Atom Feed"><link rel=alternate type=application/json href=/en/feed.json title="gracefullight.dev JSON Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-E99DNE7S05"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-E99DNE7S05",{})</script><link rel=search type=application/opensearchdescription+xml title=gracefullight.dev href=/en/opensearch.xml><link href=/en/img/favicon-32x32.png rel=icon><link href=/en/manifest.json rel=manifest><meta content=#f28913 name=theme-color><meta content=yes name=mobile-web-app-capable><meta content=#f28913 name=apple-mobile-web-app-status-bar-style><link href=/en/img/apple-touch-icon.png rel=apple-touch-icon><link rel=preconnect href=https://pagead2.googlesyndication.com><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3004788392777865" async crossorigin=anonymous></script><link rel=preconnect href=https://www.clarity.ms><script>!function(t,e,n,a,c,i,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(i=e.createElement(a)).async=1,i.src="https://www.clarity.ms/tag/"+c,(r=e.getElementsByTagName(a)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script","aongv9xgi6")</script><link rel=preconnect href=https://wcs.naver.net><script src=https://wcs.naver.net/wcslog.js async></script><script>if(!wcs_add)var wcs_add={};wcs_add.wa="156bc73a81e3bd0",window.wcs&&wcs_do()</script><link rel=preconnect href=https://cdn.channel.io><script>!function(){var n=window;if(n.ChannelIO)return(window.console.error||window.console.log||function(){})("ChannelIO script included twice.");var e=function(){e.c(arguments)};function t(){if(!n.ChannelIOInitialized){n.ChannelIOInitialized=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cdn.channel.io/plugin/ch-plugin-web.js",e.charset="UTF-8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}}e.q=[],e.c=function(n){e.q.push(n)},n.ChannelIO=e,"complete"===document.readyState?t():window.attachEvent?window.attachEvent("onload",t):(window.addEventListener("DOMContentLoaded",t,!1),window.addEventListener("load",t,!1))}(),ChannelIO("boot",{pluginKey:"0fd130ba-a1a6-4b7e-802a-e82a885a7fd8"})</script><link rel=preconnect href=https://static.cloudflareinsights.com><script src=https://static.cloudflareinsights.com/beacon.min.js defer data-cf-beacon='{"token":"c0899829e72b45e98dff77241127252c"}'></script><link href=https://mc.yandex.ru rel=preconnect><script>!function(e,t,c,n,r,a,s){e[r]=e[r]||function(){(e[r].a=e[r].a||[]).push(arguments)},e[r].l=+new Date;for(var i=0;i<document.scripts.length;i++)if(document.scripts[i].src===n)return;a=t.createElement(c),s=t.getElementsByTagName(c)[0],a.async=1,a.src=n,s.parentNode.insertBefore(a,s)}(window,document,"script","https://mc.yandex.ru/metrika/tag.js?id=104072655","ym"),ym(0x63405cf,"init",{ssr:!0,clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!1})</script><link href=https://cdn.jsdelivr.net rel=preconnect><script type=application/ld+json>{"@context":"http://schema.org","@type":"Person","email":"mailto:gracefullight.dev@gmail.com","image":"https://avatars.githubusercontent.com/u/11773683?v=4","jobTitle":"FullStack JavaScript Developer","logo":"https://gracefullight.dev/img/apple-touch-icon.png","name":"Eunkwang Shin","nationality":"Korean","sameAs":["https://github.com/gracefullight","https://linkedin.com/in/gracefullight"],"url":"https://gracefullight.dev"}</script><link rel=stylesheet crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css type=text/css><link rel=stylesheet href=/en/assets/css/styles.60923ef7.css /><script src=/en/assets/js/runtime~main.1b51a719.js defer></script><script src=/en/assets/js/main.bdbba33d.js defer></script></head><body class=navigation-with-keyboard><svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/favicon-32x32.png /><link rel=preload as=image href="https://avatars.githubusercontent.com/u/11773683?v=4"/><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_mlkZ themedComponent--light_NVdE"/><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"/></div><b class="navbar__title text--truncate">gracefullight.dev</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/2025/08/17/trustworthiness-in-vision-language-models-review/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ko>한국어</a><li><a href=/en/2025/08/17/trustworthiness-in-vision-language-models-review/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a></ul></div><a class="navbar__item navbar__link" href=/en/archive/>Archives</a><a class="navbar__item navbar__link" href=/en/tags/>Tags</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts=Meta+k><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 24 24" aria-hidden=true><circle cx=11 cy=11 r=8 stroke=currentColor fill=none stroke-width=1.4 /><path d="m21 21-4.3-4.3" stroke=currentColor fill=none stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class=row><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role=group><h3 class=yearGroupHeading_rMGB>2026</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/27/free-up-storage-space-on-mac/>Free up storage space on mac</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/27/promoting-an-opensource-project/>Promoting an opensource project</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/24/iqc-002/>IQC 002</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/23/tim-002/>TIM 002</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/17/innovation-tactics/>Innovation Tactics</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/01/31/agentic-sdlc/>Agentic SDLC</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/01/29/local-docker-env/>로컬 도커 환경 툴 비교</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-01/>Phrasal Verbs 01</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-014/>Phrasal Verbs 014</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-013/>Phrasal Verbs 013</a></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class=title_f1Hy>Trustworthiness in Vision-Language Models Review</h1><div class="container_mt6G margin-vert--md"><time datetime=2025-08-17T00:02:41.350Z>August 17, 2025</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div id=__blog-post-container class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=overview>Overview<a href=#overview class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Mitigates exposure of private data, produces harmful outputs, or is vulnerable to attacks.</li>
<li class="">SOTA models: LLaVA, Flamingo, GPT-4</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=privacy>Privacy<a href=#privacy class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=privacy-issues>Privacy Issues<a href=#privacy-issues class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">risk escalates significantly with relevant images as optimizing in the pixel domain is easier than in text</li>
<li class="">can unintentionally memorize sensitive data, leading to leaks without knowledge of the model’s specifics</li>
<li class="">Overfitting may also cause retention of sensitive attributes during inference</li>
<li class="">gradient-based and backdoor attacks further jeopardize VLM privacy with open-source data</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=privacy-mitigation-methods>Privacy Mitigation Methods<a href=#privacy-mitigation-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">New metrics have been created to assess a model’s ability to reproduce training instances and facilitate cross-model comparisons</li>
<li class="">models utilizing multiple modalities provide better privacy</li>
<li class="">safety modules can be integrated to boost resilience against violations</li>
<li class="">adversarial training can enhance privacy but risks reducing accuracy</li>
<li class="">New architecture: differentially private CLIP model</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=privacy-future-research-directions>Privacy Future Research Directions<a href=#privacy-future-research-directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Cryptography-based Privacy Preservation<!-- -->
<ul>
<li class="">Secure multi-party computation (SMPC): divides secret information into shares among multiple parties, ensuring that individual shares reveal nothing unless combined</li>
<li class="">Homomorphic encryption (HE): allows computations on encrypted data without decryption, and has also been utilized for privacy preservation in transformers</li>
</ul>
</li>
<li class="">Federated Learning<!-- -->
<ul>
<li class="">enhances privacy in vision-language models (VLMs) by localizing model training, which protects training data from leakage.</li>
<li class="">challenges such as communication overhead among devices and statistical heterogeneity from diverse data distributions</li>
</ul>
</li>
<li class="">Data Manipulation and Finetunning<!-- -->
<ul>
<li class="">Data pseudonymization: substitutes sensitive information with synthetic alternatives.</li>
<li class="">Data Sanitization: removes duplicates to reduce memorization and privacy risks.</li>
<li class="">knowledge sanitization-fine-tuning: provide safe responses when leakage risks arise.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=fairness-and-bias>Fairness and Bias<a href=#fairness-and-bias class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=fairness-and-bias-issues>Fairness and Bias Issues<a href=#fairness-and-bias-issues class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Bias from training data<!-- -->
<ul>
<li class="">disproportionately features men and lighter-skinned individuals</li>
<li class="">outdated vocabulary and imbalanced representation</li>
<li class="">clinical models may favor certain patient groups based on gender, language, etc.</li>
</ul>
</li>
<li class="">Bias from Model<!-- -->
<ul>
<li class="">Gender biases</li>
<li class="">misclassification of race-related elements and biased outputs</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=fairness-and-bias-mitigation-methods>Fairness and Bias Mitigation Methods<a href=#fairness-and-bias-mitigation-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">New Datasets and Benchmarks<!-- -->
<ul>
<li class="">Harvard-FairVLMed, PATA, and BOLD enhance evaluations but often lack the scale of established benchmarks.</li>
<li class="">create synthetic datasets to improve fairness assessments<!-- -->
<ul>
<li class="">gender-balanced dataset generated with DALL-E-3 and another consisting of gender-swapped images</li>
<li class="">counterfactual image-text pairs that highlight biases in datasets like COCO Captions</li>
</ul>
</li>
<li class="">new metrics<!-- -->
<ul>
<li class="">gender polarity</li>
<li class="">bias distance in embeddings</li>
</ul>
</li>
<li class="">human evaluation</li>
</ul>
</li>
<li class="">De-biasing<!-- -->
<ul>
<li class="">adjust model instructions and architectures for improved fairness</li>
<li class="">detecting biased prompts in pre-trained models</li>
<li class="">Post-hoc Bias Mitigation (PBM) effectively reduce bias in image retrieval</li>
<li class="">Re-sampling underperforming clusters can enhance fairness</li>
<li class="">modification of facial features also mitigate biases</li>
<li class="">self-debiasing reduces biased text generation, especially when paired with other methods</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=fairness-future-research-directions>Fairness Future Research Directions<a href=#fairness-future-research-directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Optimized De-biasing<!-- -->
<ul>
<li class="">Additive residual learning: for fairer image representations.</li>
<li class="">Calibration loss: retain semantically similar embeddings.</li>
<li class="">Counterfactual inference framework: help models learn correct responses through cause and effect.</li>
<li class="">Adversarial classifiers: predict image attributes from visual-textual similarities can be combined with instruction tuning to reduce bias.</li>
</ul>
</li>
<li class="">Disentangled Representation Learning (DRL): simplifies complex data by breaking it in to independent feature groups, improving model predictions.<!-- -->
<ul>
<li class="">Traditional DRL<!-- -->
<ul>
<li class="">Variational autoencoders (VAEs) for feature encoding based on impact</li>
<li class="">Generative adversarial networks (GANs) for separation.</li>
</ul>
</li>
<li class="">Attention in text encoders can be adjusted for fairer outputs.</li>
<li class="">challenges: varying definitions of "disentanglement", ensuring fairness.</li>
</ul>
</li>
<li class="">Human-in-the-Loop (HITL): integrating human intervention into their training to improve precision and fairness<!-- -->
<ul>
<li class="">active learning</li>
<li class="">reinforcement learning with human feedback</li>
<li class="">explainable AI</li>
<li class="">challenges: human bias, finance, and ethical and legal issues persist</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=robustness>Robustness<a href=#robustness class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=robustness-issues>Robustness Issues<a href=#robustness-issues class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Out-of-Distribution (OOD) Robustness<!-- -->
<ul>
<li class="">ChatGPT excels in adversarial tasks but struggles with OOD robustness and informal medical responses</li>
<li class="">MLLMs often fail to generalize beyond training domains due to mapping issues</li>
<li class="">vision-language models face difficulties with open-domain concepts, especially when overfitting during fine-tuning</li>
<li class="">Large pre-trained image classifiers show initial robustness, which diminishes over time</li>
<li class="">Current visual question answering (VQA) models are limited to specific benchmarks, hindering generalization to OOD datasets</li>
<li class="">fine-tuning may impair model calibration in OOD contexts.</li>
</ul>
</li>
<li class="">Adversarial Attack Robustness<!-- -->
<ul>
<li class="">Studies indicate that open-sourced VLMs show performance gaps in red teaming tasks, highlighting the need for improved safety and security.</li>
<li class="">misalignment between language and vision modalities creates a "modality gap", complicating adversarial vulnerability.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=robustness-mitigation-methods>Robustness Mitigation Methods<a href=#robustness-mitigation-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Improving Out-of-Distribution Robustness<!-- -->
<ul>
<li class="">enhance OOD detection and generalization. A simple maximum logit detector has been shown to outperform complex methods for anomaly segmentation</li>
<li class="">In-context learning (ICL) can also improve multimodal generalization</li>
<li class="">A fine-tuned CLIP excels in unsupervised OOD detection</li>
<li class="">The OGEN method synthesizes OOD features</li>
<li class="">Maximum Concept Matching aligns visual and textual features, and anchor-based finetuning leads to better domain shifts</li>
</ul>
</li>
<li class="">Defense Against Adversarial Attacks<!-- -->
<ul>
<li class="">VILLA is a two-stage framework for adversarial training of VLMs, featuring task-agnostic <strong>adversarial pre-training</strong> and <strong>task-specific finetuning</strong>
<ul>
<li class="">conducts adversarial training in the embedding space rather than on raw image pixels and text tokens, improving the model’s resilience against adversarial examples</li>
<li class="">SOTA performance across various tasks</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=robustness-future-research-directions>Robustness Future Research Directions<a href=#robustness-future-research-directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Data Augmentation<!-- -->
<ul>
<li class="">MixGen: a data augmentation method that generates new image-text pairs by interpolating images and concatenating text to preserve semantics.</li>
<li class="">creating synthetic images involves extracting text prompts via an image captioning model for use in text-to-image diffusion, then mixing these with real datasets.</li>
<li class="">bimodal augmentation (BiAug): decouples objects and attributes to synthesize vision-language examples and hard negatives, using LLMs and an object detector to generate detailed descriptions and inpaint corresponding images.</li>
</ul>
</li>
<li class="">Improved Cross-Modal Alignment<!-- -->
<ul>
<li class="">Sharing learnable parameters</li>
<li class="">Applying bidirectional constraints</li>
<li class="">Adjusting cross-modal projections</li>
</ul>
</li>
<li class="">challenges: addressing the modality gap, which impacts robustness to OOD data and adversarial examples</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=safety>Safety<a href=#safety class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=safety-issues>Safety Issues<a href=#safety-issues class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Toxicity<!-- -->
<ul>
<li class="">LAION-400M: contains problematic content, including explicit materials and harmful stereotypes</li>
<li class="">Advanced models like GeminiProVision and GPT-4V show inherent biases</li>
<li class="">Assigning personas to ChatGPT can increase toxicity and reinforce harmful stereotypes</li>
</ul>
</li>
<li class="">Jailbreaking Risk<!-- -->
<ul>
<li class="">Perturbation can be performed effectively, while FigStep converts harmful content into images with an 82.5% attack rate across multiple VLMs</li>
<li class="">replaces captions with malicious prompts, enabling jailbreaks.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=safety-mitigation-methods>Safety Mitigation Methods<a href=#safety-mitigation-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class=""><strong>Safety Fine-Tuning</strong>
<ul>
<li class="">VLGuard</li>
<li class="">fine-tuned on synthetic data, reducing sensitivity to NSFW inputs and enhancing performance in cross-modal tasks</li>
</ul>
</li>
<li class="">Other approach<!-- -->
<ul>
<li class="">Reinforce-Detoxify: uses reinforcement learning to mitigate toxicity and bias in transformer models</li>
<li class="">simple mitigations improve automatic scores, these methods risk over-filtering marginalized texts and create discrepancies between automatic and human judgments</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=safety-future-research-directions>Safety Future Research Directions<a href=#safety-future-research-directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Context Awareness<!-- -->
<ul>
<li class="">integrating Chain-of-Thought for improved reasoning can enhance CAER tasks with Large VLMs.</li>
<li class="">Dual-Aligned Prompt Tuning: combines explicit context from pre-trained LLMs with implicit modeling to create more context-aware prompts</li>
<li class="">Visual In-Context Learning: optimizes image retrieval and summarization to enhance task-specific interactions.</li>
</ul>
</li>
<li class="">Automated Red Teaming (ART)<!-- -->
<ul>
<li class="">RTVLM: a dataset that benchmarks VLMs across faithfulness, privacy, safety, and fairness</li>
<li class="">Arondight: automates multi-modal jailbreak attacks using reinforcement learning and uncovers significant security vulnerabilities</li>
<li class="">GPT-4 and GPT-4V are more robust against jailbreaks than open-source models</li>
<li class="">limited transferability of visual jailbreak methods compared to textual ones</li>
<li class="">connects unsafe outputs to prompts, improving the detection of vulnerabilities in text-to-image models</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Vu, K., & Lai, P. (2025). Trustworthiness in Vision-Language Models. In J. Kertesz, B. Li, T. Supnithi, & A. Takhom, Computational Data and Social Networks Singapore.</li>
</ul></div><footer class=docusaurus-mt-lg><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/2025/08/17/zotero-initial-setup/><div class=pagination-nav__sublabel>Newer Post</div><div class=pagination-nav__label>Zotero 초기 세팅</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/2025/08/16/vision-language-models-for-vision-tasks-review/><div class=pagination-nav__sublabel>Older Post</div><div class=pagination-nav__label>Vision-Language Models for Vision Tasks Review</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#overview class="table-of-contents__link toc-highlight">Overview</a><li><a href=#privacy class="table-of-contents__link toc-highlight">Privacy</a><ul><li><a href=#privacy-issues class="table-of-contents__link toc-highlight">Privacy Issues</a><li><a href=#privacy-mitigation-methods class="table-of-contents__link toc-highlight">Privacy Mitigation Methods</a><li><a href=#privacy-future-research-directions class="table-of-contents__link toc-highlight">Privacy Future Research Directions</a></ul><li><a href=#fairness-and-bias class="table-of-contents__link toc-highlight">Fairness and Bias</a><ul><li><a href=#fairness-and-bias-issues class="table-of-contents__link toc-highlight">Fairness and Bias Issues</a><li><a href=#fairness-and-bias-mitigation-methods class="table-of-contents__link toc-highlight">Fairness and Bias Mitigation Methods</a><li><a href=#fairness-future-research-directions class="table-of-contents__link toc-highlight">Fairness Future Research Directions</a></ul><li><a href=#robustness class="table-of-contents__link toc-highlight">Robustness</a><ul><li><a href=#robustness-issues class="table-of-contents__link toc-highlight">Robustness Issues</a><li><a href=#robustness-mitigation-methods class="table-of-contents__link toc-highlight">Robustness Mitigation Methods</a><li><a href=#robustness-future-research-directions class="table-of-contents__link toc-highlight">Robustness Future Research Directions</a></ul><li><a href=#safety class="table-of-contents__link toc-highlight">Safety</a><ul><li><a href=#safety-issues class="table-of-contents__link toc-highlight">Safety Issues</a><li><a href=#safety-mitigation-methods class="table-of-contents__link toc-highlight">Safety Mitigation Methods</a><li><a href=#safety-future-research-directions class="table-of-contents__link toc-highlight">Safety Future Research Directions</a></ul><li><a href=#ref class="table-of-contents__link toc-highlight">Ref</a></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Support Me</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://www.buymeacoffee.com/LOUB2kN target=_blank rel="noopener noreferrer" style="cursor: pointer;">
                <img src=https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png style="height: auto !important;width: auto !important;">
              </a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Feeds</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://gracefullight.dev/rss.xml target=_blank rel="noopener noreferrer" class=footer__link-item>RSS<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a><li class=footer__item><a href=https://gracefullight.dev/atom.xml target=_blank rel="noopener noreferrer" class=footer__link-item>Atom<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a></ul></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2016 - 2023 Gracefullight. Built with Docusaurus.</div></div></div></footer></div></body>