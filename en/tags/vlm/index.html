<!doctype html><html lang=en dir=ltr class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated=false><head><meta charset=UTF-8><meta name=generator content="Docusaurus v3.9.2"><title data-rh=true>16 posts tagged with "vlm" | gracefullight.dev</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"/><meta data-rh=true name=twitter:card content=summary_large_image /><meta data-rh=true property=og:url content=https://gracefullight.dev/en/tags/vlm/ /><meta data-rh=true property=og:locale content=en /><meta data-rh=true property=og:locale:alternate content=ko /><meta data-rh=true name=docusaurus_locale content=en /><meta data-rh=true name=docsearch:language content=en /><meta data-rh=true content=gcY9SiftHgQoJjBZ7IgwNNN5_atLPAX6kWb1nFVfa6E name=google-site-verification /><meta data-rh=true content=65AD1E28C0D057CEB3C68FBC0293E55B name=msvalidate.01 /><meta data-rh=true content=d024c2837887f72dc7b3792b958be74d69ba9593 name=naver-site-verification /><meta data-rh=true content=f7c93483a6f87c79 name=yandex-verification /><meta data-rh=true content=yZEdU1ABcR name=baidu-site-verification /><meta data-rh=true content=uelupjqqsm5egzlhy1aev2rfxow5yt name=facebook-domain-verification /><meta data-rh=true property=og:title content='16 posts tagged with "vlm" | gracefullight.dev'/><meta data-rh=true name=description content="Vision-Language Models"/><meta data-rh=true property=og:description content="Vision-Language Models"/><meta data-rh=true name=docusaurus_tag content=blog_tags_posts /><meta data-rh=true name=docsearch:docusaurus_tag content=blog_tags_posts /><link data-rh=true rel=icon href=/en/img/favicon.ico /><link data-rh=true rel=canonical href=https://gracefullight.dev/en/tags/vlm/ /><link data-rh=true rel=alternate href=https://gracefullight.dev/tags/vlm/ hreflang=ko /><link data-rh=true rel=alternate href=https://gracefullight.dev/en/tags/vlm/ hreflang=en /><link data-rh=true rel=alternate href=https://gracefullight.dev/tags/vlm/ hreflang=x-default /><link data-rh=true rel=preconnect href=https://RFS69RSYOJ-dsn.algolia.net crossorigin=anonymous /><link rel=alternate type=application/rss+xml href=/en/rss.xml title="gracefullight.dev RSS Feed"><link rel=alternate type=application/atom+xml href=/en/atom.xml title="gracefullight.dev Atom Feed"><link rel=alternate type=application/json href=/en/feed.json title="gracefullight.dev JSON Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-E99DNE7S05"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-E99DNE7S05",{})</script><link rel=search type=application/opensearchdescription+xml title=gracefullight.dev href=/en/opensearch.xml><link href=/en/img/favicon-32x32.png rel=icon><link href=/en/manifest.json rel=manifest><meta content=#f28913 name=theme-color><meta content=yes name=mobile-web-app-capable><meta content=#f28913 name=apple-mobile-web-app-status-bar-style><link href=/en/img/apple-touch-icon.png rel=apple-touch-icon><link rel=preconnect href=https://pagead2.googlesyndication.com><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3004788392777865" async crossorigin=anonymous></script><link rel=preconnect href=https://www.clarity.ms><script>!function(t,e,n,a,c,i,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(i=e.createElement(a)).async=1,i.src="https://www.clarity.ms/tag/"+c,(r=e.getElementsByTagName(a)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script","aongv9xgi6")</script><link rel=preconnect href=https://wcs.naver.net><script src=https://wcs.naver.net/wcslog.js async></script><script>if(!wcs_add)var wcs_add={};wcs_add.wa="156bc73a81e3bd0",window.wcs&&wcs_do()</script><link rel=preconnect href=https://cdn.channel.io><script>!function(){var n=window;if(n.ChannelIO)return(window.console.error||window.console.log||function(){})("ChannelIO script included twice.");var e=function(){e.c(arguments)};function t(){if(!n.ChannelIOInitialized){n.ChannelIOInitialized=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cdn.channel.io/plugin/ch-plugin-web.js",e.charset="UTF-8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}}e.q=[],e.c=function(n){e.q.push(n)},n.ChannelIO=e,"complete"===document.readyState?t():window.attachEvent?window.attachEvent("onload",t):(window.addEventListener("DOMContentLoaded",t,!1),window.addEventListener("load",t,!1))}(),ChannelIO("boot",{pluginKey:"0fd130ba-a1a6-4b7e-802a-e82a885a7fd8"})</script><link rel=preconnect href=https://static.cloudflareinsights.com><script src=https://static.cloudflareinsights.com/beacon.min.js defer data-cf-beacon='{"token":"c0899829e72b45e98dff77241127252c"}'></script><link href=https://mc.yandex.ru rel=preconnect><script>!function(e,t,c,n,r,a,s){e[r]=e[r]||function(){(e[r].a=e[r].a||[]).push(arguments)},e[r].l=+new Date;for(var i=0;i<document.scripts.length;i++)if(document.scripts[i].src===n)return;a=t.createElement(c),s=t.getElementsByTagName(c)[0],a.async=1,a.src=n,s.parentNode.insertBefore(a,s)}(window,document,"script","https://mc.yandex.ru/metrika/tag.js?id=104072655","ym"),ym(0x63405cf,"init",{ssr:!0,clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!1})</script><link href=https://cdn.jsdelivr.net rel=preconnect><script type=application/ld+json>{"@context":"http://schema.org","@type":"Person","email":"mailto:gracefullight.dev@gmail.com","image":"https://avatars.githubusercontent.com/u/11773683?v=4","jobTitle":"FullStack JavaScript Developer","logo":"https://gracefullight.dev/img/apple-touch-icon.png","name":"Eunkwang Shin","nationality":"Korean","sameAs":["https://github.com/gracefullight","https://linkedin.com/in/gracefullight"],"url":"https://gracefullight.dev"}</script><link rel=stylesheet crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css type=text/css><link rel=stylesheet href=/en/assets/css/styles.60923ef7.css /><script src=/en/assets/js/runtime~main.1b51a719.js defer></script><script src=/en/assets/js/main.bdbba33d.js defer></script></head><body class=navigation-with-keyboard><svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/favicon-32x32.png /><link rel=preload as=image href="https://avatars.githubusercontent.com/u/11773683?v=4"/><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_mlkZ themedComponent--light_NVdE"/><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"/></div><b class="navbar__title text--truncate">gracefullight.dev</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/tags/vlm/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ko>한국어</a><li><a href=/en/tags/vlm/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a></ul></div><a class="navbar__item navbar__link" href=/en/archive/>Archives</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/tags/>Tags</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts=Meta+k><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 24 24" aria-hidden=true><circle cx=11 cy=11 r=8 stroke=currentColor fill=none stroke-width=1.4 /><path d="m21 21-4.3-4.3" stroke=currentColor fill=none stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class=row><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role=group><h3 class=yearGroupHeading_rMGB>2026</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/27/free-up-storage-space-on-mac/>Free up storage space on mac</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/27/promoting-an-opensource-project/>Promoting an opensource project</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/24/iqc-002/>IQC 002</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/23/tim-002/>TIM 002</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/02/17/innovation-tactics/>Innovation Tactics</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/01/31/agentic-sdlc/>Agentic SDLC</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/2026/01/29/local-docker-env/>로컬 도커 환경 툴 비교</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-01/>Phrasal Verbs 01</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-014/>Phrasal Verbs 014</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/vocab/phrasal-verbs-013/>Phrasal Verbs 013</a></ul></div></nav></aside><main class="col col--7"><header class=margin-bottom--xl><h1>16 posts tagged with "vlm"</h1><p>Vision-Language Models</p><a href=/en/tags/>View All Tags</a></header><article class=margin-bottom--xl><header><h2 class=title_f1Hy><a href=/en/2025/12/10/pi-oh-five-review/>π0.5 Review</a></h2><div class="container_mt6G margin-vert--md"><time datetime=2025-12-10T10:43:46.884Z>December 10, 2025</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=1-abstract>1. Abstract<a href=#1-abstract class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Core Concept:</strong> <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>π</mi><mn>0.5</mn></msub></mrow><annotation encoding=application/x-tex>\pi_{0.5}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0.5</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> is a model designed for broad generalization by utilizing co-training on heterogeneous tasks.</li>
<li class=""><strong>Method:</strong> It combines hybrid multi-modal examples including image observations, language commands, object detection, semantic subtask prediction, and low-level actions.</li>
<li class=""><strong>Impact:</strong> This knowledge transfer is essential for effective generalization, enabling the execution of long-horizon and dexterous manipulation skills in the wild.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=2-introduction>2. Introduction<a href=#2-introduction class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Goal:</strong> Design training recipes that provide the breadth of knowledge required for robots to generalize at multiple levels of abstraction, from physical behaviors to scene semantics.</li>
<li class=""><strong>Unified Framework:</strong> By casting different modalities into a single sequence modeling framework, VLAs can be trained on diverse sources: robot data, language data, computer vision tasks, and combinations thereof.</li>
<li class=""><strong>Capabilities:</strong> The model can control mobile manipulators to perform varied household tasks even in homes never seen during training.</li>
<li class=""><strong>Hierarchical Architecture:</strong>
<ul>
<li class=""><strong>Training:</strong> Pre-trains on a heterogeneous mixture of tasks, then fine-tunes specifically for mobile manipulation using both low-level action examples and high-level semantic actions (e.g., predicting "pick up the cutting board").</li>
<li class=""><strong>Inference:</strong> At runtime, the model first predicts a <strong>semantic subtask</strong> (inferring appropriate next behavior based on scene semantics) and then predicts the <strong>robot action chunk</strong> based on this subtask.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=3-model-structure>3. Model Structure<a href=#3-model-structure class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<p><img decoding=async loading=lazy alt="Pi 0.5 model architecture" src=/en/assets/images/pi-oh-five-model-5ddee93c84a12569f557680694527318.png width=1323 height=330 class=img_ev3q /></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=unified-transformer-architecture>Unified Transformer Architecture<a href=#unified-transformer-architecture class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">The model corresponds to a transformer taking in <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>N</mi></mrow><annotation encoding=application/x-tex>N</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.10903em>N</span></span></span></span> multimodal input tokens <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>N</mi></mrow></msub></mrow><annotation encoding=application/x-tex>x_{1:N}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight" style=margin-right:0.10903em>N</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> (images, text, and actions) and producing multimodal outputs.</li>
<li class=""><strong>Input Processing:</strong> Different token types are processed by specific encoders (e.g., Vision Encoder for images, Embedding Matrix for text).</li>
<li class=""><strong>Output Split:</strong> The output is split into two streams:<!-- -->
<ul>
<li class=""><strong>Text Logits (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msubsup><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mi>M</mi></mrow><mi>l</mi></msubsup></mrow><annotation encoding=application/x-tex>y^{l}_{1:M}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.1244em;vertical-align:-0.2753em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8491em><span style=top:-2.4247em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight" style=margin-right:0.10903em>M</span></span></span></span><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.01968em>l</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2753em><span></span></span></span></span></span></span></span></span></span>):</strong> Used for QA, reasoning, and <strong>dividing the task</strong> (predicting subtasks <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mover accent=true><mi>l</mi><mo>^</mo></mover></mrow><annotation encoding=application/x-tex>\hat{l}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9579em></span><span class="mord accent"><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.9579em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span></span><span style=top:-3.2634em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1667em><span class=mord>^</span></span></span></span></span></span></span></span></span></span>).</li>
<li class=""><strong>Action Tokens (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msubsup><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mi>H</mi></mrow><mi>a</mi></msubsup></mrow><annotation encoding=application/x-tex>y^{a}_{1:H}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9397em;vertical-align:-0.2753em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6644em><span style=top:-2.4247em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight" style=margin-right:0.08125em>H</span></span></span></span><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2753em><span></span></span></span></span></span></span></span></span></span>):</strong> Produced by a separate <strong>Action Expert</strong> to create continuous outputs for robot control.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=probabilistic-decomposition>Probabilistic Decomposition<a href=#probabilistic-decomposition class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<p>The distribution captured by the model is decomposed using the chain rule and a conditional independence assumption:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=false>(</mo><msub><mi>a</mi><mrow><mi>t</mi><mo>:</mo><mi>t</mi><mo>+</mo><mi>H</mi></mrow></msub><mo separator=true>,</mo><mover accent=true><mi>l</mi><mo>^</mo></mover><mi mathvariant=normal>∣</mi><msub><mi>o</mi><mi>t</mi></msub><mo separator=true>,</mo><mi>l</mi><mo stretchy=false>)</mo><mo>=</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=false>(</mo><msub><mi>a</mi><mrow><mi>t</mi><mo>:</mo><mi>t</mi><mo>+</mo><mi>H</mi></mrow></msub><mi mathvariant=normal>∣</mi><msub><mi>o</mi><mi>t</mi></msub><mo separator=true>,</mo><mover accent=true><mi>l</mi><mo>^</mo></mover><mo stretchy=false>)</mo><mo>⋅</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=false>(</mo><mover accent=true><mi>l</mi><mo>^</mo></mover><mi mathvariant=normal>∣</mi><msub><mi>o</mi><mi>t</mi></msub><mo separator=true>,</mo><mi>l</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>\pi_{\theta}(a_{t:t+H}, \hat{l} | o_{t}, l) = \pi_{\theta}(a_{t:t+H} | o_{t}, \hat{l}) \cdot \pi_{\theta}(\hat{l} | o_{t}, l)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.2079em;vertical-align:-0.25em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">a</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style=margin-right:0.08125em>H</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2083em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord accent"><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.9579em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span></span><span style=top:-3.2634em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1667em><span class=mord>^</span></span></span></span></span></span></span><span class=mord>∣</span><span class=mord><span class="mord mathnormal">o</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1.2079em;vertical-align:-0.25em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">a</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style=margin-right:0.08125em>H</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2083em><span></span></span></span></span></span></span><span class=mord>∣</span><span class=mord><span class="mord mathnormal">o</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord accent"><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.9579em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span></span><span style=top:-3.2634em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1667em><span class=mord>^</span></span></span></span></span></span></span><span class=mclose>)</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1.2079em;vertical-align:-0.25em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord accent"><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.9579em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span></span><span style=top:-3.2634em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1667em><span class=mord>^</span></span></span></span></span></span></span><span class=mord>∣</span><span class=mord><span class="mord mathnormal">o</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span><span class=mclose>)</span></span></span></span></span>
<ul>
<li class=""><strong>Assumption:</strong> The action distribution (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>a</mi><mrow><mi>t</mi><mo>:</mo><mi>t</mi><mo>+</mo><mi>H</mi></mrow></msub></mrow><annotation encoding=application/x-tex>a_{t:t+H}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6389em;vertical-align:-0.2083em></span><span class=mord><span class="mord mathnormal">a</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style=margin-right:0.08125em>H</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2083em><span></span></span></span></span></span></span></span></span></span>) does not depend on the overall task prompt (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>l</mi></mrow><annotation encoding=application/x-tex>l</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span></span></span></span>), but only on the predicted subtask (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mover accent=true><mi>l</mi><mo>^</mo></mover></mrow><annotation encoding=application/x-tex>\hat{l}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9579em></span><span class="mord accent"><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.9579em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span></span><span style=top:-3.2634em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1667em><span class=mord>^</span></span></span></span></span></span></span></span></span></span>).</li>
<li class=""><strong>High-Level Inference:</strong> <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=false>(</mo><mover accent=true><mi>l</mi><mo>^</mo></mover><mi mathvariant=normal>∣</mi><msub><mi>o</mi><mi>t</mi></msub><mo separator=true>,</mo><mi>l</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>\pi_{\theta}(\hat{l} | o_{t}, l)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.2079em;vertical-align:-0.25em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord accent"><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.9579em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span></span><span style=top:-3.2634em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1667em><span class=mord>^</span></span></span></span></span></span></span><span class=mord>∣</span><span class=mord><span class="mord mathnormal">o</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span><span class=mclose>)</span></span></span></span> (Predicting "what to do next").</li>
<li class=""><strong>Low-Level Inference:</strong> <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=false>(</mo><msub><mi>a</mi><mrow><mi>t</mi><mo>:</mo><mi>t</mi><mo>+</mo><mi>H</mi></mrow></msub><mi mathvariant=normal>∣</mi><msub><mi>o</mi><mi>t</mi></msub><mo separator=true>,</mo><mover accent=true><mi>l</mi><mo>^</mo></mover><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>\pi_{\theta}(a_{t:t+H} | o_{t}, \hat{l})</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.2079em;vertical-align:-0.25em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">a</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style=margin-right:0.08125em>H</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2083em><span></span></span></span></span></span></span><span class=mord>∣</span><span class=mord><span class="mord mathnormal">o</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord accent"><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.9579em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span></span><span style=top:-3.2634em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1667em><span class=mord>^</span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span> (Predicting "how to move").</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=4-combining-discrete--continuous-actions>4. Combining Discrete & Continuous Actions<a href=#4-combining-discrete--continuous-actions class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<p>The model employs a hybrid approach to balance training efficiency with inference speed and quality.</p>
<ul>
<li class=""><strong>The Dilemma:</strong>
<ul>
<li class=""><strong>Discrete Tokens (FAST):</strong> Fast training, but requires slow autoregressive decoding during inference.</li>
<li class=""><strong>Continuous (Flow Matching):</strong> High quality and smooth control, but computationally expensive to train from scratch on massive datasets.</li>
</ul>
</li>
<li class=""><strong>The Solution:</strong> Train on discretized actions (FAST) but use Flow Matching for inference.<!-- -->
<ul>
<li class=""><strong>Attention Masking:</strong> Ensures discrete and continuous action representations do not attend to each other during joint training.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=hybrid-loss-function>Hybrid Loss Function<a href=#hybrid-loss-function class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<p>The model minimizes a combined objective:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mi mathvariant=double-struck>E</mi><mrow><mo fence=true>[</mo><munder><munder><mrow><mi>H</mi><mo stretchy=false>(</mo><mi>x</mi><mo separator=true>,</mo><msubsup><mi>f</mi><mi>θ</mi><mi>l</mi></msubsup><mo stretchy=false>)</mo></mrow><mo stretchy=true>⏟</mo></munder><mtext>Cross Entropy</mtext></munder><mo>+</mo><mi>α</mi><munder><munder><mrow><mi mathvariant=normal>∥</mi><mi>ω</mi><mo>−</mo><mi>a</mi><mo>−</mo><msubsup><mi>f</mi><mi>θ</mi><mi>a</mi></msubsup><msup><mi mathvariant=normal>∥</mi><mn>2</mn></msup></mrow><mo stretchy=true>⏟</mo></munder><mtext>MSE for Flow</mtext></munder><mo fence=true>]</mo></mrow></mrow><annotation encoding=application/x-tex>\mathbb{E} \left[ \underbrace{H(x, f^l_\theta)}_{\text{Cross Entropy}} + \alpha \underbrace{\| \omega - a - f^a_\theta \|^2}_{\text{MSE for Flow}} \right]</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:3.7624em;vertical-align:-1.7124em></span><span class="mord mathbb">E</span><span class=mspace style=margin-right:0.1667em></span><span class=minner><span class=mopen><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.05em><span style=top:-4.05em><span class=pstrut style=height:5.6em></span><span style=width:0.667em;height:3.600em><svg xmlns=http://www.w3.org/2000/svg width=0.667em height=3.600em viewBox="0 0 667 3600"><path d="M403 1759 V84 H666 V0 H319 V1759 v0 v1759 h347 v-84
H403z M403 1759 V0 H319 V1759 v0 v1759 h84z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.55em><span></span></span></span></span></span></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8991em><span style=top:-1.4237em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">Cross Entropy</span></span></span></span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8991em><span class=svg-align style=top:-2.102em><span class=pstrut style=height:3em></span><span class=stretchy style=height:0.548em;min-width:1.6em><span class=brace-left style=height:0.548em><svg xmlns=http://www.w3.org/2000/svg width=400em height=0.548em viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"/></svg></span><span class=brace-center style=height:0.548em><svg xmlns=http://www.w3.org/2000/svg width=400em height=0.548em viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"/></svg></span><span class=brace-right style=height:0.548em><svg xmlns=http://www.w3.org/2000/svg width=400em height=0.548em viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"/></svg></span></span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em>H</span><span class=mopen>(</span><span class="mord mathnormal">x</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10764em>f</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8991em><span style=top:-2.453em;margin-left:-0.1076em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.01968em>l</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.898em><span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.7124em><span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span><span class="mord mathnormal" style=margin-right:0.0037em>α</span><span class="mord munder"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8641em><span style=top:-1.4159em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">MSE for Flow</span></span></span></span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8641em><span class=svg-align style=top:-2.102em><span class=pstrut style=height:3em></span><span class=stretchy style=height:0.548em;min-width:1.6em><span class=brace-left style=height:0.548em><svg xmlns=http://www.w3.org/2000/svg width=400em height=0.548em viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"/></svg></span><span class=brace-center style=height:0.548em><svg xmlns=http://www.w3.org/2000/svg width=400em height=0.548em viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"/></svg></span><span class=brace-right style=height:0.548em><svg xmlns=http://www.w3.org/2000/svg width=400em height=0.548em viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"/></svg></span></span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>∥</span><span class="mord mathnormal" style=margin-right:0.03588em>ω</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:0.2222em></span><span class="mord mathnormal">a</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:0.2222em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10764em>f</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.7144em><span style=top:-2.453em;margin-left:-0.1076em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span><span class=mord><span class=mord>∥</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8641em><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.898em><span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.5841em><span></span></span></span></span></span><span class=mclose><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.05em><span style=top:-4.05em><span class=pstrut style=height:5.6em></span><span style=width:0.667em;height:3.600em><svg xmlns=http://www.w3.org/2000/svg width=0.667em height=3.600em viewBox="0 0 667 3600"><path d="M347 1759 V0 H0 V84 H263 V1759 v0 v1759 H0 v84 H347z
M347 1759 V0 H263 V1759 v0 v1759 h84z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.55em><span></span></span></span></span></span></span></span></span></span></span></span>
<ul>
<li class=""><strong>Cross Entropy:</strong> For text and discrete action tokens.</li>
<li class=""><strong>MSE:</strong> For the Flow Matching vector field (Action Expert).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=5-training-recipe>5. Training Recipe<a href=#5-training-recipe class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<p>The training is split into two distinct stages based on the <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>α</mi></mrow><annotation encoding=application/x-tex>\alpha</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.0037em>α</span></span></span></span> parameter and the inclusion of the Action Expert.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=stage-1-pre-training-alpha--0>Stage 1: Pre-training (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>α</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=application/x-tex>\alpha = 0</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.0037em>α</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0</span></span></span></span>)<a href=#stage-1-pre-training-alpha--0 class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class=""><strong>Goal:</strong> Efficient large-scale learning.</li>
<li class=""><strong>Method:</strong> <strong>Action Expert is OFF.</strong> Trains as a standard auto-regressive transformer using next-token prediction for text and <strong>discrete FAST action tokens</strong>.</li>
<li class=""><strong>Datasets</strong>:<!-- -->
<ul>
<li class=""><strong>MM:</strong> Mobile Manipulator data (100+ homes).</li>
<li class=""><strong>ME:</strong> Multi-Environment non-mobile robots.</li>
<li class=""><strong>CE:</strong> Cross-Embodiment laboratory data (diverse tasks like folding).</li>
<li class=""><strong>HL:</strong> High-Level subtask prediction data.</li>
<li class=""><strong>WD:</strong> Multimodal Web Data (VQA, captioning).</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=stage-2-post-training-alpha--100>Stage 2: Post-training (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>α</mi><mo>=</mo><mn>10.0</mn></mrow><annotation encoding=application/x-tex>\alpha = 10.0</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.0037em>α</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>10.0</span></span></span></span>)<a href=#stage-2-post-training-alpha--100 class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class=""><strong>Goal:</strong> Specialization for mobile manipulation and enabling continuous control.</li>
<li class=""><strong>Method:</strong> <strong>Action Expert is ON.</strong>
<ul>
<li class="">Initialized with random weights.</li>
<li class="">Jointly trains next-token prediction (to preserve text capabilities) and <strong>Flow Matching</strong> for continuous actions.</li>
</ul>
</li>
<li class=""><strong>Key Addition (Verbal Instructions - VI):</strong>
<ul>
<li class="">Data collected by "teleoperating" the robot using language commands (e.g., expert users selecting sub-tasks step-by-step).</li>
<li class="">Crucial for training the model to predict high-quality subtasks (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mover accent=true><mi>l</mi><mo>^</mo></mover></mrow><annotation encoding=application/x-tex>\hat{l}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9579em></span><span class="mord accent"><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.9579em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span></span><span style=top:-3.2634em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1667em><span class=mord>^</span></span></span></span></span></span></span></span></span></span>).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=6-evaluation>6. Evaluation<a href=#6-evaluation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=methodology>Methodology<a href=#methodology class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class=""><strong>Settings:</strong> Tested in entirely new kitchens and bedrooms not seen during training.</li>
<li class=""><strong>Tasks:</strong> Long-horizon tasks like cleaning kitchens, putting laundry away, and making beds.</li>
<li class=""><strong>Metrics:</strong> Task progress (percentage of steps completed) and Language Following Rate.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=key-findings>Key Findings<a href=#key-findings class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class=""><strong>Generalization:</strong> <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>π</mi><mn>0.5</mn></msub></mrow><annotation encoding=application/x-tex>\pi_{0.5}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0.5</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> successfully performs multi-stage tasks in real, unseen homes.</li>
<li class=""><strong>Scaling:</strong> Performance improves consistently as the number of training environments increases.</li>
<li class=""><strong>Ablation Studies:</strong>
<ul>
<li class=""><strong>Cross-Embodiment (CE/ME):</strong> Excluding data from other robots significantly degrades performance, indicating strong transfer learning.</li>
<li class=""><strong>Web Data (WD):</strong> While less critical for general task progress, it is essential for <strong>Out-of-Distribution (OOD)</strong> object generalization and language following.</li>
</ul>
</li>
<li class=""><strong>Comparison:</strong> Significantly outperforms <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>π</mi><mn>0</mn></msub></mrow><annotation encoding=application/x-tex>\pi_0</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> and the <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>π</mi><mn>0</mn></msub></mrow><annotation encoding=application/x-tex>\pi_0</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>-FAST+Flow baseline.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=7-conclusions--future-work>7. Conclusions & Future Work<a href=#7-conclusions--future-work class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Current Status:</strong> <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>π</mi><mn>0.5</mn></msub></mrow><annotation encoding=application/x-tex>\pi_{0.5}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0.5</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> demonstrates that co-training with heterogeneous data enables end-to-end robotic systems to perform long-horizon, dexterous skills in open-world settings.</li>
<li class=""><strong>Limitations:</strong>
<ul>
<li class="">Struggles with physical constraints (hard-to-open cabinets) or partial observability.</li>
<li class="">Limited to relatively simple prompts based on training data.</li>
</ul>
</li>
<li class=""><strong>Future Directions:</strong>
<ul>
<li class="">Incorporating richer context and memory for better handling of partial observability.</li>
<li class="">Expanding data sources, particularly exploring <strong>verbal instructions</strong> as a powerful new supervision modality.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Intelligence, P., Black, K., Brown, N., Darpinian, J., Dhabalia, K., Driess, D., Esmail, A., Equi, M., Finn, C., & Fusai, N. (2025). π0.5: a Vision-Language-Action Model with Open-World Generalization. arXiv preprint arXiv:2504.16054.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_f1Hy><a href=/en/2025/09/04/vlatest-review/>VLA Test Review</a></h2><div class="container_mt6G margin-vert--md"><time datetime=2025-09-03T14:38:22.388Z>September 3, 2025</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=vlatest-testing-and-evaluating-vision-language-action--models-for-robotic-manipulation>VLATest: Testing and Evaluating Vision-Language-Action  Models for Robotic Manipulation<a href=#vlatest-testing-and-evaluating-vision-language-action--models-for-robotic-manipulation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">VLATest fuzzes 18,604 manipulation scenes (10 operators, 4 tasks) to systematically stress-test VLA robustness.</li>
<li class="">Seven VLA models show low success and brittleness to confounders, lighting/camera changes, unseen objects, and instruction mutations; larger pretraining helps.</li>
<li class="">Priorities: scale/augment demo data (incl. sim2real), use stepwise/CoT prompting & multi-agent setups, and expand benchmarks with online risk assessment.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=motivation--gap>Motivation & Gap<a href=#motivation--gap class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Problem:</strong> Current VLA models are typically evaluated on <strong>small, hand-crafted scenes</strong>, leaving <strong>general performance and robustness</strong> in diverse scenarios underexplored.</li>
<li class=""><strong>Goal:</strong> Introduce <strong>VLATest</strong>, a <strong>generation-based fuzzing framework</strong> that automatically creates robotic manipulation scenes to <strong>test performance and robustness</strong> of VLA models.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=what-are-vla-models>What Are VLA Models?<a href=#what-are-vla-models class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Vision-Language-Action (VLA)</strong> models take <strong>natural language instructions</strong> + <strong>camera images</strong> and output <strong>low-level robot actions</strong> (Δx, Δθ, Δgrip).</li>
<li class=""><strong>Inference loop:</strong> Tokenize text/image → transformer predicts action token <strong>A₁</strong> → execute → append <strong>A₁</strong> + new image tokens <strong>I₂</strong> → predict <strong>A₂</strong> → … until success or step limit.</li>
</ul>
<p><img decoding=async loading=lazy alt="VLA Architecture" src=/en/assets/images/vla-architecture-6c70ccb1f0cb43def096bc28e5b0abf9.png width=2022 height=514 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=training--evaluation>Training & Evaluation<a href=#training--evaluation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Training:</strong> (1) Train from scratch on robot demonstrations, or (2) <strong>fine-tune a large VLM</strong> (e.g., Llava) with <code>></code>1B params pretraining.</li>
<li class=""><strong>Evaluation:</strong> Task-specific metrics (e.g., <strong>grasp</strong>, <strong>lift</strong>, <strong>hold</strong> for “pick up”), either in <strong>sim</strong> (auto-metrics) or <strong>real</strong> (manual labels).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=vlatest-framework>VLATest Framework<a href=#vlatest-framework class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Ten testing operators</strong> grouped across:<!-- -->
<ul>
<li class=""><strong>Target objects:</strong> type, position, orientation</li>
<li class=""><strong>Confounding objects:</strong> type, position, orientation, <strong>count</strong></li>
<li class=""><strong>Lighting:</strong> <strong>intensity</strong></li>
<li class=""><strong>Camera:</strong> <strong>position</strong>, <strong>orientation</strong></li>
</ul>
</li>
<li class=""><strong>Scene generation (Alg. 1):</strong> sample valid targets → (optional) confounders → mutate lighting (factor <strong>α</strong>) → mutate camera pose (<strong>d</strong>, <strong>θ</strong>). Semantic validity checks prevent infeasible scenes.</li>
</ul>
<p><img decoding=async loading=lazy alt="VLA Test" src=/en/assets/images/vla-test-7989c6573c1574d8dfc5e67ca632ca0d.png width=1926 height=340 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=research-questions-rq>Research Questions (RQ)<a href=#research-questions-rq class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>RQ1:</strong> Basic performance on popular manipulation tasks</li>
<li class=""><strong>RQ2:</strong> Effect of <strong>confounding object count</strong></li>
<li class=""><strong>RQ3:</strong> Effect of <strong>lighting changes</strong></li>
<li class=""><strong>RQ4:</strong> Effect of <strong>camera pose changes</strong></li>
<li class=""><strong>RQ5:</strong> Robustness to <strong>unseen objects</strong> (OOD)</li>
<li class=""><strong>RQ6:</strong> Robustness to <strong>instruction mutations</strong></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=tasks--prompting>Tasks & Prompting<a href=#tasks--prompting class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Tasks:</strong>
<ol>
<li class=""><strong>Pick up</strong> an object (grasp + lift ≥0.02 m for 5 frames)</li>
<li class=""><strong>Move A near B</strong> (≤0.05 m)</li>
<li class=""><strong>Put A on B</strong> (stable stacking)</li>
<li class=""><strong>Put A into B</strong> (fully inside)</li>
</ol>
</li>
<li class=""><strong>Standard prompts (RQ1–RQ5):</strong>
<ul>
<li class=""><code>pick up [obj]</code> · <code>move [objA] near [objB]</code> · <code>put [objA] on [objB]</code> · <code>put [objA] into [objB]</code></li>
</ul>
</li>
<li class=""><strong>Instruction mutations (RQ6):</strong> 10 paraphrases per task (GPT-4o), manually validated for semantic equivalence.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=experimental-setup>Experimental Setup<a href=#experimental-setup class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Scenes:</strong> <strong>18,604</strong> across 4 tasks (ManiSkill2).</li>
<li class=""><strong>Models:</strong> 7 public VLAs (RT-1-1k/58k/400k, RT-1-X, Octo-small/base, OpenVLA-7b).</li>
<li class=""><strong>Compute:</strong> <code>></code><strong>580 GPU hours</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=key-results--findings>Key Results & Findings<a href=#key-results--findings class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=rq1--overall-performance>RQ1 — Overall Performance<a href=#rq1--overall-performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">VLA models <strong>underperform</strong> overall; no single model dominates across tasks.</li>
<li class="">Example best-case rates (default settings): <strong>34.4%</strong> (Task1, RT-1-400k), <strong>12.7%</strong> (Task2, OpenVLA-7b), <strong>2.2%</strong> (Task3, RT-1-X), <strong>2.1%</strong> (Task4, Octo-small).</li>
<li class=""><strong>Stepwise breakdown (Task 1):</strong> grasp <strong>23.3%</strong> → lift <strong>15.7%</strong> → hold <strong>12.4%</strong> ⇒ difficulty <strong>composing sequential actions</strong>.<!-- -->
<ul>
<li class=""><strong>Implication (Finding 2):</strong> Consider <strong>stepwise prompting / chain-of-thought</strong> to decompose complex tasks.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=rq1--coverage-metric>RQ1 — Coverage Metric<a href=#rq1--coverage-metric class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">No established coverage for VLA; adopted <strong>trajectory coverage</strong> (pragmatic).</li>
<li class="">Increasing cases from <strong>n=10</strong> to <strong>n=1000</strong> achieved <strong>100%</strong> coverage across tasks (object-position novelty relative to workspace).</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=rq2--confounding-objects>RQ2 — Confounding Objects<a href=#rq2--confounding-objects class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class=""><strong>More confounders ⇒ worse performance</strong>; models struggle to <strong>locate the correct object</strong>.</li>
<li class=""><strong>Similarity doesn’t matter much:</strong> Mann–Whitney U shows <strong>no significant difference</strong> between <strong>similar</strong> vs <strong>dissimilar</strong> distractors (p = 0.443, 0.614, 0.657, 0.443; effect sizes ≈ 0.23–0.29).</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=rq3--lighting-robustness>RQ3 — Lighting Robustness<a href=#rq3--lighting-robustness class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class=""><strong>Lighting perturbations significantly hurt performance.</strong></li>
<li class=""><strong>OpenVLA-7b</strong> most robust (<strong>77.9%</strong> of previously passed cases still pass), plausibly due to <strong>SigLIP + DINOv2</strong> pretraining and LLaVA 1.5 mixture.</li>
<li class=""><strong>Sensitivity:</strong> even <strong>α <code>&lt;</code> 2.5</strong> increase drops success to ~<strong>0.7×</strong>; <strong>α <code>></code> 8</strong> ⇒ ~<strong>40%</strong> of default-pass scenes succeed.</li>
<li class=""><strong>Decreasing</strong> light hurts <strong>less</strong> than increasing; <strong>α <code>&lt;</code> 0.2</strong> still ~<strong>60%</strong> pass.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=rq4--camera-pose-robustness>RQ4 — Camera Pose Robustness<a href=#rq4--camera-pose-robustness class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Small pose changes (≤<strong>5°</strong> rotation, ≤<strong>5 cm</strong> shift) reduce success to <strong>34.0%</strong> of default.</li>
<li class=""><strong>RT-1-400k</strong> most robust (<strong>45.6%</strong> retain), <strong>OpenVLA-7b</strong> at <strong>31.3%</strong>; <strong>Octo</strong> models <code>&lt;</code><strong>10%</strong>.<!-- -->
<ul>
<li class="">Likely due to <strong>training data scale</strong> differences.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=rq5--unseen-objects>RQ5 — Unseen Objects<a href=#rq5--unseen-objects class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Using <strong>YCB (56 unseen objects)</strong> leads to large performance drops versus seen objects: avg <strong>–74.2%</strong>, <strong>–66.7%</strong>, <strong>–66.7%</strong>, <strong>–20.0%</strong> on Tasks 1–4.</li>
<li class=""><strong>Transfer rate</strong> across steps:<!-- -->
<ul>
<li class=""><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mstyle scriptlevel=0 displaystyle=true><msubsup><mi>T</mi><mi>r</mi><mi>n</mi></msubsup><mo>=</mo><mfrac><msub><mtext>Success rate</mtext><mi>n</mi></msub><msub><mtext>Success rate</mtext><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mfrac></mstyle></mrow><annotation encoding=application/x-tex>\displaystyle T_r^n = \frac{\text{Success rate}_n}{\text{Success rate}_{n-1}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9614em;vertical-align:-0.247em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em>T</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.7144em><span style=top:-2.453em;margin-left:-0.1389em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>r</span></span></span><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.2547em;vertical-align:-0.8943em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.3603em><span style=top:-2.314em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord text"><span class=mord>Success rate</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2083em><span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:0.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord text"><span class=mord>Success rate</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.8943em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>, with <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mtext>Success rate</mtext><mn>0</mn></msub><mo>=</mo><mn>100</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>\text{Success rate}_0 = 100\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord text"><span class=mord>Success rate</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em></span><span class=mord>100%</span></span></span></span></li>
<li class="">Paired t-tests show significant differences on <strong><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msubsup><mi>T</mi><mi>r</mi><mn>1</mn></msubsup></mrow><annotation encoding=application/x-tex>T_r^1</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0611em;vertical-align:-0.247em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em>T</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-2.453em;margin-left:-0.1389em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>r</span></span></span><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span></span></span></span></strong> for <strong>Task 1 & 2</strong> (p = 0.011, 0.007; Cohen’s d = 1.34, 0.891).</li>
<li class=""><strong>Primary failure mode:</strong> <strong>recognizing/locating unseen objects</strong>.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id=rq6--instruction-mutations>RQ6 — Instruction Mutations<a href=#rq6--instruction-mutations class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h3>
<ul>
<li class="">Mutated instructions generally <strong>reduce performance</strong> (avg drops: <strong>–32.8%</strong> T1, <strong>–1.7%</strong> T2, <strong>–8.3%</strong> T3; negligible on T4).</li>
<li class=""><strong>Larger language backbones help:</strong> <strong>OpenVLA-7b (Llama 2-7B)</strong> is <strong>more robust</strong>, sometimes <strong>improving</strong> under mutations (e.g., T1, T4).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=implications--directions>Implications & Directions<a href=#implications--directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Scale matters:</strong> larger <strong>pretraining</strong> and <strong>robot-demo datasets</strong> improve robustness (lighting/camera).</li>
<li class=""><strong>Data enrichment:</strong> use <strong>data augmentation</strong> and <strong>sim-to-real</strong> to diversify external factors; leverage <strong>traditional controllers</strong> to auto-generate demonstrations.</li>
<li class=""><strong>Prompting strategies:</strong> adopt <strong>stepwise/CoT prompting</strong>; consider <strong>multi-agent</strong> decompositions.</li>
<li class=""><strong>Benchmarking:</strong> the <strong>18,604</strong> VLATest scenes serve as an <strong>early benchmark</strong>; expand to more tasks/robots/conditions.</li>
<li class=""><strong>Online risk assessment:</strong> explore <strong>uncertainty estimation</strong> and <strong>safety monitoring</strong> for runtime quality control.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=related-work>Related Work<a href=#related-work class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Robotics foundation models:</strong> (1) LLMs for planning/rewards; (2) <strong>Multi-modal</strong> FMs (VLMs/VLAs) for manipulation & perception.</li>
<li class=""><strong>CPS testing:</strong> gray-box/black-box fuzzing and search-based testing exist, but <strong>not directly applicable</strong> to VLAs (multimodality, autoregression, scale).</li>
<li class=""><strong>FM evaluation:</strong> beyond static benchmarks, VLATest <strong>dynamically generates</strong> 3D manipulation test cases—distinct from <strong>text-only</strong> testing.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=threats-to-validity-mitigations-in-study>Threats to Validity (mitigations in study)<a href=#threats-to-validity-mitigations-in-study class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Internal:</strong> randomness (mitigated by <strong>18,604</strong> scenes); potential prompt bias (mutations <strong>manually validated</strong>).</li>
<li class=""><strong>External:</strong> generalization to other tasks/models; chose <strong>popular tasks</strong> (Open X-Embodiment) and <strong>SOTA public models</strong>.</li>
<li class=""><strong>Construct:</strong> limited operators (lighting/camera/confounders chosen; future: #lights, camera intrinsics, resolution).<!-- -->
<ul>
<li class="">Coverage: <strong>trajectory coverage</strong> used as a pragmatic proxy.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>VLATest</strong>: early, <strong>generation-based fuzzing</strong> framework (10 operators) for VLA testing in ManiSkill2.</li>
<li class=""><strong>Empirical evidence</strong> across <strong>7 models / 4 tasks / 18,604 scenes</strong> shows <strong>limited robustness</strong> (lighting, camera, unseen objects, instruction variation).</li>
<li class="">Points to <strong>data scaling</strong>, <strong>prompting</strong>, <strong>benchmarking</strong>, and <strong>risk assessment</strong> as practical paths to <strong>more reliable</strong> VLA systems.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Wang, Z., Zhou, Z., Song, J., Huang, Y., Shu, Z., & Ma, L. (2025). VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation. Proceedings of the ACM on Software Engineering, 2(FSE), 1615–1638.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_f1Hy><a href=/en/2025/09/01/open-x-embodiment-review/>Open X-Embodiment review</a></h2><div class="container_mt6G margin-vert--md"><time datetime=2025-09-01T03:47:28.350Z>September 1, 2025</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=rt-x>RT-X<a href=#rt-x class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">RT-X trains generalist robot policies by co-training RT-1/RT-2 on an X-embodiment mix of multi-robot, multi-task data, enabling efficient adaptation to new robots, tasks, and environments.</li>
<li class="">It standardizes 1M+ trajectories from 22 embodiments into the Open X-Embodiment (RLDS/tfrecord) repository, unifying observations and 7-DoF actions via coarse alignment.</li>
<li class="">Experiments show strong positive transfer and emergent skills (≈3× with RT-2-X on cross-robot tasks); performance scales with model capacity, short image histories, and web pretraining, while sensing/actuation diversity and frame alignment remain open problems.</li>
</ul>
<p><img decoding=async loading=lazy alt="RT-X Architecture" src=/en/assets/images/rt-x-architecture-cc2128128460577bc8f720626e0d671d.png width=1552 height=412 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=motivation>Motivation<a href=#motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Seeks a <strong>generalist X-robot policy</strong> that can be efficiently adapted to new robots, tasks, and environments.</li>
<li class="">Mirrors a trend from CV/NLP where <strong>general-purpose, web-scale pretrained models</strong> outperform narrow, task-specific models.</li>
<li class="">Robotics lacks comparably large, diverse <strong>interaction datasets</strong>, making direct transfer of these lessons challenging.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=objectives>Objectives<a href=#objectives class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ol>
<li class=""><strong>Positive transfer:</strong> Test whether co-training on data from many robots improves performance on each training domain.</li>
<li class=""><strong>Ecosystem building:</strong> Organize large robotic datasets to enable future X-embodiment research.</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=core-approach>Core Approach<a href=#core-approach class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Train <strong>RT-1</strong> and <strong>RT-2</strong> on data from <strong>9 different manipulators</strong>, producing <strong>RT-X</strong> variants that outperform policies trained only on the evaluation domain and show <strong>better generalization</strong> and <strong>new capabilities</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=whats-different-from-prior-transfer-methods>What’s Different From Prior Transfer Methods<a href=#whats-different-from-prior-transfer-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Many prior works reduce the <strong>embodiment gap</strong> via specialized mechanisms (shared action spaces, representation learning objectives, policy adaptation using embodiment metadata, decoupled robot/environment representations, domain translation).</li>
<li class=""><strong>RT-X directly trains on X-embodiment data without explicit gap-reduction machinery</strong> and still observes <strong>positive transfer</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=dataset--format-open-x-embodiment>Dataset & Format (Open X-Embodiment)<a href=#dataset--format-open-x-embodiment class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>1M+ real robot trajectories, 22 embodiments</strong> (single-arm, bimanual, quadrupeds), pooled from <strong>60 datasets / 34 labs</strong>, standardized for easy use.</li>
<li class="">Uses <a href=https://github.com/google-research/rlds target=_blank rel="noopener noreferrer" class=""><strong>RLDS</strong></a> (serialized <code>tfrecord</code>), supporting varied action spaces and input modalities (RGB, depth, point clouds), and efficient parallel loading across major DL frameworks.</li>
<li class="">Language annotations are leveraged; <strong>PaLM</strong> is used to extract objects/behaviors from instructions.</li>
</ul>
<p><img decoding=async loading=lazy alt=RLDS src=/en/assets/images/rlds-5e68d1c660ef048892d5594530c62239.png width=726 height=353 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=data-format-consolidation-coarse-alignment>Data Format Consolidation (Coarse Alignment)<a href=#data-format-consolidation-coarse-alignment class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Observations:</strong> History of recent images + language instruction. One <strong>canonical camera view</strong> per dataset is resized to a common resolution.</li>
<li class=""><strong>Actions:</strong> Convert original controls to a <strong>7-DoF end-effector vector</strong> (x, y, z, roll, pitch, yaw, gripper or their rates). Actions are <strong>normalized before discretization</strong>; outputs are <strong>de-normalized per embodiment</strong>.</li>
<li class=""><strong>Deliberate non-alignment:</strong> Camera poses/properties are <strong>not</strong> standardized; action frame alignment across datasets is <strong>not</strong> enforced. The same action vector may cause <strong>different motions</strong> on different robots (absolute/relative, position/velocity allowed).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=policy-architectures>Policy Architectures<a href=#policy-architectures class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>RT-1 (≈35M params):</strong> Transformer for control. Inputs: 15-frame image history + natural-language instruction.<!-- -->
<ul>
<li class="">Vision via ImageNet-pretrained <strong>EfficientNet</strong>; language via <strong>USE</strong> embedding.</li>
<li class="">Fuse via <strong>FiLM</strong> → 81 vision–language tokens → <strong>decoder-only Transformer</strong> outputs tokenized actions.</li>
</ul>
</li>
<li class=""><strong>RT-2 (VLA family):</strong> Internet-scale VLM co-fine-tuned to output <strong>action as text tokens</strong> (e.g., <code>1 128 91 241 5 101 127</code>).<!-- -->
<ul>
<li class="">Any pretrained VLM can be adapted; this work uses <strong>RT-2–PaLI-X</strong> (ViT backbone + UL2 LM; primarily pretrained on WebLI).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=training-setup>Training Setup<a href=#training-setup class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Robotics data mixture:</strong> Data from <strong>9 manipulators</strong> (a union of multiple well-known robotics datasets).</li>
<li class=""><strong>Loss:</strong> Standard <strong>categorical cross-entropy</strong> over tokenized actions.</li>
<li class=""><strong>Regimes:</strong>
<ul>
<li class=""><strong>RT-1-X:</strong> Trained solely on the robotics mixture.</li>
<li class=""><strong>RT-2-X:</strong> <strong>Co-fine-tuned</strong> on a ~1:1 mix of original VLM data and the robotics mixture.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=experimental-questions>Experimental Questions<a href=#experimental-questions class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ol>
<li class="">Does X-embodiment co-training improve in-domain performance (positive transfer)?</li>
<li class="">Does it improve <strong>generalization</strong> to <strong>unseen tasks</strong>?</li>
<li class="">How do <strong>model size</strong>, <strong>architecture</strong>, and <strong>dataset composition</strong> influence performance/generalization?</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=key-results>Key Results<a href=#key-results class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Small-scale domains:</strong> <strong>RT-1-X</strong> outperforms the <strong>Original Method</strong> (the authors’ per-dataset baselines) on <strong>4/5</strong> datasets with a large average gain → <strong>limited data domains</strong> benefit greatly from X-embodiment co-training.</li>
<li class=""><strong>Large-scale domains:</strong>
<ul>
<li class=""><strong>RT-1-X</strong> does <strong>not</strong> beat an RT-1 trained only on the embodiment-specific large dataset (suggests underfitting for this class).</li>
<li class=""><strong>RT-2-X</strong> (larger capacity) <strong>outperforms both</strong> Original Method and RT-1 → X-robot training helps even in <strong>data-rich</strong> regimes when using <strong>sufficient capacity</strong>.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=generalization--emergent-skills>Generalization & Emergent Skills<a href=#generalization--emergent-skills class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Unseen objects/backgrounds/environments:</strong> RT-2 and RT-2-X perform <strong>on par</strong> (VLM backbone already strong here).</li>
<li class=""><strong>Emergent skills (transfer across robots):</strong> On Google Robot tasks that <strong>do not appear</strong> in RT-2’s dataset but exist in <strong>Bridge</strong> (for <strong>WidowX</strong>), <strong>RT-2-X ≈ 3×</strong> RT-2.<!-- -->
<ul>
<li class="">Removing <strong>Bridge</strong> from RT-2-X training <strong>significantly reduces</strong> hold-out performance → skills likely <strong>transferred</strong> from WidowX data.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=design-insights-ablations>Design Insights (Ablations)<a href=#design-insights-ablations class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Short image history</strong> notably <strong>improves generalization</strong>.</li>
<li class=""><strong>Web pretraining</strong> is <strong>critical</strong> for large models’ high performance.</li>
<li class=""><strong>Model capacity matters:</strong> <strong>55B</strong> model succeeds more than <strong>5B</strong> on emergent skills → greater capacity ⇒ greater cross-dataset transfer.</li>
<li class=""><strong>Co-fine-tuning vs. fine-tuning:</strong> Similar performance in this study (attributed to the <strong>greater diversity</strong> of robotics data in RT-2-X vs. prior works).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=limitations-open-problems>Limitations (Open Problems)<a href=#limitations-open-problems class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Does <strong>not</strong> cover robots with <strong>very different sensing/actuation modalities</strong>.</li>
<li class="">Does <strong>not</strong> study generalization to <strong>new robots</strong> nor define a <strong>decision criterion</strong> for when positive transfer will occur.</li>
<li class="">Camera pose/properties and control frame <strong>remain unaligned</strong>; a deliberate but still challenging domain gap to address in future work.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">O’Neill, A., Rehman, A., Maddukuri, A., Gupta, A., Padalkar, A., Lee, A., Pooley, A., Gupta, A., Mandlekar, A., & Jain, A. (2024). Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. 2024 IEEE International Conference on Robotics and Automation (ICRA).</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_f1Hy><a href=/en/2025/09/01/pi-zero-reivew/>π0 Review</a></h2><div class="container_mt6G margin-vert--md"><time datetime=2025-08-31T14:24:33.716Z>August 31, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=π0>π0<a href=#π0 class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=problem--motivation>Problem & Motivation<a href=#problem--motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Achieving real-world generality in robot learning is blocked by <strong>data scarcity, generalization, and robustness</strong> limits.</li>
<li class="">Human intelligence most outpaces machines in <strong>versatility</strong>—solving diverse, physically situated tasks under constraints, language commands, and perturbations.</li>
<li class="">In NLP/CV, <strong>foundation models</strong> pre-trained on diverse multi-task data, then <strong>fine-tuned (aligned)</strong> on curated datasets, outperform narrow specialists; the same paradigm is hypothesized for robotics.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=core-proposal>Core Proposal<a href=#core-proposal class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">A <strong>novel flow-matching architecture</strong> built on a pre-trained <strong>Vision-Language Model (VLM)</strong> to inherit Internet-scale semantics.</li>
<li class="">Further training adds <strong>robot actions</strong>, turning the model into a <strong>Vision-Language-Action (VLA)</strong> policy.</li>
<li class="">Use <strong>cross-embodiment training</strong> to combine data from many robot types (single/dual-arm, mobile), despite differing configuration/action spaces.</li>
<li class="">Employ <strong>action chunking</strong> + <strong>flow matching</strong> (diffusion variant) to model complex, continuous, high-frequency actions.</li>
<li class="">Introduce an <strong>Action Expert</strong> (separate weights for action/state tokens), akin to a <strong>Mixture-of-Experts</strong>, augmenting the standard VLM.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=training-recipe-pre--vs-post-training>Training Recipe (Pre- vs Post-Training)<a href=#training-recipe-pre--vs-post-training class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Pre-training</strong> on highly diverse data builds broad, general physical abilities.</li>
<li class=""><strong>Post-training</strong> on curated, task-specific data instills <strong>fluent, efficient strategies</strong>.</li>
<li class="">Rationale: high-quality-only training lacks recovery behaviors; low-quality-only training lacks efficiency/robustness; <strong>combining both</strong> yields desired behavior.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=data--backbone>Data & Backbone<a href=#data--backbone class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">~<strong>10,000 hours</strong> of demonstrations + the <strong>OXE</strong> dataset; data spans <strong>7 robot configurations</strong> and <strong>68 tasks</strong>.</li>
<li class="">VLM backbone initialized from <strong>PaliGemma (3B)</strong>; add <strong>~300M</strong> parameters for the action expert (total <strong>~3.3B</strong>).</li>
<li class="">Pre-training mixture: weighted combination of internal datasets + full OXE; <strong>n^0.43</strong> weighting to down-weight overrepresented task-robot pairs.</li>
<li class="">Unify interfaces: zero-pad <strong>qt/at</strong> to the largest robot dimension (18); mask missing image slots; late-fusion encoders map images/states to the same token space as language.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=modeling-details>Modeling Details<a href=#modeling-details class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Conditional flow matching</strong> models the continuous distribution over action chunks.</li>
<li class="">Train with a <strong>diffusion-style loss</strong> on individual sequence elements (instead of cross-entropy), with separate weights for diffusion-related tokens.</li>
<li class="">Flow path uses a <strong>linear-Gaussian</strong> schedule; sample noisy actions with ε∼N(0, I); predict denoising vector field; <strong>Euler integration</strong> from τ=0→1 at inference.</li>
<li class="">Efficient inference by <strong>caching</strong> K/V for the observation prefix; action tokens recomputed per integration step.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=high-level-language-policy>High-Level Language Policy<a href=#high-level-language-policy class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Because the policy consumes language, a <strong>high-level VLM</strong> can decompose tasks (e.g., bussing) into intermediate language subgoals (SayCan-style planning), improving performance on complex, temporally extended tasks.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=evaluation-setup--baselines>Evaluation Setup & Baselines<a href=#evaluation-setup--baselines class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Out-of-box</strong> (direct prompting), <strong>fine-tuning</strong> on downstream tasks, and <strong>with high-level VLM</strong> providing intermediate commands.</li>
<li class="">Compare against <strong>OpenVLA (7B, autoregressive discretization; no action chunks/high-frequency control)</strong> and <strong>Octo (93M; diffusion)</strong>, trained on the same mixture.</li>
<li class="">Include a <strong>compute-parity</strong> π0 (160k steps vs 700k) and a <strong>π0-small</strong> variant (no VLM init).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=key-results>Key Results<a href=#key-results class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Out-of-box</strong>: π0 outperforms all baselines; even compute-parity π0 beats OpenVLA/Octo; π0-small still surpasses them—highlighting the benefits of <strong>expressive architectures + diffusion/flow matching + VLM pre-training</strong>.</li>
<li class=""><strong>Language following</strong>: π0 clearly exceeds π0-small across conditions:<!-- -->
<ul>
<li class=""><strong>π0-flat</strong>: only overall task command.</li>
<li class=""><strong>π0-human</strong>: human-provided intermediate steps.</li>
<li class=""><strong>π0-HL</strong>: high-level VLM-provided steps (fully autonomous).</li>
<li class="">Better language-following accuracy <strong>directly translates</strong> into stronger autonomous performance with high-level guidance.</li>
</ul>
</li>
<li class=""><strong>New dexterous tasks</strong> (e.g., bowls stacking, towel folding, microwave, drawer items, paper towel replacement):<!-- -->
<ul>
<li class="">Fine-tuned π0 generally outperforms <strong>OpenVLA</strong>, <strong>Octo</strong>, and small-data methods <strong>ACT</strong> / <strong>Diffusion Policy</strong>.</li>
<li class="">Pre-training helps most when tasks resemble pre-training data; pretrained π0 often beats from-scratch by up to <strong>2×</strong>.</li>
</ul>
</li>
<li class=""><strong>Complex multi-stage tasks</strong> (laundry folding, table bussing, box building, to-go box, eggs):<!-- -->
<ul>
<li class="">π0 solves many tasks; <strong>full pre-training + fine-tuning</strong> performs best.</li>
<li class="">Gains from pre-training are <strong>especially large</strong> on harder tasks; absolute performance varies with task difficulty and pre-training coverage.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=takeaways--limitations>Takeaways & Limitations<a href=#takeaways--limitations class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">π0 mirrors LLM training: <strong>pre-train for knowledge</strong>, <strong>post-train for alignment</strong> (instruction-following and execution).</li>
<li class="">Limitations/open questions:<!-- -->
<ul>
<li class="">Optimal <strong>composition/weighting</strong> of pre-training data remains unclear.</li>
<li class="">Not all tasks work reliably; difficult to predict <strong>how much/what kind</strong> of data is needed for near-perfect performance.</li>
<li class="">Uncertain <strong>positive transfer</strong> across very diverse tasks/robots and to distinct domains (e.g., driving, navigation, legged locomotion).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B., Jakubczak, S., Jones, T., Ke, L., Levine, S., Li‑Bell, A., Mothukuri, M., Nair, S., Pertsch, K., Shi, L. X, … Zhilinsky, U. (2025, June 21). π₀: A vision‑language‑action flow model for general robot control Robotics: Science and Systems (RSS), Los Angeles, CA, United States. <code>https://roboticsconference.org/program/papers/10/</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_f1Hy><a href=/en/2025/08/31/vima-review/>Vima Review</a></h2><div class="container_mt6G margin-vert--md"><time datetime=2025-08-31T10:20:03.726Z>August 31, 2025</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=vima>VIMA<a href=#vima class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Unified Multimodal Prompts</strong>: Reformulates diverse robot tasks (language, images, video) into a single sequence modeling problem.</li>
<li class=""><strong>Object-Centric Tokenization</strong>: Uses object-level tokens (Mask R-CNN + ViT) instead of raw pixels, improving data efficiency and semantic generalization.</li>
<li class=""><strong>Cross-Attention Conditioning</strong>: Conditions the policy on prompts via cross-attention, maintaining strong zero-shot performance even with small models or novel tasks.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=motivation>Motivation<a href=#motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Robot task specification comes in many forms: one-shot demonstrations, language instructions, and visual goals.</li>
<li class="">Traditionally, each task required distinct architectures and pipelines, leading to siloed systems with poor generalization.</li>
</ul>
<p><img decoding=async loading=lazy alt="VIMA Architecture" src=/en/assets/images/vima-architecture-f4e3269ed85bc5b1c954f82ab1ef77cd.png width=1424 height=988 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=key-contributions>Key Contributions<a href=#key-contributions class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ol>
<li class="">
<p><strong>Multimodal Prompting</strong></p>
<ul>
<li class="">A novel formulation that unifies diverse robot manipulation tasks into a <strong>sequence modeling problem</strong>.</li>
<li class="">Prompts are defined as interleaved sequences of text and images, enabling flexibility across task formats.</li>
</ul>
</li>
<li class="">
<p><strong>VIMA-BENCH</strong></p>
<ul>
<li class="">A large-scale benchmark with <strong>17 tasks</strong> across six categories (object manipulation, goal reaching, novel concept grounding, video imitation, constraint satisfaction, visual reasoning).</li>
<li class="">Provides <strong>650K expert trajectories</strong> and a <strong>four-level evaluation protocol</strong> for systematic generalization.</li>
</ul>
</li>
<li class="">
<p><strong>VIMA Agent</strong></p>
<ul>
<li class="">A transformer-based visuomotor agent with <strong>encoder-decoder architecture</strong> and <strong>object-centric design</strong>.</li>
<li class="">Encodes prompts with a pre-trained <strong>T5 model</strong>, parses images into object tokens via <strong>Mask R-CNN + ViT</strong>, and decodes actions autoregressively using <strong>cross-attention</strong>.</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=design-insights>Design Insights<a href=#design-insights class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Object-Centric Representation</strong>: Passing variable-length object token sequences directly to the controller is more effective than pixel-based tokenization.</li>
<li class=""><strong>Cross-Attention Conditioning</strong>: Stronger prompt focus and efficiency compared to simple concatenation (e.g., GPT-style).</li>
<li class=""><strong>Robustness</strong>: Minimal degradation under distractors or corrupted prompts, aided by T5 backbone and object augmentation.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=results>Results<a href=#results class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">
<p><strong>Performance</strong>:</p>
<ul>
<li class="">Outperforms baselines (VIMA-Gato, VIMA-Flamingo, VIMA-GPT) by up to <strong>2.9× success rate</strong> in hardest zero-shot generalization.</li>
<li class="">With <strong>10× less training data</strong>, still <strong>2.7× better</strong> than best competitor.</li>
</ul>
</li>
<li class="">
<p><strong>Scaling</strong>:</p>
<ul>
<li class="">Sample-efficient: with just <strong>1% of data</strong>, matches baselines trained with 10× more.</li>
<li class="">Generalization holds across L1–L4 evaluation, with smaller regression than alternatives.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<p>VIMA demonstrates that multimodal prompting is a powerful unifying framework for robot learning.<br/>
<!-- -->It achieves strong scalability, data efficiency, and generalization, establishing a <strong>solid starting point for future generalist robot agents</strong>.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A., Zhu, Y., & Fan, L. (2023). VIMA: Robot Manipulation with Multimodal Prompts Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v202/jiang23b.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_f1Hy><a href=/en/2025/08/31/roboflamingo-review/>RoboFlamingo Review</a></h2><div class="container_mt6G margin-vert--md"><time datetime=2025-08-31T05:35:06.415Z>August 31, 2025</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=roboflamingo>RoboFlamingo<a href=#roboflamingo class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">RoboFlamingo <strong>decouples vision-language understanding and control</strong>, using OpenFlamingo for perception and a lightweight policy head for sequential decision-making.</li>
<li class="">Unlike prior VLM-based approaches, it requires only <strong>small-scale imitation fine-tuning</strong> on language-conditioned manipulation data, without large-scale co-fine-tuning.</li>
<li class="">This design enables <strong>data-efficient, zero-shot generalizable, and deployable</strong> robot manipulation policies on modest compute resources.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=key-idea>Key Idea<a href=#key-idea class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Proposes <strong>RoboFlamingo</strong>, a simple framework to adapt existing VLMs for robotic manipulation with lightweight fine-tuning.</li>
<li class="">Built on <strong>OpenFlamingo</strong>, decoupling <strong>vision-language understanding</strong> from <strong>decision-making</strong>.</li>
<li class="">Pre-trained VLM handles <strong>language and visual comprehension</strong>, while a dedicated <strong>policy head models sequential history</strong>.</li>
<li class="">Fine-tuned only on <strong>language-conditioned manipulation datasets</strong> using imitation learning.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=advantages>Advantages<a href=#advantages class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Requires only a <strong>small amount of demonstrations</strong> to adapt to downstream manipulation tasks.</li>
<li class="">Provides <strong>open-loop control</strong> capability → deployable on low-performance platforms.</li>
<li class="">Can be trained/evaluated on a <strong>single GPU server</strong>, making it a cost-effective and accessible solution.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=benchmarks>Benchmarks<a href=#benchmarks class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Evaluated on <strong>CALVIN benchmark</strong> (34 tasks, 1000 instruction chains).</li>
<li class="">RoboFlamingo achieves <strong>2× performance improvements</strong> over previous state-of-the-art methods.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=performance>Performance<a href=#performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Imitation Learning</strong>: Outperforms all baselines across all metrics.</li>
<li class=""><strong>Zero-shot Generalization</strong>:<!-- -->
<ul>
<li class=""><strong>Vision</strong>: Stronger generalization in ABC→D setting.</li>
<li class=""><strong>Language</strong>: Robust to GPT-4 generated synonymous instructions.</li>
</ul>
</li>
<li class=""><strong>Ablation Studies</strong>:<!-- -->
<ul>
<li class="">Ignoring history (MLP w/o hist) gives worst results.</li>
<li class="">LSTM and GPT-based policy heads perform best (LSTM chosen as default).</li>
<li class=""><strong>VL pre-training</strong> is crucial for downstream manipulation.</li>
<li class=""><strong>Larger VLMs</strong> show better data efficiency.</li>
<li class=""><strong>Instruction fine-tuning</strong> improves both seen and unseen tasks.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=flexibility-of-deployment>Flexibility of Deployment<a href=#flexibility-of-deployment class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Supports <strong>open-loop control</strong> by predicting entire action sequences with a single inference → reduces latency and test-time compute.</li>
<li class="">Direct open-loop use without retraining can degrade performance; mitigated with <strong>jump-step demonstrations</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Demonstrates that pre-trained VLMs enable <strong>data efficiency</strong> and strong <strong>zero-shot generalization</strong> in robotic manipulation.</li>
<li class="">RoboFlamingo is presented as an <strong>intuitive, efficient, and open solution</strong>, with high potential when combined with large-scale real robot data.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Li, X., Liu, M., Zhang, H., Yu, C., Xu, J., Wu, H., Cheang, C., Jing, Y., Zhang, W., & Liu, H. (2024). Vision-language foundation models as effective robot imitators. International Conference on Learning Representations (ICLR 2024), Vienna, Austria.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_f1Hy><a href=/en/2025/08/29/open-vla-review/>OpenVLA Review</a></h2><div class="container_mt6G margin-vert--md"><time datetime=2025-08-29T07:43:25.796Z>August 29, 2025</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=openvla>OpenVLA<a href=#openvla class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">OpenVLA is a 7B open-source VLA model built on Llama2 + DINOv2 + SigLIP, trained on 970k demos, achieving stronger generalization and robustness than closed RT-2-X (55B) and outperforming Diffusion Policy.</li>
<li class="">It introduces efficient adaptation via LoRA (1.4% params, 8× compute reduction) and 4-bit quantization (half memory, same accuracy), enabling fine-tuning and inference on consumer GPUs.</li>
<li class="">Limitations remain (single-image input, <code>&lt;90%</code> reliability, limited throughput), but OpenVLA provides the first open, scalable framework for generalist robot policies.</li>
</ul>
<p><img decoding=async loading=lazy alt="OpenVLA Architecture" src=/en/assets/images/open-vla-architecture-df4421cc82c1ebceca0ccf1061cd4593.png width=915 height=366 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=motivation>Motivation<a href=#motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Training robot policies from scratch struggles with robustness and generalization.</li>
<li class="">Fine-tuning <strong>vision-language-action (VLA)</strong> models offers reusable, generalizable visuomotor policies.</li>
<li class="">Barriers: prior VLAs are <strong>closed-source</strong>, lack best practices for adaptation, and need server-class hardware.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=model--training>Model & Training<a href=#model--training class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>OpenVLA</strong>: 7B parameters, open-source.</li>
<li class="">Built on <strong>Llama 2</strong> with fused <strong>DINOv2 + SigLIP</strong> vision encoders.</li>
<li class="">Trained on <strong>970k robot demonstrations</strong> from Open-X Embodiment dataset.</li>
<li class="">Represents robot actions as <strong>tokens</strong> (discretized into 256 bins, replacing unused Llama tokens).</li>
<li class="">Standard <strong>next-token prediction</strong> objective.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=architecture--approach>Architecture & Approach<a href=#architecture--approach class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">End-to-end fine-tuning of VLM to generate robot actions as tokens.</li>
<li class="">Differs from modular methods (e.g., Octo) that stitch separate encoders/decoders.</li>
<li class="">Vision features are obtained by encoding the same input image with both SigLIP and DINOv2, then channel-wise concatenated and passed through an MLP projector. This preserves SigLIP’s semantic alignment with language and DINOv2's spatial reasoning, giving the VLM richer multimodal context for manipulation tasks.</li>
<li class="">Uses Prismatic VLM backbone with multi-resolution features (spatial reasoning + semantics).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=performance>Performance<a href=#performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Outperforms closed <strong>RT-2-X (55B)</strong> by <strong>+16.5% task success</strong> with 7× fewer parameters.</li>
<li class="">Beats <strong>Diffusion Policy</strong> (from-scratch imitation learning) by <strong>+20.4%</strong> on multi-task language-grounded settings.</li>
<li class="">Demonstrates <strong>robust behaviors</strong> (distractor resistance, error recovery).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=efficiency>Efficiency<a href=#efficiency class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Introduces <strong>parameter-efficient fine-tuning</strong>:<!-- -->
<ul>
<li class=""><strong>LoRA</strong> updates only 1.4% of parameters yet matches full fine-tuning.</li>
<li class="">Can fine-tune on a single A100 GPU in ~10–15 hours (8× compute reduction).</li>
</ul>
</li>
<li class=""><strong>Quantization</strong>:<!-- -->
<ul>
<li class="">4-bit inference matches bfloat16 accuracy while halving memory footprint.</li>
<li class="">Runs at 3Hz on consumer GPUs (e.g., A5000, 16GB).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=evaluations>Evaluations<a href=#evaluations class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Tested across <strong>29 tasks</strong> and multiple robots (WidowX, Google robot, Franka).</li>
<li class="">Strong generalization on:<!-- -->
<ul>
<li class=""><strong>Visual</strong> (unseen backgrounds/distractors).</li>
<li class=""><strong>Motion</strong> (new object positions/orientations).</li>
<li class=""><strong>Physical</strong> (new object shapes/sizes).</li>
<li class=""><strong>Semantic</strong> (unseen tasks, instructions).</li>
</ul>
</li>
<li class="">First generalist open-source VLA achieving <strong>≥50% success rate across all tested tasks</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=design-insights>Design Insights<a href=#design-insights class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Fine-tuning the vision encoder</strong> (vs. freezing) crucial for robotic control.</li>
<li class="">Higher image resolution (384px vs. 224px) adds 3× compute without performance gains.</li>
<li class="">Training required <strong>27 epochs</strong>, far more than typical VLM runs, to surpass 95% action token accuracy.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=limitations--future-work>Limitations & Future Work<a href=#limitations--future-work class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Supports only <strong>single-image observations</strong> (no proprioception, no history).</li>
<li class="">Inference throughput (~6Hz on RTX 4090) insufficient for high-frequency control (e.g., ALOHA at 50Hz).</li>
<li class="">Success rates remain below 90% in challenging tasks.</li>
<li class="">Open questions:<!-- -->
<ul>
<li class="">Impact of base VLM size on performance.</li>
<li class="">Benefits of co-training with Internet-scale data.</li>
<li class="">Best visual features for VLAs.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=contributions>Contributions<a href=#contributions class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ol>
<li class="">First <strong>open-source generalist VLA</strong> with strong performance.</li>
<li class="">Scalable <strong>end-to-end training</strong> pipeline (action-as-token).</li>
<li class="">Demonstrates <strong>LoRA + quantization</strong> for consumer-grade GPU adaptation.</li>
<li class="">Provides <strong>code, checkpoints, and data curation recipes</strong> to support future research.</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E. P., Sanketi, P. R., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake, R., Sadigh, D., Levine, S., Liang, P., & Finn, C. (2025). OpenVLA: An Open-Source Vision-Language-Action Model Proceedings of The 8th Conference on Robot Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v270/kim25c.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_f1Hy><a href=/en/2025/08/27/octo-review/>Octo Review</a></h2><div class="container_mt6G margin-vert--md"><time datetime=2025-08-27T10:54:09.447Z>August 27, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=octo>Octo<a href=#octo class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Octo is a transformer-based policy with modular tokenizers (language via T5, images via CNN patches), blockwise masking, and readout tokens, trained on 800k multi-robot trajectories.</li>
<li class="">Actions are generated through a diffusion head that produces continuous, multimodal, chunked predictions, enabling precise control and broad generalization.</li>
<li class="">It achieves state-of-the-art zero-shot performance across 7 robots and allows efficient finetuning to new sensors and action spaces, while being fully open-source.</li>
</ul>
<table><thead><tr><th>Category<th>Simple Analogy<th>Actual Tokenization<tbody><tr><td><strong>Language</strong><td><code>[Sentence]</code><td><code>[l₁, l₂, l₃, …]</code> <br/>→ multiple tokens from a tokenized sentence<tr><td><strong>Goal Image</strong><td><code>[Goal]</code><td><code>[g₁, g₂, g₃, …]</code> <br/>→ image split into patches<tr><td><strong>Observation (time t)</strong><td><code>[Observation]</code><td><code>[oₜ¹, oₜ², oₜ³, …]</code> <br/>→ camera frames/sensors tokenized into patches<tr><td><strong>Readout Token</strong><td><code>[ ]</code> (empty slot)<td><code>[TR,t]</code> <br/>→ one per timestep, reserved for predicting actions</table>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token plain">Time t-1: </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">l</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">g</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">o_</span><span class="token punctuation" style=color:#393A34>{</span><span class="token plain">t-1</span><span class="token punctuation" style=color:#393A34>}</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t-1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Time t:   </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">l</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">g</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">o_t</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">     </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain">Time t+1: </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">l</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">g</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">o_</span><span class="token punctuation" style=color:#393A34>{</span><span class="token plain">t+1</span><span class="token punctuation" style=color:#393A34>}</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t+1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br/></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t-1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">, </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">, </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t+1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">  ──►  Diffusion </span><span class="token function" style=color:#d73a49>head</span><span class="token plain">  ──►  </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">a_t, a_</span><span class="token punctuation" style=color:#393A34>{</span><span class="token plain">t+1</span><span class="token punctuation" style=color:#393A34>}</span><span class="token plain">, …</span><span class="token punctuation" style=color:#393A34>]</span><br/></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=motivation>Motivation<a href=#motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Traditional robot learning trains policies <strong>from scratch</strong> on robot/task-specific datasets → costly data collection, narrow generalization.</li>
<li class=""><strong>Generalist Robot Policies (GRPs)</strong> pretrained on diverse robots/tasks can be <strong>finetuned with little in-domain data</strong> while generalizing broadly.</li>
<li class="">Real-world deployments face challenges across <strong>robot embodiments, sensor setups, action spaces, task specs, and environments</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=prior-grps--gaps>Prior GRPs & Gaps<a href=#prior-grps--gaps class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">GRPs aim for <strong>low-level visuomotor control</strong> across tasks, environments, and robotic systems.</li>
<li class="">Existing models often have <strong>restricted inputs (e.g., a single camera)</strong>, <strong>lack efficient finetuning to new domains</strong>, and importantly, <strong>largest models are not publicly available</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=contribution-what-is-octo>Contribution (What is Octo?)<a href=#contribution-what-is-octo class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Octo</strong>: a large transformer-based policy trained on <strong>800k trajectories</strong> from the Open X-Embodiment dataset.</li>
<li class="">Accepts <strong>language instructions or goal images</strong>, and can be <strong>finetuned within hours on consumer GPUs</strong> to new sensors and action spaces.</li>
<li class=""><strong>First GRP</strong> to support <strong>effective finetuning to new observations and actions</strong> and to be <strong>fully open-source</strong> (training pipeline, checkpoints, data).</li>
<li class="">Novelty lies in combining: <strong>transformer backbone + language/goal image conditioning + diffusion head</strong> for expressive action distributions.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=architecture>Architecture<a href=#architecture class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Input tokenizers</strong>:<!-- -->
<ul>
<li class="">Language via pretrained <strong>T5-base</strong></li>
<li class="">Images via shallow CNN → patch tokens</li>
</ul>
</li>
<li class=""><strong>Transformer backbone</strong>: processes unified token sequence.</li>
<li class=""><strong>Blockwise masking + Readout tokens</strong>:<!-- -->
<ul>
<li class="">Nonexistent modalities are masked</li>
<li class="">Readout tokens <em>only attend</em> to past observations/tasks, not vice versa</li>
</ul>
</li>
<li class=""><strong>Diffusion action head</strong>: predicts <strong>continuous, multimodal, chunked actions</strong>.</li>
<li class=""><strong>Modularity</strong>: new sensors/outputs can be added by only training lightweight encoders or heads; pretrained backbone remains unchanged.</li>
</ul>
<p><img decoding=async loading=lazy alt="Octo Architecture" src=/en/assets/images/octo-architecture-49b9dd94643695f0566e74ac5a0801bd.png width=803 height=415 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=training-data--objective>Training Data & Objective<a href=#training-data--objective class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Mixture of <strong>25 heterogeneous robot datasets</strong>: diverse robots, sensors (with/without wrist cams), labels (with/without language).</li>
<li class=""><strong>Conditional diffusion decoding</strong> predicts continuous, multimodal action distributions.<!-- -->
<ul>
<li class="">Transformer runs <strong>one forward pass</strong>; denoising steps are contained in the small diffusion head.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=experiments>Experiments<a href=#experiments class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Evaluated on <strong>7 robotic platforms across 4 institutions</strong>.</li>
<li class="">Key questions:<!-- -->
<ol>
<li class="">Zero-shot multi-robot control?</li>
<li class="">Do Octo weights improve finetuning vs. scratch or standard pretrained representations?</li>
<li class="">Which design choices matter for generalist robot policies?</li>
</ol>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=results>Results<a href=#results class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Achieves <strong>state-of-the-art zero-shot multi-robot control</strong>, competitive with RT-1-X and RT-2-X.</li>
<li class="">Provides a <strong>versatile policy initialization</strong>: significantly outperforms baselines for <strong>data-efficient finetuning</strong> to new obs/action spaces.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=limitations--future-work>Limitations / Future Work<a href=#limitations--future-work class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Needs <strong>better language conditioning</strong>, <strong>improved wrist camera support</strong>, and <strong>data beyond optimal demonstrations</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=one-line-takeaway>One-line Takeaway<a href=#one-line-takeaway class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Octo = modular, efficient, open-source GRP</strong>:<br/>
<!-- -->A transformer + diffusion policy trained on large-scale multi-robot data that <strong>adapts quickly with little in-domain data</strong> to new sensors and action spaces, enabling broad generalization.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Mees, O., Ghosh, D., Pertsch, K., Black, K., Walke, H. R., Dasari, S., Hejna, J., Kreiman, T., Xu, C., & Luo, J. (2024). Octo: An open-source generalist robot policy. First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_f1Hy><a href=/en/2025/08/24/rt-2-review/>RT-2, Robotic Transformer 2 Review</a></h2><div class="container_mt6G margin-vert--md"><time datetime=2025-08-24T13:40:19.433Z>August 24, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div class=markdown><ul>
<li class="">Trains a <strong>Vision-Language-Action (VLA)</strong> model by co-fine-tuning web-scale VLMs with robot trajectories, and <strong>treats robot actions as text tokens</strong>.</li>
<li class="">Yields <strong>strong generalization</strong> and <strong>emergent capabilities</strong> (symbol understanding, reasoning, human recognition) beyond what appears in robot data.</li>
<li class="">Runs in <strong>direct closed-loop control</strong>; largest evaluated model (55B) executes at ~1–3 Hz via a cloud (multi-TPU) inference setup.</li>
</ul>
<p><img decoding=async loading=lazy alt="RT-2 Architecture" src=/en/assets/images/rt-2-architecture-dd9ff6e2cae963c14c20742089b822df.png width=1674 height=748 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=what-rt-2-is>What RT-2 Is<a href=#what-rt-2-is class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">A family of VLA models (RT-2-PaLI-X, RT-2-PaLM-E) that fine-tune large VLMs on robot trajectories to output <strong>low-level actions</strong>.</li>
<li class="">Target: <strong>generalizable, semantically aware</strong> manipulation policies that map images + instructions → actions end-to-end.</li>
<li class="">RT-2 does <strong>not rely on a restricted 2D action space or calibrated cameras</strong>.</li>
<li class="">The <strong>unified output space</strong> lets language and action tokens share the same model weights, without action-only layers.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=core-recipe>Core Recipe<a href=#core-recipe class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Directly train</strong> open-vocabulary VQA/dialogue VLMs to <strong>output robot actions</strong> while they still solve standard vision-language tasks.</li>
<li class="">Build on RT-1 protocol/data, but replace the policy backbone with a <strong>large VLM</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=action-as-language-tokenization>Action as Language (Tokenization)<a href=#action-as-language-tokenization class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Discretize continuous action dims (Δpos/Δrot, gripper, terminate) into <strong>256 bins</strong>; represent each dimension with an <strong>integer token</strong>.</li>
<li class=""><strong>PaLI-X</strong>: reuse numeric tokens (<code>≤1000</code>). <strong>PaLM-E</strong>: overwrite <strong>256 least-frequent tokens</strong> as action vocabulary (<strong>symbol tuning</strong>).</li>
<li class="">Form a single output string per step (e.g., <code>terminate Δposx Δposy Δposz Δrotx Δroty Δrotz gripper</code>).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=co-fine-tuning--output-constraint>Co-Fine-Tuning & Output Constraint<a href=#co-fine-tuning--output-constraint class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Mix robot data with original web VQA/caption data</strong> in training batches (up-weight robot samples) to prevent forgetting and improve generalization.</li>
<li class="">During decoding on robot tasks, <strong>restrict sampling to valid action tokens</strong> so outputs are always executable.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=closed-loop-control--real-time-inference>Closed-Loop Control & Real-Time Inference<a href=#closed-loop-control--real-time-inference class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">RT-2 is trained and deployed for <strong>direct closed-loop control</strong> (camera → action → camera …), not just high-level planning.</li>
<li class="">For large models, inference runs via a <strong>multi-TPU cloud service</strong>; <strong>RT-2-PaLI-X-55B</strong> reaches <strong>~1–3 Hz</strong>; smaller models ~5 Hz.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=generalization--benchmarks>Generalization & Benchmarks<a href=#generalization--benchmarks class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Matches RT-1 on seen tasks but <strong>far exceeds</strong> baselines on <strong>unseen objects/backgrounds/environments</strong> (~<strong>2×</strong> vs RT-1/MOO; up to <strong>~6×</strong> vs others).</li>
<li class="">Open-source <strong>Language-Table</strong> sim: co-fine-tuned <strong>PaLI-3B</strong> outperforms baselines, showing the approach transfers to other robots/sims.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=emergent-capabilities>Emergent Capabilities<a href=#emergent-capabilities class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Symbol understanding</strong> (e.g., “move apple to 3 / heart / star”).</li>
<li class=""><strong>Reasoning</strong> (visual matching, simple math like “sum of two plus one”, <strong>multilingual</strong> commands).</li>
<li class=""><strong>Human recognition</strong> (e.g., “person with glasses”); none of these were present as low-level actions in robot data.</li>
<li class=""><strong>Chain-of-thought (CoT) variant</strong> adds a <strong>Plan</strong> step before actions → supports <strong>multi-stage semantic reasoning</strong> (e.g., pick a rock as an improvised hammer; pick an energy drink for a tired person).</li>
</ul>
<p><img decoding=async loading=lazy alt=rt-2-cot src=/en/assets/images/rt-2-cot-eb2bee68ad29cc277f1a214c22064f0e.png width=1748 height=934 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=scaling--ablations>Scaling & Ablations<a href=#scaling--ablations class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>From-scratch</strong> training (even 5B) performs poorly; <strong>fine-tuning</strong> helps; <strong>co-fine-tuning</strong> helps <strong>most</strong>.</li>
<li class=""><strong>Bigger models</strong> (<code>55B > 5B</code>) generalize better.</li>
<li class="">PaLM-E variant shows an edge on <strong>math reasoning</strong>; PaLI-X stronger on symbols/vision reasoning on average.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=limitations>Limitations<a href=#limitations class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Does <strong>not</strong> learn fundamentally <strong>new motor skills</strong> beyond the distribution in robot data; mainly transfers <strong>semantic/visual knowledge</strong>.</li>
<li class=""><strong>Compute/latency</strong> costly; real-time control can bottleneck. Limited availability of strong open VLMs and convenient FT APIs.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=future-directions-from-the-text>Future Directions (from the text)<a href=#future-directions-from-the-text class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Acquire new skills from <strong>human videos</strong> or richer datasets.</li>
<li class=""><strong>Quantization/distillation</strong> for faster/cheaper inference.</li>
<li class="">More <strong>open VLMs / FT APIs</strong> to make VLA models broadly buildable.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran, H., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K., Mordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T.-W. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J., Irpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess, D., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., & Han, K. (2023). RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control Proceedings of The 7th Conference on Robot Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v229/zitkovich23a.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_f1Hy><a href=/en/2025/08/24/palm-e-review/>PaLM-E An Embodied Multimodal Language Model Review</a></h2><div class="container_mt6G margin-vert--md"><time datetime=2025-08-24T10:33:27.131Z>August 24, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight /></a><div class="avatar__intro authorDetails_lV9A"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_yefp translate=no>Gracefullight</span></a></div><small class=authorTitle_nd0D title=Owner>Owner</small><div class=authorSocials_rSDt></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=palm-e>PaLM-E<a href=#palm-e class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">ViT (e.g., ViT-4B, ViT-22B) extracts image embeddings.</li>
<li class="">OSRT builds object-centric slot representations.</li>
<li class="">These are injected into the LLM embedding space (PaLM variants: 8B, 62B, 540B) for high-level abstraction and planning, with execution delegated to low-level policies (e.g., RT-1).</li>
</ul>
<p><img decoding=async loading=lazy alt="PaLM-E Architecture" src=/en/assets/images/palm-e-architecture-ca3f220adc8e114bc89a0589703866fb.png width=1284 height=650 class=img_ev3q /></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=core-idea>Core idea<a href=#core-idea class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Build <strong>embodied language models</strong> by <strong>injecting continuous sensor inputs</strong> (images, states, other modalities) directly into a <strong>pretrained LLM’s embedding space</strong>, linking <strong>words ↔ percepts</strong>.</li>
<li class="">Inputs are <strong>multimodal sentences</strong> that <strong>interleave</strong> text tokens with encoded visual/state tokens; outputs are <strong>text</strong> (answers or high-level plans).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=architecture--representations>Architecture & representations<a href=#architecture--representations class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Start from a <strong>decoder-only, autoregressive LLM</strong> (PaLM) and <strong>condition on a prefix</strong> that mixes text and <strong>encoder-produced vectors</strong>.</li>
<li class="">Provide multiple encoder options:<!-- -->
<ul>
<li class=""><strong>State vectors</strong> (simplest).</li>
<li class=""><strong>ViT</strong> features with a learned <strong>projector ψ</strong> to match LLM embedding dimensionality.</li>
<li class=""><strong>Object-centric, 3D-aware OSRT</strong> (neural scene representations). Supports <strong>entity-label tokens</strong> (<code>&lt;obj j></code>) so the model can refer to specific objects in generated plans.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=training-setup>Training setup<a href=#training-setup class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Train <strong>end-to-end</strong> (encoders + projector + optionally the LLM) to output <strong>sequential decisions as natural text</strong> or answers (VQA, captioning).</li>
<li class="">Dataset items contain <strong>(continuous observations, text sequence, prefix index)</strong>; loss is <strong>cross-entropy on non-prefix tokens</strong>.</li>
<li class="">Explore <strong>freezing the LLM</strong> (train encoders/projection only), and <strong>co-training</strong> across diverse tasks ("full mixture"; only ~9% is embodied data).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=planning--control-loop>Planning & control loop<a href=#planning--control-loop class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">For <strong>planning/control</strong>, PaLM-E emits <strong>textual subgoals/skills</strong> drawn from a small skill vocabulary; a separate <strong>low-level policy</strong> executes them.</li>
<li class="">The system runs <strong>closed-loop</strong>: execute → observe → (re)plan; PaLM-E acts as a <strong>high-level policy</strong> sequencing low-level skills.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=why-not-text-only-llms-or-affordance-only-grounding>Why not text-only LLMs or affordance-only grounding?<a href=#why-not-text-only-llms-or-affordance-only-grounding class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Prior work that feeds <strong>only text to the LLM</strong> (and uses external affordance models) is <strong>insufficient</strong> when <strong>spatial layout</strong> matters.</li>
<li class="">PaLM-E instead <strong>grounds inside the LLM</strong> by <strong>injecting continuous observations</strong>, enabling <strong>direct plan generation</strong> while leveraging the LLM’s world knowledge.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=environments--use-cases>Environments & use cases<a href=#environments--use-cases class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Three domains</strong>: <strong>TAMP</strong> (grasp/stack planning), <strong>Language-Table</strong> (multi-object tabletop pushing), <strong>Mobile manipulation</strong> (kitchen tasks).</li>
<li class=""><strong>Use cases</strong> to test embodied reasoning: <strong>affordance prediction</strong>, <strong>failure detection</strong>, <strong>long-horizon planning</strong> (low-level policies from <strong>RT-1</strong>).</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=results-high-level>Results (high level)<a href=#results-high-level class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class=""><strong>Transfer via co-training</strong>: One model trained on mixed tasks/embodiments achieves <strong>higher performance</strong> than task-specialists; "full mixture" yields <code>>2×</code> gains (Fig. 3).</li>
<li class=""><strong>Few-shot/data efficiency</strong>: Solves robotics tasks with <strong>very few examples</strong> (e.g., <strong>10–80</strong> for Language-Table, <strong>320</strong> for TAMP). <strong>OSRT</strong> further improves data efficiency.</li>
<li class=""><strong>Mobile manipulation</strong>: End-to-end embodied planning works in real kitchens, robust to disturbances; PaLM-E beats <strong>PaLI (zero-shot)</strong> and <strong>QT-OPT/CLIP baselines</strong> on affordance/failure detection.</li>
<li class=""><strong>General V+L</strong>: The <strong>562B</strong> generalist achieves <strong>state-of-the-art on OK-VQA</strong> and strong VQAv2/COCO without task-specific finetuning.</li>
<li class=""><strong>Language retention & scaling</strong>: <strong>Freezing LLM</strong> preserves language ability but can struggle on some robotics tasks; <strong>unfrozen + scale up</strong> significantly reduces <strong>catastrophic forgetting</strong>.</li>
<li class=""><strong>Emergent behaviors</strong>: <strong>Multimodal chain-of-thought</strong> and <strong>multi-image reasoning</strong> emerge in <strong>PaLM-E-562B</strong>, despite training on <strong>single-image prompts</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=takeaways>Takeaways<a href=#takeaways class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Injecting <strong>neural scene representations (OSRT)</strong> and <strong>entity-labeled multimodal tokens</strong> is <strong>effective</strong> even without massive embodied data.</li>
<li class=""><strong>Diverse, joint training</strong> transfers vision-language knowledge <strong>into embodied decision-making</strong>, enabling <strong>data-efficient robot planning</strong>.</li>
<li class="">Two viable paths to retain language skills during multimodal finetuning:<!-- -->
<ol>
<li class=""><strong>Freeze the LLM</strong>, train encoders (max language retention, sometimes weaker robotics),</li>
<li class=""><strong>Unfreeze and scale</strong> the LLM (much less forgetting, strong embodied performance).</li>
</ol>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading" translate=no>​</a></h2>
<ul>
<li class="">Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., & Florence, P. (2023). PaLM-E: An Embodied Multimodal Language Model Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v202/driess23a.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class=tag_QGVx><a rel=tag title="Vision-Language Models" class="tag_zVej tagRegular_sFm0" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><nav class=pagination-nav aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--next" href=/en/tags/vlm/page/2/><div class=pagination-nav__label>Older Entries</div></a></nav></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Support Me</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://www.buymeacoffee.com/LOUB2kN target=_blank rel="noopener noreferrer" style="cursor: pointer;">
                <img src=https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png style="height: auto !important;width: auto !important;">
              </a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Feeds</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://gracefullight.dev/rss.xml target=_blank rel="noopener noreferrer" class=footer__link-item>RSS<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a><li class=footer__item><a href=https://gracefullight.dev/atom.xml target=_blank rel="noopener noreferrer" class=footer__link-item>Atom<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_nPIU><use href=#theme-svg-external-link /></svg></a></ul></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2016 - 2023 Gracefullight. Built with Docusaurus.</div></div></div></footer></div></body>