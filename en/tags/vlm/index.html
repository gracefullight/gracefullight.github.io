<!doctype html><html lang=en dir=ltr class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>15 posts tagged with "vlm" | gracefullight.dev</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://gracefullight.dev/en/tags/vlm/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=ko><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true content=gcY9SiftHgQoJjBZ7IgwNNN5_atLPAX6kWb1nFVfa6E name=google-site-verification><meta data-rh=true content=65AD1E28C0D057CEB3C68FBC0293E55B name=msvalidate.01><meta data-rh=true content=d024c2837887f72dc7b3792b958be74d69ba9593 name=naver-site-verification><meta data-rh=true content=f7c93483a6f87c79 name=yandex-verification><meta data-rh=true content=yZEdU1ABcR name=baidu-site-verification><meta data-rh=true content=uelupjqqsm5egzlhy1aev2rfxow5yt name=facebook-domain-verification><meta data-rh=true property=og:title content='15 posts tagged with "vlm" | gracefullight.dev'><meta data-rh=true name=description content="Vision-Language Models"><meta data-rh=true property=og:description content="Vision-Language Models"><meta data-rh=true name=docusaurus_tag content=blog_tags_posts><meta data-rh=true name=docsearch:docusaurus_tag content=blog_tags_posts><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://gracefullight.dev/en/tags/vlm/><link data-rh=true rel=alternate href=https://gracefullight.dev/tags/vlm/ hreflang=ko><link data-rh=true rel=alternate href=https://gracefullight.dev/en/tags/vlm/ hreflang=en><link data-rh=true rel=alternate href=https://gracefullight.dev/tags/vlm/ hreflang=x-default><link data-rh=true rel=preconnect href=https://RFS69RSYOJ-dsn.algolia.net crossorigin=anonymous><link rel=alternate type=application/rss+xml href=/en/rss.xml title="gracefullight.dev RSS Feed"><link rel=alternate type=application/atom+xml href=/en/atom.xml title="gracefullight.dev Atom Feed"><link rel=alternate type=application/json href=/en/feed.json title="gracefullight.dev JSON Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-E99DNE7S05"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-E99DNE7S05",{})</script><link rel=search type=application/opensearchdescription+xml title=gracefullight.dev href=/en/opensearch.xml><link href=/en/img/favicon-32x32.png rel=icon><link href=/en/manifest.json rel=manifest><meta content=#f28913 name=theme-color><meta content=yes name=mobile-web-app-capable><meta content=#f28913 name=apple-mobile-web-app-status-bar-style><link href=/en/img/apple-touch-icon.png rel=apple-touch-icon><link rel=preconnect href=https://pagead2.googlesyndication.com><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3004788392777865" async crossorigin=anonymous></script><link rel=preconnect href=https://www.clarity.ms><script>!function(t,e,n,a,c,i,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(i=e.createElement(a)).async=1,i.src="https://www.clarity.ms/tag/"+c,(r=e.getElementsByTagName(a)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script","aongv9xgi6")</script><link rel=preconnect href=https://wcs.naver.net><script src=https://wcs.naver.net/wcslog.js async></script><script>if(!wcs_add)var wcs_add={};wcs_add.wa="156bc73a81e3bd0",window.wcs&&wcs_do()</script><link rel=preconnect href=https://cdn.channel.io><script>!function(){var n=window;if(n.ChannelIO)return(window.console.error||window.console.log||function(){})("ChannelIO script included twice.");var e=function(){e.c(arguments)};function t(){if(!n.ChannelIOInitialized){n.ChannelIOInitialized=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cdn.channel.io/plugin/ch-plugin-web.js",e.charset="UTF-8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}}e.q=[],e.c=function(n){e.q.push(n)},n.ChannelIO=e,"complete"===document.readyState?t():window.attachEvent?window.attachEvent("onload",t):(window.addEventListener("DOMContentLoaded",t,!1),window.addEventListener("load",t,!1))}(),ChannelIO("boot",{pluginKey:"0fd130ba-a1a6-4b7e-802a-e82a885a7fd8"})</script><link rel=preconnect href=https://static.cloudflareinsights.com><script src=https://static.cloudflareinsights.com/beacon.min.js defer data-cf-beacon='{"token":"c0899829e72b45e98dff77241127252c"}'></script><link href=https://mc.yandex.ru rel=preconnect><script>!function(e,t,c,n,r,a,s){e[r]=e[r]||function(){(e[r].a=e[r].a||[]).push(arguments)},e[r].l=+new Date;for(var i=0;i<document.scripts.length;i++)if(document.scripts[i].src===n)return;a=t.createElement(c),s=t.getElementsByTagName(c)[0],a.async=1,a.src=n,s.parentNode.insertBefore(a,s)}(window,document,"script","https://mc.yandex.ru/metrika/tag.js?id=104072655","ym"),ym(0x63405cf,"init",{ssr:!0,clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!1})</script><link href=https://cdn.jsdelivr.net rel=preconnect><script type=application/ld+json>{"@context":"http://schema.org","@type":"Person","email":"mailto:gracefullight.dev@gmail.com","image":"https://avatars.githubusercontent.com/u/11773683?v=4","jobTitle":"FullStack JavaScript Developer","logo":"https://gracefullight.dev/img/apple-touch-icon.png","name":"Eunkwang Shin","nationality":"Korean","sameAs":["https://github.com/gracefullight","https://linkedin.com/in/gracefullight"],"url":"https://gracefullight.dev"}</script><link rel=stylesheet crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css type=text/css><script src=/en/assets/js/runtime~main.8cc63bf8.js defer></script><script src=/en/assets/js/main.7094eed3.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/favicon-32x32.png><link rel=preload as=image href="https://avatars.githubusercontent.com/u/11773683?v=4"><div role=region aria-label="Skip to main content"><a class=skipToContent_soTP href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_bFbq themedComponent--light_CA8X"><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent_bFbq themedComponent--dark_BzJK"></div><b class="navbar__title text--truncate">gracefullight.dev</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_LGSY><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/tags/vlm/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ko>한국어</a><li><a href=/en/tags/vlm/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a></ul></div><a class="navbar__item navbar__link" href=/en/archive/>Archives</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/tags/>Tags</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_YAYV colorModeToggle_T40I"><button class="clean-btn toggleButton_Qe_r toggleButtonDisabled_AVvq" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_T_L2 lightToggleIcon_FRTA"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_T_L2 darkToggleIcon_zlMh"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_T_L2 systemToggleIcon_h91M"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_SXBB><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_bGoB"><div class="container margin-vert--lg"><div class=row><aside class="col col--3"><nav class="sidebar_Pgmn thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_euex margin-bottom--md">Recent posts</div><div role=group><h3 class=yearGroupHeading_cosT>2025</h3><ul class="sidebarItemList_jela clean-list"><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/vocab/vocab-ai-008/>Vocabulary for AI 008</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/09/12/introduction-to-ai-006/>Introduction to AI @006</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/09/10/research-methodology/>Research methodology</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/09/09/fundamentals-of-data-analytics-006/>Fundamentals of data analytics @006</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/09/09/fundamentals-of-data-analytics-005/>Fundamentals of data analytics @005</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/vocab/vocab-ai-007/>Vocabulary for AI @007</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/09/08/fundamentals-of-software-development-007/>Fundamentals of software development @007</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/09/04/sentence-structures/>Sentence structures</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/09/04/vlatest-review/>VLA Test Review</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/09/02/introduction-to-ai-005/>Introduction to AI @005</a></ul></div></nav></aside><main class="col col--7"><header class=margin-bottom--xl><h1>15 posts tagged with "vlm"</h1><p>Vision-Language Models</p><a href=/en/tags/>View All Tags</a></header><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/09/04/vlatest-review/>VLA Test Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-09-03T14:38:22.388Z>September 3, 2025</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=vlatest-testing-and-evaluating-vision-language-action--models-for-robotic-manipulation>VLATest: Testing and Evaluating Vision-Language-Action  Models for Robotic Manipulation<a href=#vlatest-testing-and-evaluating-vision-language-action--models-for-robotic-manipulation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>VLATest fuzzes 18,604 manipulation scenes (10 operators, 4 tasks) to systematically stress-test VLA robustness.</li>
<li>Seven VLA models show low success and brittleness to confounders, lighting/camera changes, unseen objects, and instruction mutations; larger pretraining helps.</li>
<li>Priorities: scale/augment demo data (incl. sim2real), use stepwise/CoT prompting & multi-agent setups, and expand benchmarks with online risk assessment.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=motivation--gap>Motivation & Gap<a href=#motivation--gap class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Problem:</strong> Current VLA models are typically evaluated on <strong>small, hand-crafted scenes</strong>, leaving <strong>general performance and robustness</strong> in diverse scenarios underexplored.</li>
<li><strong>Goal:</strong> Introduce <strong>VLATest</strong>, a <strong>generation-based fuzzing framework</strong> that automatically creates robotic manipulation scenes to <strong>test performance and robustness</strong> of VLA models.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=what-are-vla-models>What Are VLA Models?<a href=#what-are-vla-models class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Vision-Language-Action (VLA)</strong> models take <strong>natural language instructions</strong> + <strong>camera images</strong> and output <strong>low-level robot actions</strong> (Δx, Δθ, Δgrip).</li>
<li><strong>Inference loop:</strong> Tokenize text/image → transformer predicts action token <strong>A₁</strong> → execute → append <strong>A₁</strong> + new image tokens <strong>I₂</strong> → predict <strong>A₂</strong> → … until success or step limit.</li>
</ul>
<p><img decoding=async loading=lazy alt="VLA Architecture" src=/en/assets/images/vla-architecture-6c70ccb1f0cb43def096bc28e5b0abf9.png width=2022 height=514 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=training--evaluation>Training & Evaluation<a href=#training--evaluation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Training:</strong> (1) Train from scratch on robot demonstrations, or (2) <strong>fine-tune a large VLM</strong> (e.g., Llava) with <code>></code>1B params pretraining.</li>
<li><strong>Evaluation:</strong> Task-specific metrics (e.g., <strong>grasp</strong>, <strong>lift</strong>, <strong>hold</strong> for “pick up”), either in <strong>sim</strong> (auto-metrics) or <strong>real</strong> (manual labels).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=vlatest-framework>VLATest Framework<a href=#vlatest-framework class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Ten testing operators</strong> grouped across:<!-- -->
<ul>
<li><strong>Target objects:</strong> type, position, orientation</li>
<li><strong>Confounding objects:</strong> type, position, orientation, <strong>count</strong></li>
<li><strong>Lighting:</strong> <strong>intensity</strong></li>
<li><strong>Camera:</strong> <strong>position</strong>, <strong>orientation</strong></li>
</ul>
</li>
<li><strong>Scene generation (Alg. 1):</strong> sample valid targets → (optional) confounders → mutate lighting (factor <strong>α</strong>) → mutate camera pose (<strong>d</strong>, <strong>θ</strong>). Semantic validity checks prevent infeasible scenes.</li>
</ul>
<p><img decoding=async loading=lazy alt="VLA Test" src=/en/assets/images/vla-test-7989c6573c1574d8dfc5e67ca632ca0d.png width=1926 height=340 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=research-questions-rq>Research Questions (RQ)<a href=#research-questions-rq class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>RQ1:</strong> Basic performance on popular manipulation tasks</li>
<li><strong>RQ2:</strong> Effect of <strong>confounding object count</strong></li>
<li><strong>RQ3:</strong> Effect of <strong>lighting changes</strong></li>
<li><strong>RQ4:</strong> Effect of <strong>camera pose changes</strong></li>
<li><strong>RQ5:</strong> Robustness to <strong>unseen objects</strong> (OOD)</li>
<li><strong>RQ6:</strong> Robustness to <strong>instruction mutations</strong></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=tasks--prompting>Tasks & Prompting<a href=#tasks--prompting class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Tasks:</strong>
<ol>
<li><strong>Pick up</strong> an object (grasp + lift ≥0.02 m for 5 frames)</li>
<li><strong>Move A near B</strong> (≤0.05 m)</li>
<li><strong>Put A on B</strong> (stable stacking)</li>
<li><strong>Put A into B</strong> (fully inside)</li>
</ol>
</li>
<li><strong>Standard prompts (RQ1–RQ5):</strong>
<ul>
<li><code>pick up [obj]</code> · <code>move [objA] near [objB]</code> · <code>put [objA] on [objB]</code> · <code>put [objA] into [objB]</code></li>
</ul>
</li>
<li><strong>Instruction mutations (RQ6):</strong> 10 paraphrases per task (GPT-4o), manually validated for semantic equivalence.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=experimental-setup>Experimental Setup<a href=#experimental-setup class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Scenes:</strong> <strong>18,604</strong> across 4 tasks (ManiSkill2).</li>
<li><strong>Models:</strong> 7 public VLAs (RT-1-1k/58k/400k, RT-1-X, Octo-small/base, OpenVLA-7b).</li>
<li><strong>Compute:</strong> <code>></code><strong>580 GPU hours</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=key-results--findings>Key Results & Findings<a href=#key-results--findings class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=rq1--overall-performance>RQ1 — Overall Performance<a href=#rq1--overall-performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>VLA models <strong>underperform</strong> overall; no single model dominates across tasks.</li>
<li>Example best-case rates (default settings): <strong>34.4%</strong> (Task1, RT-1-400k), <strong>12.7%</strong> (Task2, OpenVLA-7b), <strong>2.2%</strong> (Task3, RT-1-X), <strong>2.1%</strong> (Task4, Octo-small).</li>
<li><strong>Stepwise breakdown (Task 1):</strong> grasp <strong>23.3%</strong> → lift <strong>15.7%</strong> → hold <strong>12.4%</strong> ⇒ difficulty <strong>composing sequential actions</strong>.<!-- -->
<ul>
<li><strong>Implication (Finding 2):</strong> Consider <strong>stepwise prompting / chain-of-thought</strong> to decompose complex tasks.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=rq1--coverage-metric>RQ1 — Coverage Metric<a href=#rq1--coverage-metric class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>No established coverage for VLA; adopted <strong>trajectory coverage</strong> (pragmatic).</li>
<li>Increasing cases from <strong>n=10</strong> to <strong>n=1000</strong> achieved <strong>100%</strong> coverage across tasks (object-position novelty relative to workspace).</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=rq2--confounding-objects>RQ2 — Confounding Objects<a href=#rq2--confounding-objects class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li><strong>More confounders ⇒ worse performance</strong>; models struggle to <strong>locate the correct object</strong>.</li>
<li><strong>Similarity doesn’t matter much:</strong> Mann–Whitney U shows <strong>no significant difference</strong> between <strong>similar</strong> vs <strong>dissimilar</strong> distractors (p = 0.443, 0.614, 0.657, 0.443; effect sizes ≈ 0.23–0.29).</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=rq3--lighting-robustness>RQ3 — Lighting Robustness<a href=#rq3--lighting-robustness class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li><strong>Lighting perturbations significantly hurt performance.</strong></li>
<li><strong>OpenVLA-7b</strong> most robust (<strong>77.9%</strong> of previously passed cases still pass), plausibly due to <strong>SigLIP + DINOv2</strong> pretraining and LLaVA 1.5 mixture.</li>
<li><strong>Sensitivity:</strong> even <strong>α <code>&lt;</code> 2.5</strong> increase drops success to ~<strong>0.7×</strong>; <strong>α <code>></code> 8</strong> ⇒ ~<strong>40%</strong> of default-pass scenes succeed.</li>
<li><strong>Decreasing</strong> light hurts <strong>less</strong> than increasing; <strong>α <code>&lt;</code> 0.2</strong> still ~<strong>60%</strong> pass.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=rq4--camera-pose-robustness>RQ4 — Camera Pose Robustness<a href=#rq4--camera-pose-robustness class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Small pose changes (≤<strong>5°</strong> rotation, ≤<strong>5 cm</strong> shift) reduce success to <strong>34.0%</strong> of default.</li>
<li><strong>RT-1-400k</strong> most robust (<strong>45.6%</strong> retain), <strong>OpenVLA-7b</strong> at <strong>31.3%</strong>; <strong>Octo</strong> models <code>&lt;</code><strong>10%</strong>.<!-- -->
<ul>
<li>Likely due to <strong>training data scale</strong> differences.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=rq5--unseen-objects>RQ5 — Unseen Objects<a href=#rq5--unseen-objects class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Using <strong>YCB (56 unseen objects)</strong> leads to large performance drops versus seen objects: avg <strong>–74.2%</strong>, <strong>–66.7%</strong>, <strong>–66.7%</strong>, <strong>–20.0%</strong> on Tasks 1–4.</li>
<li><strong>Transfer rate</strong> across steps:<!-- -->
<ul>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mstyle scriptlevel=0 displaystyle=true><msubsup><mi>T</mi><mi>r</mi><mi>n</mi></msubsup><mo>=</mo><mfrac><msub><mtext>Success rate</mtext><mi>n</mi></msub><msub><mtext>Success rate</mtext><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mfrac></mstyle></mrow><annotation encoding=application/x-tex>\displaystyle T_r^n = \frac{\text{Success rate}_n}{\text{Success rate}_{n-1}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9614em;vertical-align:-0.247em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em>T</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.7144em><span style=top:-2.453em;margin-left:-0.1389em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>r</span></span></span><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.2547em;vertical-align:-0.8943em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.3603em><span style=top:-2.314em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord text"><span class=mord>Success rate</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2083em><span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:0.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord text"><span class=mord>Success rate</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.8943em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>, with <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mtext>Success rate</mtext><mn>0</mn></msub><mo>=</mo><mn>100</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>\text{Success rate}_0 = 100\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord text"><span class=mord>Success rate</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em></span><span class=mord>100%</span></span></span></span></li>
<li>Paired t-tests show significant differences on <strong><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msubsup><mi>T</mi><mi>r</mi><mn>1</mn></msubsup></mrow><annotation encoding=application/x-tex>T_r^1</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0611em;vertical-align:-0.247em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em>T</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-2.453em;margin-left:-0.1389em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>r</span></span></span><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span></span></span></span></strong> for <strong>Task 1 & 2</strong> (p = 0.011, 0.007; Cohen’s d = 1.34, 0.891).</li>
<li><strong>Primary failure mode:</strong> <strong>recognizing/locating unseen objects</strong>.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=rq6--instruction-mutations>RQ6 — Instruction Mutations<a href=#rq6--instruction-mutations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Mutated instructions generally <strong>reduce performance</strong> (avg drops: <strong>–32.8%</strong> T1, <strong>–1.7%</strong> T2, <strong>–8.3%</strong> T3; negligible on T4).</li>
<li><strong>Larger language backbones help:</strong> <strong>OpenVLA-7b (Llama 2-7B)</strong> is <strong>more robust</strong>, sometimes <strong>improving</strong> under mutations (e.g., T1, T4).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=implications--directions>Implications & Directions<a href=#implications--directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Scale matters:</strong> larger <strong>pretraining</strong> and <strong>robot-demo datasets</strong> improve robustness (lighting/camera).</li>
<li><strong>Data enrichment:</strong> use <strong>data augmentation</strong> and <strong>sim-to-real</strong> to diversify external factors; leverage <strong>traditional controllers</strong> to auto-generate demonstrations.</li>
<li><strong>Prompting strategies:</strong> adopt <strong>stepwise/CoT prompting</strong>; consider <strong>multi-agent</strong> decompositions.</li>
<li><strong>Benchmarking:</strong> the <strong>18,604</strong> VLATest scenes serve as an <strong>early benchmark</strong>; expand to more tasks/robots/conditions.</li>
<li><strong>Online risk assessment:</strong> explore <strong>uncertainty estimation</strong> and <strong>safety monitoring</strong> for runtime quality control.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=related-work>Related Work<a href=#related-work class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Robotics foundation models:</strong> (1) LLMs for planning/rewards; (2) <strong>Multi-modal</strong> FMs (VLMs/VLAs) for manipulation & perception.</li>
<li><strong>CPS testing:</strong> gray-box/black-box fuzzing and search-based testing exist, but <strong>not directly applicable</strong> to VLAs (multimodality, autoregression, scale).</li>
<li><strong>FM evaluation:</strong> beyond static benchmarks, VLATest <strong>dynamically generates</strong> 3D manipulation test cases—distinct from <strong>text-only</strong> testing.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=threats-to-validity-mitigations-in-study>Threats to Validity (mitigations in study)<a href=#threats-to-validity-mitigations-in-study class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Internal:</strong> randomness (mitigated by <strong>18,604</strong> scenes); potential prompt bias (mutations <strong>manually validated</strong>).</li>
<li><strong>External:</strong> generalization to other tasks/models; chose <strong>popular tasks</strong> (Open X-Embodiment) and <strong>SOTA public models</strong>.</li>
<li><strong>Construct:</strong> limited operators (lighting/camera/confounders chosen; future: #lights, camera intrinsics, resolution).<!-- -->
<ul>
<li>Coverage: <strong>trajectory coverage</strong> used as a pragmatic proxy.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>VLATest</strong>: early, <strong>generation-based fuzzing</strong> framework (10 operators) for VLA testing in ManiSkill2.</li>
<li><strong>Empirical evidence</strong> across <strong>7 models / 4 tasks / 18,604 scenes</strong> shows <strong>limited robustness</strong> (lighting, camera, unseen objects, instruction variation).</li>
<li>Points to <strong>data scaling</strong>, <strong>prompting</strong>, <strong>benchmarking</strong>, and <strong>risk assessment</strong> as practical paths to <strong>more reliable</strong> VLA systems.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Wang, Z., Zhou, Z., Song, J., Huang, Y., Shu, Z., & Ma, L. (2025). VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation. Proceedings of the ACM on Software Engineering, 2(FSE), 1615–1638.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag title="Vision-Language Models" class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/09/01/open-x-embodiment-review/>Open X-Embodiment review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-09-01T03:47:28.350Z>September 1, 2025</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=rt-x>RT-X<a href=#rt-x class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RT-X trains generalist robot policies by co-training RT-1/RT-2 on an X-embodiment mix of multi-robot, multi-task data, enabling efficient adaptation to new robots, tasks, and environments.</li>
<li>It standardizes 1M+ trajectories from 22 embodiments into the Open X-Embodiment (RLDS/tfrecord) repository, unifying observations and 7-DoF actions via coarse alignment.</li>
<li>Experiments show strong positive transfer and emergent skills (≈3× with RT-2-X on cross-robot tasks); performance scales with model capacity, short image histories, and web pretraining, while sensing/actuation diversity and frame alignment remain open problems.</li>
</ul>
<p><img decoding=async loading=lazy alt="RT-X Architecture" src=/en/assets/images/rt-x-architecture-cc2128128460577bc8f720626e0d671d.png width=1552 height=412 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=motivation>Motivation<a href=#motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Seeks a <strong>generalist X-robot policy</strong> that can be efficiently adapted to new robots, tasks, and environments.</li>
<li>Mirrors a trend from CV/NLP where <strong>general-purpose, web-scale pretrained models</strong> outperform narrow, task-specific models.</li>
<li>Robotics lacks comparably large, diverse <strong>interaction datasets</strong>, making direct transfer of these lessons challenging.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=objectives>Objectives<a href=#objectives class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li><strong>Positive transfer:</strong> Test whether co-training on data from many robots improves performance on each training domain.</li>
<li><strong>Ecosystem building:</strong> Organize large robotic datasets to enable future X-embodiment research.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=core-approach>Core Approach<a href=#core-approach class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Train <strong>RT-1</strong> and <strong>RT-2</strong> on data from <strong>9 different manipulators</strong>, producing <strong>RT-X</strong> variants that outperform policies trained only on the evaluation domain and show <strong>better generalization</strong> and <strong>new capabilities</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=whats-different-from-prior-transfer-methods>What’s Different From Prior Transfer Methods<a href=#whats-different-from-prior-transfer-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Many prior works reduce the <strong>embodiment gap</strong> via specialized mechanisms (shared action spaces, representation learning objectives, policy adaptation using embodiment metadata, decoupled robot/environment representations, domain translation).</li>
<li><strong>RT-X directly trains on X-embodiment data without explicit gap-reduction machinery</strong> and still observes <strong>positive transfer</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=dataset--format-open-x-embodiment>Dataset & Format (Open X-Embodiment)<a href=#dataset--format-open-x-embodiment class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>1M+ real robot trajectories, 22 embodiments</strong> (single-arm, bimanual, quadrupeds), pooled from <strong>60 datasets / 34 labs</strong>, standardized for easy use.</li>
<li>Uses <a href=https://github.com/google-research/rlds target=_blank rel="noopener noreferrer"><strong>RLDS</strong></a> (serialized <code>tfrecord</code>), supporting varied action spaces and input modalities (RGB, depth, point clouds), and efficient parallel loading across major DL frameworks.</li>
<li>Language annotations are leveraged; <strong>PaLM</strong> is used to extract objects/behaviors from instructions.</li>
</ul>
<p><img decoding=async loading=lazy alt=RLDS src=/en/assets/images/rlds-5e68d1c660ef048892d5594530c62239.png width=726 height=353 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=data-format-consolidation-coarse-alignment>Data Format Consolidation (Coarse Alignment)<a href=#data-format-consolidation-coarse-alignment class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Observations:</strong> History of recent images + language instruction. One <strong>canonical camera view</strong> per dataset is resized to a common resolution.</li>
<li><strong>Actions:</strong> Convert original controls to a <strong>7-DoF end-effector vector</strong> (x, y, z, roll, pitch, yaw, gripper or their rates). Actions are <strong>normalized before discretization</strong>; outputs are <strong>de-normalized per embodiment</strong>.</li>
<li><strong>Deliberate non-alignment:</strong> Camera poses/properties are <strong>not</strong> standardized; action frame alignment across datasets is <strong>not</strong> enforced. The same action vector may cause <strong>different motions</strong> on different robots (absolute/relative, position/velocity allowed).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=policy-architectures>Policy Architectures<a href=#policy-architectures class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>RT-1 (≈35M params):</strong> Transformer for control. Inputs: 15-frame image history + natural-language instruction.<!-- -->
<ul>
<li>Vision via ImageNet-pretrained <strong>EfficientNet</strong>; language via <strong>USE</strong> embedding.</li>
<li>Fuse via <strong>FiLM</strong> → 81 vision–language tokens → <strong>decoder-only Transformer</strong> outputs tokenized actions.</li>
</ul>
</li>
<li><strong>RT-2 (VLA family):</strong> Internet-scale VLM co-fine-tuned to output <strong>action as text tokens</strong> (e.g., <code>1 128 91 241 5 101 127</code>).<!-- -->
<ul>
<li>Any pretrained VLM can be adapted; this work uses <strong>RT-2–PaLI-X</strong> (ViT backbone + UL2 LM; primarily pretrained on WebLI).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=training-setup>Training Setup<a href=#training-setup class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Robotics data mixture:</strong> Data from <strong>9 manipulators</strong> (a union of multiple well-known robotics datasets).</li>
<li><strong>Loss:</strong> Standard <strong>categorical cross-entropy</strong> over tokenized actions.</li>
<li><strong>Regimes:</strong>
<ul>
<li><strong>RT-1-X:</strong> Trained solely on the robotics mixture.</li>
<li><strong>RT-2-X:</strong> <strong>Co-fine-tuned</strong> on a ~1:1 mix of original VLM data and the robotics mixture.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=experimental-questions>Experimental Questions<a href=#experimental-questions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li>Does X-embodiment co-training improve in-domain performance (positive transfer)?</li>
<li>Does it improve <strong>generalization</strong> to <strong>unseen tasks</strong>?</li>
<li>How do <strong>model size</strong>, <strong>architecture</strong>, and <strong>dataset composition</strong> influence performance/generalization?</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=key-results>Key Results<a href=#key-results class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Small-scale domains:</strong> <strong>RT-1-X</strong> outperforms the <strong>Original Method</strong> (the authors’ per-dataset baselines) on <strong>4/5</strong> datasets with a large average gain → <strong>limited data domains</strong> benefit greatly from X-embodiment co-training.</li>
<li><strong>Large-scale domains:</strong>
<ul>
<li><strong>RT-1-X</strong> does <strong>not</strong> beat an RT-1 trained only on the embodiment-specific large dataset (suggests underfitting for this class).</li>
<li><strong>RT-2-X</strong> (larger capacity) <strong>outperforms both</strong> Original Method and RT-1 → X-robot training helps even in <strong>data-rich</strong> regimes when using <strong>sufficient capacity</strong>.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=generalization--emergent-skills>Generalization & Emergent Skills<a href=#generalization--emergent-skills class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Unseen objects/backgrounds/environments:</strong> RT-2 and RT-2-X perform <strong>on par</strong> (VLM backbone already strong here).</li>
<li><strong>Emergent skills (transfer across robots):</strong> On Google Robot tasks that <strong>do not appear</strong> in RT-2’s dataset but exist in <strong>Bridge</strong> (for <strong>WidowX</strong>), <strong>RT-2-X ≈ 3×</strong> RT-2.<!-- -->
<ul>
<li>Removing <strong>Bridge</strong> from RT-2-X training <strong>significantly reduces</strong> hold-out performance → skills likely <strong>transferred</strong> from WidowX data.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=design-insights-ablations>Design Insights (Ablations)<a href=#design-insights-ablations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Short image history</strong> notably <strong>improves generalization</strong>.</li>
<li><strong>Web pretraining</strong> is <strong>critical</strong> for large models’ high performance.</li>
<li><strong>Model capacity matters:</strong> <strong>55B</strong> model succeeds more than <strong>5B</strong> on emergent skills → greater capacity ⇒ greater cross-dataset transfer.</li>
<li><strong>Co-fine-tuning vs. fine-tuning:</strong> Similar performance in this study (attributed to the <strong>greater diversity</strong> of robotics data in RT-2-X vs. prior works).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=limitations-open-problems>Limitations (Open Problems)<a href=#limitations-open-problems class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Does <strong>not</strong> cover robots with <strong>very different sensing/actuation modalities</strong>.</li>
<li>Does <strong>not</strong> study generalization to <strong>new robots</strong> nor define a <strong>decision criterion</strong> for when positive transfer will occur.</li>
<li>Camera pose/properties and control frame <strong>remain unaligned</strong>; a deliberate but still challenging domain gap to address in future work.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>O’Neill, A., Rehman, A., Maddukuri, A., Gupta, A., Padalkar, A., Lee, A., Pooley, A., Gupta, A., Mandlekar, A., & Jain, A. (2024). Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. 2024 IEEE International Conference on Robotics and Automation (ICRA).</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag title="Vision-Language Models" class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/09/01/pi-zero-reivew/>π0 Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-31T14:24:33.716Z>August 31, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=π0>π0<a href=#π0 class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=problem--motivation>Problem & Motivation<a href=#problem--motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Achieving real-world generality in robot learning is blocked by <strong>data scarcity, generalization, and robustness</strong> limits.</li>
<li>Human intelligence most outpaces machines in <strong>versatility</strong>—solving diverse, physically situated tasks under constraints, language commands, and perturbations.</li>
<li>In NLP/CV, <strong>foundation models</strong> pre-trained on diverse multi-task data, then <strong>fine-tuned (aligned)</strong> on curated datasets, outperform narrow specialists; the same paradigm is hypothesized for robotics.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=core-proposal>Core Proposal<a href=#core-proposal class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>A <strong>novel flow-matching architecture</strong> built on a pre-trained <strong>Vision-Language Model (VLM)</strong> to inherit Internet-scale semantics.</li>
<li>Further training adds <strong>robot actions</strong>, turning the model into a <strong>Vision-Language-Action (VLA)</strong> policy.</li>
<li>Use <strong>cross-embodiment training</strong> to combine data from many robot types (single/dual-arm, mobile), despite differing configuration/action spaces.</li>
<li>Employ <strong>action chunking</strong> + <strong>flow matching</strong> (diffusion variant) to model complex, continuous, high-frequency actions.</li>
<li>Introduce an <strong>Action Expert</strong> (separate weights for action/state tokens), akin to a <strong>Mixture-of-Experts</strong>, augmenting the standard VLM.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=training-recipe-pre--vs-post-training>Training Recipe (Pre- vs Post-Training)<a href=#training-recipe-pre--vs-post-training class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Pre-training</strong> on highly diverse data builds broad, general physical abilities.</li>
<li><strong>Post-training</strong> on curated, task-specific data instills <strong>fluent, efficient strategies</strong>.</li>
<li>Rationale: high-quality-only training lacks recovery behaviors; low-quality-only training lacks efficiency/robustness; <strong>combining both</strong> yields desired behavior.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=data--backbone>Data & Backbone<a href=#data--backbone class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>~<strong>10,000 hours</strong> of demonstrations + the <strong>OXE</strong> dataset; data spans <strong>7 robot configurations</strong> and <strong>68 tasks</strong>.</li>
<li>VLM backbone initialized from <strong>PaliGemma (3B)</strong>; add <strong>~300M</strong> parameters for the action expert (total <strong>~3.3B</strong>).</li>
<li>Pre-training mixture: weighted combination of internal datasets + full OXE; <strong>n^0.43</strong> weighting to down-weight overrepresented task-robot pairs.</li>
<li>Unify interfaces: zero-pad <strong>qt/at</strong> to the largest robot dimension (18); mask missing image slots; late-fusion encoders map images/states to the same token space as language.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=modeling-details>Modeling Details<a href=#modeling-details class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Conditional flow matching</strong> models the continuous distribution over action chunks.</li>
<li>Train with a <strong>diffusion-style loss</strong> on individual sequence elements (instead of cross-entropy), with separate weights for diffusion-related tokens.</li>
<li>Flow path uses a <strong>linear-Gaussian</strong> schedule; sample noisy actions with ε∼N(0, I); predict denoising vector field; <strong>Euler integration</strong> from τ=0→1 at inference.</li>
<li>Efficient inference by <strong>caching</strong> K/V for the observation prefix; action tokens recomputed per integration step.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=high-level-language-policy>High-Level Language Policy<a href=#high-level-language-policy class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Because the policy consumes language, a <strong>high-level VLM</strong> can decompose tasks (e.g., bussing) into intermediate language subgoals (SayCan-style planning), improving performance on complex, temporally extended tasks.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=evaluation-setup--baselines>Evaluation Setup & Baselines<a href=#evaluation-setup--baselines class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Out-of-box</strong> (direct prompting), <strong>fine-tuning</strong> on downstream tasks, and <strong>with high-level VLM</strong> providing intermediate commands.</li>
<li>Compare against <strong>OpenVLA (7B, autoregressive discretization; no action chunks/high-frequency control)</strong> and <strong>Octo (93M; diffusion)</strong>, trained on the same mixture.</li>
<li>Include a <strong>compute-parity</strong> π0 (160k steps vs 700k) and a <strong>π0-small</strong> variant (no VLM init).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=key-results>Key Results<a href=#key-results class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Out-of-box</strong>: π0 outperforms all baselines; even compute-parity π0 beats OpenVLA/Octo; π0-small still surpasses them—highlighting the benefits of <strong>expressive architectures + diffusion/flow matching + VLM pre-training</strong>.</li>
<li><strong>Language following</strong>: π0 clearly exceeds π0-small across conditions:<!-- -->
<ul>
<li><strong>π0-flat</strong>: only overall task command.</li>
<li><strong>π0-human</strong>: human-provided intermediate steps.</li>
<li><strong>π0-HL</strong>: high-level VLM-provided steps (fully autonomous).</li>
<li>Better language-following accuracy <strong>directly translates</strong> into stronger autonomous performance with high-level guidance.</li>
</ul>
</li>
<li><strong>New dexterous tasks</strong> (e.g., bowls stacking, towel folding, microwave, drawer items, paper towel replacement):<!-- -->
<ul>
<li>Fine-tuned π0 generally outperforms <strong>OpenVLA</strong>, <strong>Octo</strong>, and small-data methods <strong>ACT</strong> / <strong>Diffusion Policy</strong>.</li>
<li>Pre-training helps most when tasks resemble pre-training data; pretrained π0 often beats from-scratch by up to <strong>2×</strong>.</li>
</ul>
</li>
<li><strong>Complex multi-stage tasks</strong> (laundry folding, table bussing, box building, to-go box, eggs):<!-- -->
<ul>
<li>π0 solves many tasks; <strong>full pre-training + fine-tuning</strong> performs best.</li>
<li>Gains from pre-training are <strong>especially large</strong> on harder tasks; absolute performance varies with task difficulty and pre-training coverage.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=takeaways--limitations>Takeaways & Limitations<a href=#takeaways--limitations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>π0 mirrors LLM training: <strong>pre-train for knowledge</strong>, <strong>post-train for alignment</strong> (instruction-following and execution).</li>
<li>Limitations/open questions:<!-- -->
<ul>
<li>Optimal <strong>composition/weighting</strong> of pre-training data remains unclear.</li>
<li>Not all tasks work reliably; difficult to predict <strong>how much/what kind</strong> of data is needed for near-perfect performance.</li>
<li>Uncertain <strong>positive transfer</strong> across very diverse tasks/robots and to distinct domains (e.g., driving, navigation, legged locomotion).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B., Jakubczak, S., Jones, T., Ke, L., Levine, S., Li‑Bell, A., Mothukuri, M., Nair, S., Pertsch, K., Shi, L. X, … Zhilinsky, U. (2025, June 21). π₀: A vision‑language‑action flow model for general robot control Robotics: Science and Systems (RSS), Los Angeles, CA, United States. <code>https://roboticsconference.org/program/papers/10/</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag title="Vision-Language Models" class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/31/vima-review/>Vima Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-31T10:20:03.726Z>August 31, 2025</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=vima>VIMA<a href=#vima class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Unified Multimodal Prompts</strong>: Reformulates diverse robot tasks (language, images, video) into a single sequence modeling problem.</li>
<li><strong>Object-Centric Tokenization</strong>: Uses object-level tokens (Mask R-CNN + ViT) instead of raw pixels, improving data efficiency and semantic generalization.</li>
<li><strong>Cross-Attention Conditioning</strong>: Conditions the policy on prompts via cross-attention, maintaining strong zero-shot performance even with small models or novel tasks.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=motivation>Motivation<a href=#motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Robot task specification comes in many forms: one-shot demonstrations, language instructions, and visual goals.</li>
<li>Traditionally, each task required distinct architectures and pipelines, leading to siloed systems with poor generalization.</li>
</ul>
<p><img decoding=async loading=lazy alt="VIMA Architecture" src=/en/assets/images/vima-architecture-f4e3269ed85bc5b1c954f82ab1ef77cd.png width=1424 height=988 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=key-contributions>Key Contributions<a href=#key-contributions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li>
<p><strong>Multimodal Prompting</strong></p>
<ul>
<li>A novel formulation that unifies diverse robot manipulation tasks into a <strong>sequence modeling problem</strong>.</li>
<li>Prompts are defined as interleaved sequences of text and images, enabling flexibility across task formats.</li>
</ul>
</li>
<li>
<p><strong>VIMA-BENCH</strong></p>
<ul>
<li>A large-scale benchmark with <strong>17 tasks</strong> across six categories (object manipulation, goal reaching, novel concept grounding, video imitation, constraint satisfaction, visual reasoning).</li>
<li>Provides <strong>650K expert trajectories</strong> and a <strong>four-level evaluation protocol</strong> for systematic generalization.</li>
</ul>
</li>
<li>
<p><strong>VIMA Agent</strong></p>
<ul>
<li>A transformer-based visuomotor agent with <strong>encoder-decoder architecture</strong> and <strong>object-centric design</strong>.</li>
<li>Encodes prompts with a pre-trained <strong>T5 model</strong>, parses images into object tokens via <strong>Mask R-CNN + ViT</strong>, and decodes actions autoregressively using <strong>cross-attention</strong>.</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=design-insights>Design Insights<a href=#design-insights class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Object-Centric Representation</strong>: Passing variable-length object token sequences directly to the controller is more effective than pixel-based tokenization.</li>
<li><strong>Cross-Attention Conditioning</strong>: Stronger prompt focus and efficiency compared to simple concatenation (e.g., GPT-style).</li>
<li><strong>Robustness</strong>: Minimal degradation under distractors or corrupted prompts, aided by T5 backbone and object augmentation.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=results>Results<a href=#results class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>
<p><strong>Performance</strong>:</p>
<ul>
<li>Outperforms baselines (VIMA-Gato, VIMA-Flamingo, VIMA-GPT) by up to <strong>2.9× success rate</strong> in hardest zero-shot generalization.</li>
<li>With <strong>10× less training data</strong>, still <strong>2.7× better</strong> than best competitor.</li>
</ul>
</li>
<li>
<p><strong>Scaling</strong>:</p>
<ul>
<li>Sample-efficient: with just <strong>1% of data</strong>, matches baselines trained with 10× more.</li>
<li>Generalization holds across L1–L4 evaluation, with smaller regression than alternatives.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>VIMA demonstrates that multimodal prompting is a powerful unifying framework for robot learning.<br>
<!-- -->It achieves strong scalability, data efficiency, and generalization, establishing a <strong>solid starting point for future generalist robot agents</strong>.</p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A., Zhu, Y., & Fan, L. (2023). VIMA: Robot Manipulation with Multimodal Prompts Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v202/jiang23b.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag title="Vision-Language Models" class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/31/roboflamingo-review/>RoboFlamingo Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-31T05:35:06.415Z>August 31, 2025</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=roboflamingo>RoboFlamingo<a href=#roboflamingo class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RoboFlamingo <strong>decouples vision-language understanding and control</strong>, using OpenFlamingo for perception and a lightweight policy head for sequential decision-making.</li>
<li>Unlike prior VLM-based approaches, it requires only <strong>small-scale imitation fine-tuning</strong> on language-conditioned manipulation data, without large-scale co-fine-tuning.</li>
<li>This design enables <strong>data-efficient, zero-shot generalizable, and deployable</strong> robot manipulation policies on modest compute resources.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=key-idea>Key Idea<a href=#key-idea class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Proposes <strong>RoboFlamingo</strong>, a simple framework to adapt existing VLMs for robotic manipulation with lightweight fine-tuning.</li>
<li>Built on <strong>OpenFlamingo</strong>, decoupling <strong>vision-language understanding</strong> from <strong>decision-making</strong>.</li>
<li>Pre-trained VLM handles <strong>language and visual comprehension</strong>, while a dedicated <strong>policy head models sequential history</strong>.</li>
<li>Fine-tuned only on <strong>language-conditioned manipulation datasets</strong> using imitation learning.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=advantages>Advantages<a href=#advantages class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Requires only a <strong>small amount of demonstrations</strong> to adapt to downstream manipulation tasks.</li>
<li>Provides <strong>open-loop control</strong> capability → deployable on low-performance platforms.</li>
<li>Can be trained/evaluated on a <strong>single GPU server</strong>, making it a cost-effective and accessible solution.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=benchmarks>Benchmarks<a href=#benchmarks class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Evaluated on <strong>CALVIN benchmark</strong> (34 tasks, 1000 instruction chains).</li>
<li>RoboFlamingo achieves <strong>2× performance improvements</strong> over previous state-of-the-art methods.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=performance>Performance<a href=#performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Imitation Learning</strong>: Outperforms all baselines across all metrics.</li>
<li><strong>Zero-shot Generalization</strong>:<!-- -->
<ul>
<li><strong>Vision</strong>: Stronger generalization in ABC→D setting.</li>
<li><strong>Language</strong>: Robust to GPT-4 generated synonymous instructions.</li>
</ul>
</li>
<li><strong>Ablation Studies</strong>:<!-- -->
<ul>
<li>Ignoring history (MLP w/o hist) gives worst results.</li>
<li>LSTM and GPT-based policy heads perform best (LSTM chosen as default).</li>
<li><strong>VL pre-training</strong> is crucial for downstream manipulation.</li>
<li><strong>Larger VLMs</strong> show better data efficiency.</li>
<li><strong>Instruction fine-tuning</strong> improves both seen and unseen tasks.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=flexibility-of-deployment>Flexibility of Deployment<a href=#flexibility-of-deployment class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Supports <strong>open-loop control</strong> by predicting entire action sequences with a single inference → reduces latency and test-time compute.</li>
<li>Direct open-loop use without retraining can degrade performance; mitigated with <strong>jump-step demonstrations</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Demonstrates that pre-trained VLMs enable <strong>data efficiency</strong> and strong <strong>zero-shot generalization</strong> in robotic manipulation.</li>
<li>RoboFlamingo is presented as an <strong>intuitive, efficient, and open solution</strong>, with high potential when combined with large-scale real robot data.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Li, X., Liu, M., Zhang, H., Yu, C., Xu, J., Wu, H., Cheang, C., Jing, Y., Zhang, W., & Liu, H. (2024). Vision-language foundation models as effective robot imitators. International Conference on Learning Representations (ICLR 2024), Vienna, Austria.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag title="Vision-Language Models" class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/29/open-vla-review/>OpenVLA Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-29T07:43:25.796Z>August 29, 2025</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=openvla>OpenVLA<a href=#openvla class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>OpenVLA is a 7B open-source VLA model built on Llama2 + DINOv2 + SigLIP, trained on 970k demos, achieving stronger generalization and robustness than closed RT-2-X (55B) and outperforming Diffusion Policy.</li>
<li>It introduces efficient adaptation via LoRA (1.4% params, 8× compute reduction) and 4-bit quantization (half memory, same accuracy), enabling fine-tuning and inference on consumer GPUs.</li>
<li>Limitations remain (single-image input, <code>&lt;90%</code> reliability, limited throughput), but OpenVLA provides the first open, scalable framework for generalist robot policies.</li>
</ul>
<p><img decoding=async loading=lazy alt="OpenVLA Architecture" src=/en/assets/images/open-vla-architecture-df4421cc82c1ebceca0ccf1061cd4593.png width=915 height=366 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=motivation>Motivation<a href=#motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Training robot policies from scratch struggles with robustness and generalization.</li>
<li>Fine-tuning <strong>vision-language-action (VLA)</strong> models offers reusable, generalizable visuomotor policies.</li>
<li>Barriers: prior VLAs are <strong>closed-source</strong>, lack best practices for adaptation, and need server-class hardware.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=model--training>Model & Training<a href=#model--training class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>OpenVLA</strong>: 7B parameters, open-source.</li>
<li>Built on <strong>Llama 2</strong> with fused <strong>DINOv2 + SigLIP</strong> vision encoders.</li>
<li>Trained on <strong>970k robot demonstrations</strong> from Open-X Embodiment dataset.</li>
<li>Represents robot actions as <strong>tokens</strong> (discretized into 256 bins, replacing unused Llama tokens).</li>
<li>Standard <strong>next-token prediction</strong> objective.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=architecture--approach>Architecture & Approach<a href=#architecture--approach class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>End-to-end fine-tuning of VLM to generate robot actions as tokens.</li>
<li>Differs from modular methods (e.g., Octo) that stitch separate encoders/decoders.</li>
<li>Vision features are obtained by encoding the same input image with both SigLIP and DINOv2, then channel-wise concatenated and passed through an MLP projector. This preserves SigLIP’s semantic alignment with language and DINOv2's spatial reasoning, giving the VLM richer multimodal context for manipulation tasks.</li>
<li>Uses Prismatic VLM backbone with multi-resolution features (spatial reasoning + semantics).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=performance>Performance<a href=#performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Outperforms closed <strong>RT-2-X (55B)</strong> by <strong>+16.5% task success</strong> with 7× fewer parameters.</li>
<li>Beats <strong>Diffusion Policy</strong> (from-scratch imitation learning) by <strong>+20.4%</strong> on multi-task language-grounded settings.</li>
<li>Demonstrates <strong>robust behaviors</strong> (distractor resistance, error recovery).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=efficiency>Efficiency<a href=#efficiency class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Introduces <strong>parameter-efficient fine-tuning</strong>:<!-- -->
<ul>
<li><strong>LoRA</strong> updates only 1.4% of parameters yet matches full fine-tuning.</li>
<li>Can fine-tune on a single A100 GPU in ~10–15 hours (8× compute reduction).</li>
</ul>
</li>
<li><strong>Quantization</strong>:<!-- -->
<ul>
<li>4-bit inference matches bfloat16 accuracy while halving memory footprint.</li>
<li>Runs at 3Hz on consumer GPUs (e.g., A5000, 16GB).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=evaluations>Evaluations<a href=#evaluations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Tested across <strong>29 tasks</strong> and multiple robots (WidowX, Google robot, Franka).</li>
<li>Strong generalization on:<!-- -->
<ul>
<li><strong>Visual</strong> (unseen backgrounds/distractors).</li>
<li><strong>Motion</strong> (new object positions/orientations).</li>
<li><strong>Physical</strong> (new object shapes/sizes).</li>
<li><strong>Semantic</strong> (unseen tasks, instructions).</li>
</ul>
</li>
<li>First generalist open-source VLA achieving <strong>≥50% success rate across all tested tasks</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=design-insights>Design Insights<a href=#design-insights class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Fine-tuning the vision encoder</strong> (vs. freezing) crucial for robotic control.</li>
<li>Higher image resolution (384px vs. 224px) adds 3× compute without performance gains.</li>
<li>Training required <strong>27 epochs</strong>, far more than typical VLM runs, to surpass 95% action token accuracy.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=limitations--future-work>Limitations & Future Work<a href=#limitations--future-work class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Supports only <strong>single-image observations</strong> (no proprioception, no history).</li>
<li>Inference throughput (~6Hz on RTX 4090) insufficient for high-frequency control (e.g., ALOHA at 50Hz).</li>
<li>Success rates remain below 90% in challenging tasks.</li>
<li>Open questions:<!-- -->
<ul>
<li>Impact of base VLM size on performance.</li>
<li>Benefits of co-training with Internet-scale data.</li>
<li>Best visual features for VLAs.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=contributions>Contributions<a href=#contributions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li>First <strong>open-source generalist VLA</strong> with strong performance.</li>
<li>Scalable <strong>end-to-end training</strong> pipeline (action-as-token).</li>
<li>Demonstrates <strong>LoRA + quantization</strong> for consumer-grade GPU adaptation.</li>
<li>Provides <strong>code, checkpoints, and data curation recipes</strong> to support future research.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E. P., Sanketi, P. R., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake, R., Sadigh, D., Levine, S., Liang, P., & Finn, C. (2025). OpenVLA: An Open-Source Vision-Language-Action Model Proceedings of The 8th Conference on Robot Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v270/kim25c.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag title="Vision-Language Models" class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/27/octo-review/>Octo Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-27T10:54:09.447Z>August 27, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=octo>Octo<a href=#octo class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Octo is a transformer-based policy with modular tokenizers (language via T5, images via CNN patches), blockwise masking, and readout tokens, trained on 800k multi-robot trajectories.</li>
<li>Actions are generated through a diffusion head that produces continuous, multimodal, chunked predictions, enabling precise control and broad generalization.</li>
<li>It achieves state-of-the-art zero-shot performance across 7 robots and allows efficient finetuning to new sensors and action spaces, while being fully open-source.</li>
</ul>
<table><thead><tr><th>Category<th>Simple Analogy<th>Actual Tokenization<tbody><tr><td><strong>Language</strong><td><code>[Sentence]</code><td><code>[l₁, l₂, l₃, …]</code> <br>→ multiple tokens from a tokenized sentence<tr><td><strong>Goal Image</strong><td><code>[Goal]</code><td><code>[g₁, g₂, g₃, …]</code> <br>→ image split into patches<tr><td><strong>Observation (time t)</strong><td><code>[Observation]</code><td><code>[oₜ¹, oₜ², oₜ³, …]</code> <br>→ camera frames/sensors tokenized into patches<tr><td><strong>Readout Token</strong><td><code>[ ]</code> (empty slot)<td><code>[TR,t]</code> <br>→ one per timestep, reserved for predicting actions</table>
<div class="language-bash codeBlockContainer_QFtC theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_vdmG><pre tabindex=0 class="prism-code language-bash codeBlock_CpxK thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines__0Nl><span class=token-line style=color:#393A34><span class="token plain">Time t-1: </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">l</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">g</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">o_</span><span class="token punctuation" style=color:#393A34>{</span><span class="token plain">t-1</span><span class="token punctuation" style=color:#393A34>}</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t-1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">Time t:   </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">l</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">g</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">o_t</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">     </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">Time t+1: </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">l</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">g</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">o_</span><span class="token punctuation" style=color:#393A34>{</span><span class="token plain">t+1</span><span class="token punctuation" style=color:#393A34>}</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t+1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t-1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">, </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">, </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t+1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">  ──►  Diffusion </span><span class="token function" style=color:#d73a49>head</span><span class="token plain">  ──►  </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">a_t, a_</span><span class="token punctuation" style=color:#393A34>{</span><span class="token plain">t+1</span><span class="token punctuation" style=color:#393A34>}</span><span class="token plain">, …</span><span class="token punctuation" style=color:#393A34>]</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=motivation>Motivation<a href=#motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Traditional robot learning trains policies <strong>from scratch</strong> on robot/task-specific datasets → costly data collection, narrow generalization.</li>
<li><strong>Generalist Robot Policies (GRPs)</strong> pretrained on diverse robots/tasks can be <strong>finetuned with little in-domain data</strong> while generalizing broadly.</li>
<li>Real-world deployments face challenges across <strong>robot embodiments, sensor setups, action spaces, task specs, and environments</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=prior-grps--gaps>Prior GRPs & Gaps<a href=#prior-grps--gaps class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>GRPs aim for <strong>low-level visuomotor control</strong> across tasks, environments, and robotic systems.</li>
<li>Existing models often have <strong>restricted inputs (e.g., a single camera)</strong>, <strong>lack efficient finetuning to new domains</strong>, and importantly, <strong>largest models are not publicly available</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=contribution-what-is-octo>Contribution (What is Octo?)<a href=#contribution-what-is-octo class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Octo</strong>: a large transformer-based policy trained on <strong>800k trajectories</strong> from the Open X-Embodiment dataset.</li>
<li>Accepts <strong>language instructions or goal images</strong>, and can be <strong>finetuned within hours on consumer GPUs</strong> to new sensors and action spaces.</li>
<li><strong>First GRP</strong> to support <strong>effective finetuning to new observations and actions</strong> and to be <strong>fully open-source</strong> (training pipeline, checkpoints, data).</li>
<li>Novelty lies in combining: <strong>transformer backbone + language/goal image conditioning + diffusion head</strong> for expressive action distributions.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=architecture>Architecture<a href=#architecture class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Input tokenizers</strong>:<!-- -->
<ul>
<li>Language via pretrained <strong>T5-base</strong></li>
<li>Images via shallow CNN → patch tokens</li>
</ul>
</li>
<li><strong>Transformer backbone</strong>: processes unified token sequence.</li>
<li><strong>Blockwise masking + Readout tokens</strong>:<!-- -->
<ul>
<li>Nonexistent modalities are masked</li>
<li>Readout tokens <em>only attend</em> to past observations/tasks, not vice versa</li>
</ul>
</li>
<li><strong>Diffusion action head</strong>: predicts <strong>continuous, multimodal, chunked actions</strong>.</li>
<li><strong>Modularity</strong>: new sensors/outputs can be added by only training lightweight encoders or heads; pretrained backbone remains unchanged.</li>
</ul>
<p><img decoding=async loading=lazy alt="Octo Architecture" src=/en/assets/images/octo-architecture-49b9dd94643695f0566e74ac5a0801bd.png width=803 height=415 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=training-data--objective>Training Data & Objective<a href=#training-data--objective class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Mixture of <strong>25 heterogeneous robot datasets</strong>: diverse robots, sensors (with/without wrist cams), labels (with/without language).</li>
<li><strong>Conditional diffusion decoding</strong> predicts continuous, multimodal action distributions.<!-- -->
<ul>
<li>Transformer runs <strong>one forward pass</strong>; denoising steps are contained in the small diffusion head.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=experiments>Experiments<a href=#experiments class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Evaluated on <strong>7 robotic platforms across 4 institutions</strong>.</li>
<li>Key questions:<!-- -->
<ol>
<li>Zero-shot multi-robot control?</li>
<li>Do Octo weights improve finetuning vs. scratch or standard pretrained representations?</li>
<li>Which design choices matter for generalist robot policies?</li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=results>Results<a href=#results class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Achieves <strong>state-of-the-art zero-shot multi-robot control</strong>, competitive with RT-1-X and RT-2-X.</li>
<li>Provides a <strong>versatile policy initialization</strong>: significantly outperforms baselines for <strong>data-efficient finetuning</strong> to new obs/action spaces.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=limitations--future-work>Limitations / Future Work<a href=#limitations--future-work class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Needs <strong>better language conditioning</strong>, <strong>improved wrist camera support</strong>, and <strong>data beyond optimal demonstrations</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=one-line-takeaway>One-line Takeaway<a href=#one-line-takeaway class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Octo = modular, efficient, open-source GRP</strong>:<br>
<!-- -->A transformer + diffusion policy trained on large-scale multi-robot data that <strong>adapts quickly with little in-domain data</strong> to new sensors and action spaces, enabling broad generalization.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Mees, O., Ghosh, D., Pertsch, K., Black, K., Walke, H. R., Dasari, S., Hejna, J., Kreiman, T., Xu, C., & Luo, J. (2024). Octo: An open-source generalist robot policy. First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag title="Vision-Language Models" class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/24/rt-2-review/>RT-2, Robotic Transformer 2 Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-24T13:40:19.433Z>August 24, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><ul>
<li>Trains a <strong>Vision-Language-Action (VLA)</strong> model by co-fine-tuning web-scale VLMs with robot trajectories, and <strong>treats robot actions as text tokens</strong>.</li>
<li>Yields <strong>strong generalization</strong> and <strong>emergent capabilities</strong> (symbol understanding, reasoning, human recognition) beyond what appears in robot data.</li>
<li>Runs in <strong>direct closed-loop control</strong>; largest evaluated model (55B) executes at ~1–3 Hz via a cloud (multi-TPU) inference setup.</li>
</ul>
<p><img decoding=async loading=lazy alt="RT-2 Architecture" src=/en/assets/images/rt-2-architecture-dd9ff6e2cae963c14c20742089b822df.png width=1674 height=748 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=what-rt-2-is>What RT-2 Is<a href=#what-rt-2-is class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>A family of VLA models (RT-2-PaLI-X, RT-2-PaLM-E) that fine-tune large VLMs on robot trajectories to output <strong>low-level actions</strong>.</li>
<li>Target: <strong>generalizable, semantically aware</strong> manipulation policies that map images + instructions → actions end-to-end.</li>
<li>RT-2 does <strong>not rely on a restricted 2D action space or calibrated cameras</strong>.</li>
<li>The <strong>unified output space</strong> lets language and action tokens share the same model weights, without action-only layers.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=core-recipe>Core Recipe<a href=#core-recipe class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Directly train</strong> open-vocabulary VQA/dialogue VLMs to <strong>output robot actions</strong> while they still solve standard vision-language tasks.</li>
<li>Build on RT-1 protocol/data, but replace the policy backbone with a <strong>large VLM</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=action-as-language-tokenization>Action as Language (Tokenization)<a href=#action-as-language-tokenization class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Discretize continuous action dims (Δpos/Δrot, gripper, terminate) into <strong>256 bins</strong>; represent each dimension with an <strong>integer token</strong>.</li>
<li><strong>PaLI-X</strong>: reuse numeric tokens (<code>≤1000</code>). <strong>PaLM-E</strong>: overwrite <strong>256 least-frequent tokens</strong> as action vocabulary (<strong>symbol tuning</strong>).</li>
<li>Form a single output string per step (e.g., <code>terminate Δposx Δposy Δposz Δrotx Δroty Δrotz gripper</code>).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=co-fine-tuning--output-constraint>Co-Fine-Tuning & Output Constraint<a href=#co-fine-tuning--output-constraint class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Mix robot data with original web VQA/caption data</strong> in training batches (up-weight robot samples) to prevent forgetting and improve generalization.</li>
<li>During decoding on robot tasks, <strong>restrict sampling to valid action tokens</strong> so outputs are always executable.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=closed-loop-control--real-time-inference>Closed-Loop Control & Real-Time Inference<a href=#closed-loop-control--real-time-inference class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RT-2 is trained and deployed for <strong>direct closed-loop control</strong> (camera → action → camera …), not just high-level planning.</li>
<li>For large models, inference runs via a <strong>multi-TPU cloud service</strong>; <strong>RT-2-PaLI-X-55B</strong> reaches <strong>~1–3 Hz</strong>; smaller models ~5 Hz.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=generalization--benchmarks>Generalization & Benchmarks<a href=#generalization--benchmarks class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Matches RT-1 on seen tasks but <strong>far exceeds</strong> baselines on <strong>unseen objects/backgrounds/environments</strong> (~<strong>2×</strong> vs RT-1/MOO; up to <strong>~6×</strong> vs others).</li>
<li>Open-source <strong>Language-Table</strong> sim: co-fine-tuned <strong>PaLI-3B</strong> outperforms baselines, showing the approach transfers to other robots/sims.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=emergent-capabilities>Emergent Capabilities<a href=#emergent-capabilities class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Symbol understanding</strong> (e.g., “move apple to 3 / heart / star”).</li>
<li><strong>Reasoning</strong> (visual matching, simple math like “sum of two plus one”, <strong>multilingual</strong> commands).</li>
<li><strong>Human recognition</strong> (e.g., “person with glasses”); none of these were present as low-level actions in robot data.</li>
<li><strong>Chain-of-thought (CoT) variant</strong> adds a <strong>Plan</strong> step before actions → supports <strong>multi-stage semantic reasoning</strong> (e.g., pick a rock as an improvised hammer; pick an energy drink for a tired person).</li>
</ul>
<p><img decoding=async loading=lazy alt=rt-2-cot src=/en/assets/images/rt-2-cot-eb2bee68ad29cc277f1a214c22064f0e.png width=1748 height=934 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=scaling--ablations>Scaling & Ablations<a href=#scaling--ablations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>From-scratch</strong> training (even 5B) performs poorly; <strong>fine-tuning</strong> helps; <strong>co-fine-tuning</strong> helps <strong>most</strong>.</li>
<li><strong>Bigger models</strong> (<code>55B > 5B</code>) generalize better.</li>
<li>PaLM-E variant shows an edge on <strong>math reasoning</strong>; PaLI-X stronger on symbols/vision reasoning on average.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=limitations>Limitations<a href=#limitations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Does <strong>not</strong> learn fundamentally <strong>new motor skills</strong> beyond the distribution in robot data; mainly transfers <strong>semantic/visual knowledge</strong>.</li>
<li><strong>Compute/latency</strong> costly; real-time control can bottleneck. Limited availability of strong open VLMs and convenient FT APIs.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=future-directions-from-the-text>Future Directions (from the text)<a href=#future-directions-from-the-text class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Acquire new skills from <strong>human videos</strong> or richer datasets.</li>
<li><strong>Quantization/distillation</strong> for faster/cheaper inference.</li>
<li>More <strong>open VLMs / FT APIs</strong> to make VLA models broadly buildable.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran, H., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K., Mordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T.-W. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J., Irpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess, D., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., & Han, K. (2023). RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control Proceedings of The 7th Conference on Robot Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v229/zitkovich23a.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag title="Vision-Language Models" class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/24/palm-e-review/>PaLM-E An Embodied Multimodal Language Model Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-24T10:33:27.131Z>August 24, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=palm-e>PaLM-E<a href=#palm-e class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>ViT (e.g., ViT-4B, ViT-22B) extracts image embeddings.</li>
<li>OSRT builds object-centric slot representations.</li>
<li>These are injected into the LLM embedding space (PaLM variants: 8B, 62B, 540B) for high-level abstraction and planning, with execution delegated to low-level policies (e.g., RT-1).</li>
</ul>
<p><img decoding=async loading=lazy alt="PaLM-E Architecture" src=/en/assets/images/palm-e-architecture-ca3f220adc8e114bc89a0589703866fb.png width=1284 height=650 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=core-idea>Core idea<a href=#core-idea class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Build <strong>embodied language models</strong> by <strong>injecting continuous sensor inputs</strong> (images, states, other modalities) directly into a <strong>pretrained LLM’s embedding space</strong>, linking <strong>words ↔ percepts</strong>.</li>
<li>Inputs are <strong>multimodal sentences</strong> that <strong>interleave</strong> text tokens with encoded visual/state tokens; outputs are <strong>text</strong> (answers or high-level plans).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=architecture--representations>Architecture & representations<a href=#architecture--representations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Start from a <strong>decoder-only, autoregressive LLM</strong> (PaLM) and <strong>condition on a prefix</strong> that mixes text and <strong>encoder-produced vectors</strong>.</li>
<li>Provide multiple encoder options:<!-- -->
<ul>
<li><strong>State vectors</strong> (simplest).</li>
<li><strong>ViT</strong> features with a learned <strong>projector ψ</strong> to match LLM embedding dimensionality.</li>
<li><strong>Object-centric, 3D-aware OSRT</strong> (neural scene representations). Supports <strong>entity-label tokens</strong> (<code>&lt;obj j></code>) so the model can refer to specific objects in generated plans.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=training-setup>Training setup<a href=#training-setup class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Train <strong>end-to-end</strong> (encoders + projector + optionally the LLM) to output <strong>sequential decisions as natural text</strong> or answers (VQA, captioning).</li>
<li>Dataset items contain <strong>(continuous observations, text sequence, prefix index)</strong>; loss is <strong>cross-entropy on non-prefix tokens</strong>.</li>
<li>Explore <strong>freezing the LLM</strong> (train encoders/projection only), and <strong>co-training</strong> across diverse tasks ("full mixture"; only ~9% is embodied data).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=planning--control-loop>Planning & control loop<a href=#planning--control-loop class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>For <strong>planning/control</strong>, PaLM-E emits <strong>textual subgoals/skills</strong> drawn from a small skill vocabulary; a separate <strong>low-level policy</strong> executes them.</li>
<li>The system runs <strong>closed-loop</strong>: execute → observe → (re)plan; PaLM-E acts as a <strong>high-level policy</strong> sequencing low-level skills.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=why-not-text-only-llms-or-affordance-only-grounding>Why not text-only LLMs or affordance-only grounding?<a href=#why-not-text-only-llms-or-affordance-only-grounding class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Prior work that feeds <strong>only text to the LLM</strong> (and uses external affordance models) is <strong>insufficient</strong> when <strong>spatial layout</strong> matters.</li>
<li>PaLM-E instead <strong>grounds inside the LLM</strong> by <strong>injecting continuous observations</strong>, enabling <strong>direct plan generation</strong> while leveraging the LLM’s world knowledge.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=environments--use-cases>Environments & use cases<a href=#environments--use-cases class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Three domains</strong>: <strong>TAMP</strong> (grasp/stack planning), <strong>Language-Table</strong> (multi-object tabletop pushing), <strong>Mobile manipulation</strong> (kitchen tasks).</li>
<li><strong>Use cases</strong> to test embodied reasoning: <strong>affordance prediction</strong>, <strong>failure detection</strong>, <strong>long-horizon planning</strong> (low-level policies from <strong>RT-1</strong>).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=results-high-level>Results (high level)<a href=#results-high-level class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Transfer via co-training</strong>: One model trained on mixed tasks/embodiments achieves <strong>higher performance</strong> than task-specialists; "full mixture" yields <code>>2×</code> gains (Fig. 3).</li>
<li><strong>Few-shot/data efficiency</strong>: Solves robotics tasks with <strong>very few examples</strong> (e.g., <strong>10–80</strong> for Language-Table, <strong>320</strong> for TAMP). <strong>OSRT</strong> further improves data efficiency.</li>
<li><strong>Mobile manipulation</strong>: End-to-end embodied planning works in real kitchens, robust to disturbances; PaLM-E beats <strong>PaLI (zero-shot)</strong> and <strong>QT-OPT/CLIP baselines</strong> on affordance/failure detection.</li>
<li><strong>General V+L</strong>: The <strong>562B</strong> generalist achieves <strong>state-of-the-art on OK-VQA</strong> and strong VQAv2/COCO without task-specific finetuning.</li>
<li><strong>Language retention & scaling</strong>: <strong>Freezing LLM</strong> preserves language ability but can struggle on some robotics tasks; <strong>unfrozen + scale up</strong> significantly reduces <strong>catastrophic forgetting</strong>.</li>
<li><strong>Emergent behaviors</strong>: <strong>Multimodal chain-of-thought</strong> and <strong>multi-image reasoning</strong> emerge in <strong>PaLM-E-562B</strong>, despite training on <strong>single-image prompts</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=takeaways>Takeaways<a href=#takeaways class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Injecting <strong>neural scene representations (OSRT)</strong> and <strong>entity-labeled multimodal tokens</strong> is <strong>effective</strong> even without massive embodied data.</li>
<li><strong>Diverse, joint training</strong> transfers vision-language knowledge <strong>into embodied decision-making</strong>, enabling <strong>data-efficient robot planning</strong>.</li>
<li>Two viable paths to retain language skills during multimodal finetuning:<!-- -->
<ol>
<li><strong>Freeze the LLM</strong>, train encoders (max language retention, sometimes weaker robotics),</li>
<li><strong>Unfreeze and scale</strong> the LLM (much less forgetting, strong embodied performance).</li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., & Florence, P. (2023). PaLM-E: An Embodied Multimodal Language Model Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v202/driess23a.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag title="Vision-Language Models" class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/24/rt-1-review/>RT-1, Robot Transformer 1 Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-24T05:52:19.257Z>August 24, 2025</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=rt-1>RT-1<a href=#rt-1 class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RT-1 discretizes robot actions into 256-bin tokens, creating a shared "action language" across robots.</li>
<li>It absorbs heterogeneous data from simulation and other robot morphologies without losing performance.</li>
<li>It generalizes robustly to new tasks, environments, and long-horizon scenarios (up to 50 steps).</li>
</ul>
<p><img decoding=async loading=lazy alt="RT-1 Architecture" src=/en/assets/images/rt-1-architecture-adc4a381e6221b91a20467516cdd1752.png width=820 height=1315 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=introduction--motivation>Introduction & Motivation<a href=#introduction--motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Leveraging large, diverse, task-agnostic datasets enables high performance in zero-shot or small task-specific settings.</li>
<li>Data collection and curation is a critical bottleneck in robotics ("the unsung hero" of large-scale ML).</li>
<li>Transformer-based controllers are powerful but inefficient for real-time robotics, requiring architectural adaptations.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=model--architecture>Model & Architecture<a href=#model--architecture class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RT-1 architecture: EfficientNet + FiLM layers + TokenLearner for compact vision-language tokenization.</li>
<li>Action tokenization: 11 action dimensions (7 arm, 3 base, 1 mode) discretized into 256 bins each.</li>
<li>This abstraction converts continuous robot actions into a discrete "token language", enabling cross-domain and cross-robot transfer.</li>
<li>Real-time feasibility: optimized design achieves ~3Hz inference speed suitable for real-world control.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=experiments--results>Experiments & Results<a href=#experiments--results class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=general-performance>General Performance<a href=#general-performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>RT-1 executes over 700 unique instructions at <strong>97% success rate</strong>.</li>
<li>On unseen instructions: <strong>76% success</strong>, outperforming next-best baseline by +24%.</li>
<li>Robustness: 83% success with distractors, 59% with background changes (significantly higher than baselines).</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=absorbing-simulation-data>Absorbing Simulation Data<a href=#absorbing-simulation-data class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Adding sim data does not degrade real-task performance.</li>
<li>Objects/tasks only seen in simulation: performance boosted <strong>23% ⇒ 87%</strong>.</li>
<li>Unseen instructions with sim objects: <strong>7% ⇒ 33%</strong>, showing strong sim-to-real domain transfer.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=absorbing-multi-robot-data>Absorbing Multi-Robot Data<a href=#absorbing-multi-robot-data class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Mixed RT-1 + Kuka datasets: only 2% drop in original tasks.</li>
<li>Bin-picking eval: RT-1 only 22% ⇒ mixed training 39% (almost 2×).</li>
<li>Kuka-only training: 0% on EDR robots ⇒ morphology transfer alone fails.</li>
<li>Mixed data enables RT-1 to <strong>leverage cross-robot experiences</strong> without explicit demonstrations.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=long-horizon-scenarios-saycan-integration>Long-Horizon Scenarios (SayCan Integration)<a href=#long-horizon-scenarios-saycan-integration class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Evaluated in two kitchens:<!-- -->
<ul>
<li>Kitchen1: 67% execution success.</li>
<li>Kitchen2 (novel environment): also 67% execution success.</li>
</ul>
</li>
<li>Outperforms Gato (0% in Kitchen2) and BC-Z (13% in Kitchen2).</li>
<li>Demonstrated execution of <strong>ultra-long tasks up to 50 steps</strong>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=data-quantity-vs-diversity>Data Quantity vs Diversity<a href=#data-quantity-vs-diversity class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p><img decoding=async loading=lazy alt="Data Diversity" src=/en/assets/images/rt-1-data-diversity-299befbcb9d0f7eb7912b666b02988e1.png width=733 height=376 class=img_f7zd></p>
<ul>
<li>Reducing dataset size ⇒ gradual performance/generalization decline.</li>
<li>Reducing task diversity ⇒ <strong>much sharper decline</strong>, especially in generalization.</li>
<li>Key takeaway: <strong>Data diversity is more critical than data quantity.</strong></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=conclusions--limitations>Conclusions & Limitations<a href=#conclusions--limitations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RT-1 proves large-scale data absorption and strong generalization in robotics.</li>
<li>Limitations:<!-- -->
<ul>
<li>Based on imitation learning ⇒ cannot surpass demonstrator performance.</li>
<li>Generalization limited to recombinations of known concepts ⇒ fails on truly novel motions.</li>
<li>Dataset is large but not dexterous (fine manipulation limited).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=future-directions>Future Directions<a href=#future-directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Enable non-experts to collect training data and prompt models for faster skill scaling.</li>
<li>Increase environmental diversity to strengthen robustness to backgrounds/environments.</li>
<li>Improve reaction speed and context retention via scalable attention and memory.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., & Hsu, J. (2022). Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag title="Vision-Language Models" class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><nav class=pagination-nav aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--next" href=/en/tags/vlm/page/2/><div class=pagination-nav__label>Older Entries</div></a></nav></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Support Me</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://www.buymeacoffee.com/LOUB2kN target=_blank rel="noopener noreferrer" style="cursor: pointer;">
                <img src=https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png style="height: auto !important;width: auto !important;">
              </a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Feeds</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://gracefullight.dev/rss.xml target=_blank rel="noopener noreferrer" class=footer__link-item>RSS<svg width=13.5 height=13.5 aria-hidden=true class=iconExternalLink_E7SL><use href=#theme-svg-external-link /></svg></a><li class=footer__item><a href=https://gracefullight.dev/atom.xml target=_blank rel="noopener noreferrer" class=footer__link-item>Atom<svg width=13.5 height=13.5 aria-hidden=true class=iconExternalLink_E7SL><use href=#theme-svg-external-link /></svg></a></ul></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2016 - 2023 Gracefullight. Built with Docusaurus.</div></div></div></footer></div>