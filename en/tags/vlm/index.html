<!doctype html><html lang=en dir=ltr class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>11 posts tagged with "vlm" | gracefullight.dev</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://gracefullight.dev/en/tags/vlm/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=ko><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true content=gcY9SiftHgQoJjBZ7IgwNNN5_atLPAX6kWb1nFVfa6E name=google-site-verification><meta data-rh=true content=65AD1E28C0D057CEB3C68FBC0293E55B name=msvalidate.01><meta data-rh=true content=d024c2837887f72dc7b3792b958be74d69ba9593 name=naver-site-verification><meta data-rh=true content=6672f93d837354fb name=yandex-verification><meta data-rh=true content=yZEdU1ABcR name=baidu-site-verification><meta data-rh=true content=uelupjqqsm5egzlhy1aev2rfxow5yt name=facebook-domain-verification><meta data-rh=true property=og:title content='11 posts tagged with "vlm" | gracefullight.dev'><meta data-rh=true name=docusaurus_tag content=blog_tags_posts><meta data-rh=true name=docsearch:docusaurus_tag content=blog_tags_posts><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://gracefullight.dev/en/tags/vlm/><link data-rh=true rel=alternate href=https://gracefullight.dev/tags/vlm/ hreflang=ko><link data-rh=true rel=alternate href=https://gracefullight.dev/en/tags/vlm/ hreflang=en><link data-rh=true rel=alternate href=https://gracefullight.dev/tags/vlm/ hreflang=x-default><link data-rh=true rel=preconnect href=https://RFS69RSYOJ-dsn.algolia.net crossorigin=anonymous><link rel=alternate type=application/rss+xml href=/en/rss.xml title="gracefullight.dev RSS Feed"><link rel=alternate type=application/atom+xml href=/en/atom.xml title="gracefullight.dev Atom Feed"><link rel=alternate type=application/json href=/en/feed.json title="gracefullight.dev JSON Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-E99DNE7S05"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-E99DNE7S05",{})</script><link rel=search type=application/opensearchdescription+xml title=gracefullight.dev href=/en/opensearch.xml><link href=/en/img/favicon-32x32.png rel=icon><link href=/en/manifest.json rel=manifest><meta content=#f28913 name=theme-color><meta content=yes name=mobile-web-app-capable><meta content=#f28913 name=apple-mobile-web-app-status-bar-style><link href=/en/img/apple-touch-icon.png rel=apple-touch-icon><link rel=preconnect href=https://pagead2.googlesyndication.com><script src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3004788392777865" async crossorigin=anonymous></script><link rel=preconnect href=https://www.clarity.ms><script>!function(t,e,n,a,c,i,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(i=e.createElement(a)).async=1,i.src="https://www.clarity.ms/tag/"+c,(r=e.getElementsByTagName(a)[0]).parentNode.insertBefore(i,r)}(window,document,"clarity","script","aongv9xgi6")</script><link rel=preconnect href=https://wcs.naver.net><script src=https://wcs.naver.net/wcslog.js async></script><script>if(!wcs_add)var wcs_add={};wcs_add.wa="156bc73a81e3bd0",window.wcs&&wcs_do()</script><link rel=preconnect href=https://cdn.channel.io><script>!function(){var n=window;if(n.ChannelIO)return(window.console.error||window.console.log||function(){})("ChannelIO script included twice.");var e=function(){e.c(arguments)};function t(){if(!n.ChannelIOInitialized){n.ChannelIOInitialized=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cdn.channel.io/plugin/ch-plugin-web.js",e.charset="UTF-8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}}e.q=[],e.c=function(n){e.q.push(n)},n.ChannelIO=e,"complete"===document.readyState?t():window.attachEvent?window.attachEvent("onload",t):(window.addEventListener("DOMContentLoaded",t,!1),window.addEventListener("load",t,!1))}(),ChannelIO("boot",{pluginKey:"0fd130ba-a1a6-4b7e-802a-e82a885a7fd8"})</script><link rel=preconnect href=https://static.cloudflareinsights.com><script src=https://static.cloudflareinsights.com/beacon.min.js defer data-cf-beacon='{"token":"c0899829e72b45e98dff77241127252c"}'></script><link href=https://cdn.jsdelivr.net rel=preconnect><script type=application/ld+json>{"@context":"http://schema.org","@type":"Person","email":"mailto:gracefullight.dev@gmail.com","image":"https://avatars.githubusercontent.com/u/11773683?v=4","jobTitle":"FullStack JavaScript Developer","logo":"https://gracefullight.dev/img/apple-touch-icon.png","name":"Eunkwang Shin","nationality":"Korean","sameAs":["https://github.com/gracefullight","https://linkedin.com/in/gracefullight"],"url":"https://gracefullight.dev"}</script><link rel=stylesheet crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css type=text/css><script src=/en/assets/js/runtime~main.07ab8dd4.js defer></script><script src=/en/assets/js/main.f25fd7e2.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/favicon-32x32.png><link rel=preload as=image href="https://avatars.githubusercontent.com/u/11773683?v=4"><div role=region aria-label="Skip to main content"><a class=skipToContent_soTP href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent__Vw_ themedComponent--light_S0y_"><img src=/en/img/favicon-32x32.png alt="gracefullight.dev blog logo" class="themedComponent__Vw_ themedComponent--dark_IFCp"></div><b class="navbar__title text--truncate">gracefullight.dev</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_LGSY><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/tags/vlm/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ko>한국어</a><li><a href=/en/tags/vlm/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a></ul></div><a class="navbar__item navbar__link" href=/en/archive/>Archives</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/tags/>Tags</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_YAYV colorModeToggle_T40I"><button class="clean-btn toggleButton_Qe_r toggleButtonDisabled_AVvq" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_T_L2 lightToggleIcon_FRTA"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_T_L2 darkToggleIcon_zlMh"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_T_L2 systemToggleIcon_h91M"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_SXBB><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_bGoB"><div class="container margin-vert--lg"><div class=row><aside class="col col--3"><nav class="sidebar_Pgmn thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_euex margin-bottom--md">Recent posts</div><div role=group><h3 class=yearGroupHeading_cosT>2025</h3><ul class="sidebarItemList_jela clean-list"><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/31/roboflamingo-review/>RoboFlamingo Review</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/29/open-vla-review/>OpenVLA Review</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/28/data-visualization-desicion-tree/>데이터 시각화 의사 결정 트리</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/vocab/vocab-ai-006/>Vocabulary for AI @006</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/28/developing-ml-systems/>Developing ML Systems</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/28/nonparametric-models/>Nonparametric Models</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/28/logistic-regression/>Logistic regression</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/27/octo-review/>Octo Review</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/26/introduction-to-ai-004/>Introduction to AI @004</a><li class=sidebarItem_drnv><a class=sidebarItemLink_hMRR href=/en/2025/08/25/fundamentals-of-software-development-005/>Fundamentals of software development @005</a></ul></div></nav></aside><main class="col col--7"><header class=margin-bottom--xl><h1>11 posts tagged with "vlm"</h1><a href=/en/tags/>View All Tags</a></header><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/31/roboflamingo-review/>RoboFlamingo Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-31T05:35:06.415Z>August 31, 2025</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=roboflamingo>RoboFlamingo<a href=#roboflamingo class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RoboFlamingo <strong>decouples vision-language understanding and control</strong>, using OpenFlamingo for perception and a lightweight policy head for sequential decision-making.</li>
<li>Unlike prior VLM-based approaches, it requires only <strong>small-scale imitation fine-tuning</strong> on language-conditioned manipulation data, without large-scale co-fine-tuning.</li>
<li>This design enables <strong>data-efficient, zero-shot generalizable, and deployable</strong> robot manipulation policies on modest compute resources.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=key-idea>Key Idea<a href=#key-idea class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Proposes <strong>RoboFlamingo</strong>, a simple framework to adapt existing VLMs for robotic manipulation with lightweight fine-tuning.</li>
<li>Built on <strong>OpenFlamingo</strong>, decoupling <strong>vision-language understanding</strong> from <strong>decision-making</strong>.</li>
<li>Pre-trained VLM handles <strong>language and visual comprehension</strong>, while a dedicated <strong>policy head models sequential history</strong>.</li>
<li>Fine-tuned only on <strong>language-conditioned manipulation datasets</strong> using imitation learning.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=advantages>Advantages<a href=#advantages class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Requires only a <strong>small amount of demonstrations</strong> to adapt to downstream manipulation tasks.</li>
<li>Provides <strong>open-loop control</strong> capability → deployable on low-performance platforms.</li>
<li>Can be trained/evaluated on a <strong>single GPU server</strong>, making it a cost-effective and accessible solution.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=benchmarks>Benchmarks<a href=#benchmarks class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Evaluated on <strong>CALVIN benchmark</strong> (34 tasks, 1000 instruction chains).</li>
<li>RoboFlamingo achieves <strong>2× performance improvements</strong> over previous state-of-the-art methods.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=performance>Performance<a href=#performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Imitation Learning</strong>: Outperforms all baselines across all metrics.</li>
<li><strong>Zero-shot Generalization</strong>:<!-- -->
<ul>
<li><strong>Vision</strong>: Stronger generalization in ABC→D setting.</li>
<li><strong>Language</strong>: Robust to GPT-4 generated synonymous instructions.</li>
</ul>
</li>
<li><strong>Ablation Studies</strong>:<!-- -->
<ul>
<li>Ignoring history (MLP w/o hist) gives worst results.</li>
<li>LSTM and GPT-based policy heads perform best (LSTM chosen as default).</li>
<li><strong>VL pre-training</strong> is crucial for downstream manipulation.</li>
<li><strong>Larger VLMs</strong> show better data efficiency.</li>
<li><strong>Instruction fine-tuning</strong> improves both seen and unseen tasks.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=flexibility-of-deployment>Flexibility of Deployment<a href=#flexibility-of-deployment class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Supports <strong>open-loop control</strong> by predicting entire action sequences with a single inference → reduces latency and test-time compute.</li>
<li>Direct open-loop use without retraining can degrade performance; mitigated with <strong>jump-step demonstrations</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Demonstrates that pre-trained VLMs enable <strong>data efficiency</strong> and strong <strong>zero-shot generalization</strong> in robotic manipulation.</li>
<li>RoboFlamingo is presented as an <strong>intuitive, efficient, and open solution</strong>, with high potential when combined with large-scale real robot data.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Li, X., Liu, M., Zhang, H., Yu, C., Xu, J., Wu, H., Cheang, C., Jing, Y., Zhang, W., & Liu, H. (2024). Vision-language foundation models as effective robot imitators. International Conference on Learning Representations (ICLR 2024), Vienna, Austria.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/29/open-vla-review/>OpenVLA Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-29T07:43:25.796Z>August 29, 2025</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=openvla>OpenVLA<a href=#openvla class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>OpenVLA is a 7B open-source VLA model built on Llama2 + DINOv2 + SigLIP, trained on 970k demos, achieving stronger generalization and robustness than closed RT-2-X (55B) and outperforming Diffusion Policy.</li>
<li>It introduces efficient adaptation via LoRA (1.4% params, 8× compute reduction) and 4-bit quantization (half memory, same accuracy), enabling fine-tuning and inference on consumer GPUs.</li>
<li>Limitations remain (single-image input, <code>&lt;90%</code> reliability, limited throughput), but OpenVLA provides the first open, scalable framework for generalist robot policies.</li>
</ul>
<p><img decoding=async loading=lazy alt="OpenVLA Architecture" src=/en/assets/images/open-vla-architecture-df4421cc82c1ebceca0ccf1061cd4593.png width=915 height=366 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=motivation>Motivation<a href=#motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Training robot policies from scratch struggles with robustness and generalization.</li>
<li>Fine-tuning <strong>vision-language-action (VLA)</strong> models offers reusable, generalizable visuomotor policies.</li>
<li>Barriers: prior VLAs are <strong>closed-source</strong>, lack best practices for adaptation, and need server-class hardware.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=model--training>Model & Training<a href=#model--training class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>OpenVLA</strong>: 7B parameters, open-source.</li>
<li>Built on <strong>Llama 2</strong> with fused <strong>DINOv2 + SigLIP</strong> vision encoders.</li>
<li>Trained on <strong>970k robot demonstrations</strong> from Open-X Embodiment dataset.</li>
<li>Represents robot actions as <strong>tokens</strong> (discretized into 256 bins, replacing unused Llama tokens).</li>
<li>Standard <strong>next-token prediction</strong> objective.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=architecture--approach>Architecture & Approach<a href=#architecture--approach class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>End-to-end fine-tuning of VLM to generate robot actions as tokens.</li>
<li>Differs from modular methods (e.g., Octo) that stitch separate encoders/decoders.</li>
<li>Vision features are obtained by encoding the same input image with both SigLIP and DINOv2, then channel-wise concatenated and passed through an MLP projector. This preserves SigLIP’s semantic alignment with language and DINOv2's spatial reasoning, giving the VLM richer multimodal context for manipulation tasks.</li>
<li>Uses Prismatic VLM backbone with multi-resolution features (spatial reasoning + semantics).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=performance>Performance<a href=#performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Outperforms closed <strong>RT-2-X (55B)</strong> by <strong>+16.5% task success</strong> with 7× fewer parameters.</li>
<li>Beats <strong>Diffusion Policy</strong> (from-scratch imitation learning) by <strong>+20.4%</strong> on multi-task language-grounded settings.</li>
<li>Demonstrates <strong>robust behaviors</strong> (distractor resistance, error recovery).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=efficiency>Efficiency<a href=#efficiency class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Introduces <strong>parameter-efficient fine-tuning</strong>:<!-- -->
<ul>
<li><strong>LoRA</strong> updates only 1.4% of parameters yet matches full fine-tuning.</li>
<li>Can fine-tune on a single A100 GPU in ~10–15 hours (8× compute reduction).</li>
</ul>
</li>
<li><strong>Quantization</strong>:<!-- -->
<ul>
<li>4-bit inference matches bfloat16 accuracy while halving memory footprint.</li>
<li>Runs at 3Hz on consumer GPUs (e.g., A5000, 16GB).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=evaluations>Evaluations<a href=#evaluations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Tested across <strong>29 tasks</strong> and multiple robots (WidowX, Google robot, Franka).</li>
<li>Strong generalization on:<!-- -->
<ul>
<li><strong>Visual</strong> (unseen backgrounds/distractors).</li>
<li><strong>Motion</strong> (new object positions/orientations).</li>
<li><strong>Physical</strong> (new object shapes/sizes).</li>
<li><strong>Semantic</strong> (unseen tasks, instructions).</li>
</ul>
</li>
<li>First generalist open-source VLA achieving <strong>≥50% success rate across all tested tasks</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=design-insights>Design Insights<a href=#design-insights class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Fine-tuning the vision encoder</strong> (vs. freezing) crucial for robotic control.</li>
<li>Higher image resolution (384px vs. 224px) adds 3× compute without performance gains.</li>
<li>Training required <strong>27 epochs</strong>, far more than typical VLM runs, to surpass 95% action token accuracy.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=limitations--future-work>Limitations & Future Work<a href=#limitations--future-work class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Supports only <strong>single-image observations</strong> (no proprioception, no history).</li>
<li>Inference throughput (~6Hz on RTX 4090) insufficient for high-frequency control (e.g., ALOHA at 50Hz).</li>
<li>Success rates remain below 90% in challenging tasks.</li>
<li>Open questions:<!-- -->
<ul>
<li>Impact of base VLM size on performance.</li>
<li>Benefits of co-training with Internet-scale data.</li>
<li>Best visual features for VLAs.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=contributions>Contributions<a href=#contributions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li>First <strong>open-source generalist VLA</strong> with strong performance.</li>
<li>Scalable <strong>end-to-end training</strong> pipeline (action-as-token).</li>
<li>Demonstrates <strong>LoRA + quantization</strong> for consumer-grade GPU adaptation.</li>
<li>Provides <strong>code, checkpoints, and data curation recipes</strong> to support future research.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E. P., Sanketi, P. R., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake, R., Sadigh, D., Levine, S., Liang, P., & Finn, C. (2025). OpenVLA: An Open-Source Vision-Language-Action Model Proceedings of The 8th Conference on Robot Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v270/kim25c.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/27/octo-review/>Octo Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-27T10:54:09.447Z>August 27, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=octo>Octo<a href=#octo class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Octo is a transformer-based policy with modular tokenizers (language via T5, images via CNN patches), blockwise masking, and readout tokens, trained on 800k multi-robot trajectories.</li>
<li>Actions are generated through a diffusion head that produces continuous, multimodal, chunked predictions, enabling precise control and broad generalization.</li>
<li>It achieves state-of-the-art zero-shot performance across 7 robots and allows efficient finetuning to new sensors and action spaces, while being fully open-source.</li>
</ul>
<table><thead><tr><th>Category<th>Simple Analogy<th>Actual Tokenization<tbody><tr><td><strong>Language</strong><td><code>[Sentence]</code><td><code>[l₁, l₂, l₃, …]</code> <br>→ multiple tokens from a tokenized sentence<tr><td><strong>Goal Image</strong><td><code>[Goal]</code><td><code>[g₁, g₂, g₃, …]</code> <br>→ image split into patches<tr><td><strong>Observation (time t)</strong><td><code>[Observation]</code><td><code>[oₜ¹, oₜ², oₜ³, …]</code> <br>→ camera frames/sensors tokenized into patches<tr><td><strong>Readout Token</strong><td><code>[ ]</code> (empty slot)<td><code>[TR,t]</code> <br>→ one per timestep, reserved for predicting actions</table>
<div class="language-bash codeBlockContainer_QFtC theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_vdmG><pre tabindex=0 class="prism-code language-bash codeBlock_CpxK thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines__0Nl><span class=token-line style=color:#393A34><span class="token plain">Time t-1: </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">l</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">g</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">o_</span><span class="token punctuation" style=color:#393A34>{</span><span class="token plain">t-1</span><span class="token punctuation" style=color:#393A34>}</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t-1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">Time t:   </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">l</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">g</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">o_t</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">     </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">Time t+1: </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">l</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">g</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">o_</span><span class="token punctuation" style=color:#393A34>{</span><span class="token plain">t+1</span><span class="token punctuation" style=color:#393A34>}</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t+1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t-1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">, </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">, </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">TR,t+1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain">  ──►  Diffusion </span><span class="token function" style=color:#d73a49>head</span><span class="token plain">  ──►  </span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain">a_t, a_</span><span class="token punctuation" style=color:#393A34>{</span><span class="token plain">t+1</span><span class="token punctuation" style=color:#393A34>}</span><span class="token plain">, …</span><span class="token punctuation" style=color:#393A34>]</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=motivation>Motivation<a href=#motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Traditional robot learning trains policies <strong>from scratch</strong> on robot/task-specific datasets → costly data collection, narrow generalization.</li>
<li><strong>Generalist Robot Policies (GRPs)</strong> pretrained on diverse robots/tasks can be <strong>finetuned with little in-domain data</strong> while generalizing broadly.</li>
<li>Real-world deployments face challenges across <strong>robot embodiments, sensor setups, action spaces, task specs, and environments</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=prior-grps--gaps>Prior GRPs & Gaps<a href=#prior-grps--gaps class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>GRPs aim for <strong>low-level visuomotor control</strong> across tasks, environments, and robotic systems.</li>
<li>Existing models often have <strong>restricted inputs (e.g., a single camera)</strong>, <strong>lack efficient finetuning to new domains</strong>, and importantly, <strong>largest models are not publicly available</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=contribution-what-is-octo>Contribution (What is Octo?)<a href=#contribution-what-is-octo class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Octo</strong>: a large transformer-based policy trained on <strong>800k trajectories</strong> from the Open X-Embodiment dataset.</li>
<li>Accepts <strong>language instructions or goal images</strong>, and can be <strong>finetuned within hours on consumer GPUs</strong> to new sensors and action spaces.</li>
<li><strong>First GRP</strong> to support <strong>effective finetuning to new observations and actions</strong> and to be <strong>fully open-source</strong> (training pipeline, checkpoints, data).</li>
<li>Novelty lies in combining: <strong>transformer backbone + language/goal image conditioning + diffusion head</strong> for expressive action distributions.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=architecture>Architecture<a href=#architecture class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Input tokenizers</strong>:<!-- -->
<ul>
<li>Language via pretrained <strong>T5-base</strong></li>
<li>Images via shallow CNN → patch tokens</li>
</ul>
</li>
<li><strong>Transformer backbone</strong>: processes unified token sequence.</li>
<li><strong>Blockwise masking + Readout tokens</strong>:<!-- -->
<ul>
<li>Nonexistent modalities are masked</li>
<li>Readout tokens <em>only attend</em> to past observations/tasks, not vice versa</li>
</ul>
</li>
<li><strong>Diffusion action head</strong>: predicts <strong>continuous, multimodal, chunked actions</strong>.</li>
<li><strong>Modularity</strong>: new sensors/outputs can be added by only training lightweight encoders or heads; pretrained backbone remains unchanged.</li>
</ul>
<p><img decoding=async loading=lazy alt="Octo Architecture" src=/en/assets/images/octo-architecture-49b9dd94643695f0566e74ac5a0801bd.png width=803 height=415 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=training-data--objective>Training Data & Objective<a href=#training-data--objective class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Mixture of <strong>25 heterogeneous robot datasets</strong>: diverse robots, sensors (with/without wrist cams), labels (with/without language).</li>
<li><strong>Conditional diffusion decoding</strong> predicts continuous, multimodal action distributions.<!-- -->
<ul>
<li>Transformer runs <strong>one forward pass</strong>; denoising steps are contained in the small diffusion head.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=experiments>Experiments<a href=#experiments class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Evaluated on <strong>7 robotic platforms across 4 institutions</strong>.</li>
<li>Key questions:<!-- -->
<ol>
<li>Zero-shot multi-robot control?</li>
<li>Do Octo weights improve finetuning vs. scratch or standard pretrained representations?</li>
<li>Which design choices matter for generalist robot policies?</li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=results>Results<a href=#results class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Achieves <strong>state-of-the-art zero-shot multi-robot control</strong>, competitive with RT-1-X and RT-2-X.</li>
<li>Provides a <strong>versatile policy initialization</strong>: significantly outperforms baselines for <strong>data-efficient finetuning</strong> to new obs/action spaces.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=limitations--future-work>Limitations / Future Work<a href=#limitations--future-work class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Needs <strong>better language conditioning</strong>, <strong>improved wrist camera support</strong>, and <strong>data beyond optimal demonstrations</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=one-line-takeaway>One-line Takeaway<a href=#one-line-takeaway class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Octo = modular, efficient, open-source GRP</strong>:<br>
<!-- -->A transformer + diffusion policy trained on large-scale multi-robot data that <strong>adapts quickly with little in-domain data</strong> to new sensors and action spaces, enabling broad generalization.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Mees, O., Ghosh, D., Pertsch, K., Black, K., Walke, H. R., Dasari, S., Hejna, J., Kreiman, T., Xu, C., & Luo, J. (2024). Octo: An open-source generalist robot policy. First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/24/rt-2-review/>RT-2, Robotic Transformer 2 Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-24T13:40:19.433Z>August 24, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><ul>
<li>Trains a <strong>Vision-Language-Action (VLA)</strong> model by co-fine-tuning web-scale VLMs with robot trajectories, and <strong>treats robot actions as text tokens</strong>.</li>
<li>Yields <strong>strong generalization</strong> and <strong>emergent capabilities</strong> (symbol understanding, reasoning, human recognition) beyond what appears in robot data.</li>
<li>Runs in <strong>direct closed-loop control</strong>; largest evaluated model (55B) executes at ~1–3 Hz via a cloud (multi-TPU) inference setup.</li>
</ul>
<p><img decoding=async loading=lazy alt="RT-2 Architecture" src=/en/assets/images/rt-2-architecture-dd9ff6e2cae963c14c20742089b822df.png width=1674 height=748 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=what-rt-2-is>What RT-2 Is<a href=#what-rt-2-is class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>A family of VLA models (RT-2-PaLI-X, RT-2-PaLM-E) that fine-tune large VLMs on robot trajectories to output <strong>low-level actions</strong>.</li>
<li>Target: <strong>generalizable, semantically aware</strong> manipulation policies that map images + instructions → actions end-to-end.</li>
<li>RT-2 does <strong>not rely on a restricted 2D action space or calibrated cameras</strong>.</li>
<li>The <strong>unified output space</strong> lets language and action tokens share the same model weights, without action-only layers.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=core-recipe>Core Recipe<a href=#core-recipe class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Directly train</strong> open-vocabulary VQA/dialogue VLMs to <strong>output robot actions</strong> while they still solve standard vision-language tasks.</li>
<li>Build on RT-1 protocol/data, but replace the policy backbone with a <strong>large VLM</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=action-as-language-tokenization>Action as Language (Tokenization)<a href=#action-as-language-tokenization class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Discretize continuous action dims (Δpos/Δrot, gripper, terminate) into <strong>256 bins</strong>; represent each dimension with an <strong>integer token</strong>.</li>
<li><strong>PaLI-X</strong>: reuse numeric tokens (<code>≤1000</code>). <strong>PaLM-E</strong>: overwrite <strong>256 least-frequent tokens</strong> as action vocabulary (<strong>symbol tuning</strong>).</li>
<li>Form a single output string per step (e.g., <code>terminate Δposx Δposy Δposz Δrotx Δroty Δrotz gripper</code>).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=co-fine-tuning--output-constraint>Co-Fine-Tuning & Output Constraint<a href=#co-fine-tuning--output-constraint class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Mix robot data with original web VQA/caption data</strong> in training batches (up-weight robot samples) to prevent forgetting and improve generalization.</li>
<li>During decoding on robot tasks, <strong>restrict sampling to valid action tokens</strong> so outputs are always executable.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=closed-loop-control--real-time-inference>Closed-Loop Control & Real-Time Inference<a href=#closed-loop-control--real-time-inference class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RT-2 is trained and deployed for <strong>direct closed-loop control</strong> (camera → action → camera …), not just high-level planning.</li>
<li>For large models, inference runs via a <strong>multi-TPU cloud service</strong>; <strong>RT-2-PaLI-X-55B</strong> reaches <strong>~1–3 Hz</strong>; smaller models ~5 Hz.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=generalization--benchmarks>Generalization & Benchmarks<a href=#generalization--benchmarks class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Matches RT-1 on seen tasks but <strong>far exceeds</strong> baselines on <strong>unseen objects/backgrounds/environments</strong> (~<strong>2×</strong> vs RT-1/MOO; up to <strong>~6×</strong> vs others).</li>
<li>Open-source <strong>Language-Table</strong> sim: co-fine-tuned <strong>PaLI-3B</strong> outperforms baselines, showing the approach transfers to other robots/sims.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=emergent-capabilities>Emergent Capabilities<a href=#emergent-capabilities class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Symbol understanding</strong> (e.g., “move apple to 3 / heart / star”).</li>
<li><strong>Reasoning</strong> (visual matching, simple math like “sum of two plus one”, <strong>multilingual</strong> commands).</li>
<li><strong>Human recognition</strong> (e.g., “person with glasses”); none of these were present as low-level actions in robot data.</li>
<li><strong>Chain-of-thought (CoT) variant</strong> adds a <strong>Plan</strong> step before actions → supports <strong>multi-stage semantic reasoning</strong> (e.g., pick a rock as an improvised hammer; pick an energy drink for a tired person).</li>
</ul>
<p><img decoding=async loading=lazy alt=rt-2-cot src=/en/assets/images/rt-2-cot-eb2bee68ad29cc277f1a214c22064f0e.png width=1748 height=934 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=scaling--ablations>Scaling & Ablations<a href=#scaling--ablations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>From-scratch</strong> training (even 5B) performs poorly; <strong>fine-tuning</strong> helps; <strong>co-fine-tuning</strong> helps <strong>most</strong>.</li>
<li><strong>Bigger models</strong> (<code>55B > 5B</code>) generalize better.</li>
<li>PaLM-E variant shows an edge on <strong>math reasoning</strong>; PaLI-X stronger on symbols/vision reasoning on average.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=limitations>Limitations<a href=#limitations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Does <strong>not</strong> learn fundamentally <strong>new motor skills</strong> beyond the distribution in robot data; mainly transfers <strong>semantic/visual knowledge</strong>.</li>
<li><strong>Compute/latency</strong> costly; real-time control can bottleneck. Limited availability of strong open VLMs and convenient FT APIs.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=future-directions-from-the-text>Future Directions (from the text)<a href=#future-directions-from-the-text class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Acquire new skills from <strong>human videos</strong> or richer datasets.</li>
<li><strong>Quantization/distillation</strong> for faster/cheaper inference.</li>
<li>More <strong>open VLMs / FT APIs</strong> to make VLA models broadly buildable.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran, H., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K., Mordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T.-W. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J., Irpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess, D., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., & Han, K. (2023). RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control Proceedings of The 7th Conference on Robot Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v229/zitkovich23a.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/24/palm-e-review/>PaLM-E An Embodied Multimodal Language Model Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-24T10:33:27.131Z>August 24, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=palm-e>PaLM-E<a href=#palm-e class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>ViT (e.g., ViT-4B, ViT-22B) extracts image embeddings.</li>
<li>OSRT builds object-centric slot representations.</li>
<li>These are injected into the LLM embedding space (PaLM variants: 8B, 62B, 540B) for high-level abstraction and planning, with execution delegated to low-level policies (e.g., RT-1).</li>
</ul>
<p><img decoding=async loading=lazy alt="PaLM-E Architecture" src=/en/assets/images/palm-e-architecture-ca3f220adc8e114bc89a0589703866fb.png width=1284 height=650 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=core-idea>Core idea<a href=#core-idea class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Build <strong>embodied language models</strong> by <strong>injecting continuous sensor inputs</strong> (images, states, other modalities) directly into a <strong>pretrained LLM’s embedding space</strong>, linking <strong>words ↔ percepts</strong>.</li>
<li>Inputs are <strong>multimodal sentences</strong> that <strong>interleave</strong> text tokens with encoded visual/state tokens; outputs are <strong>text</strong> (answers or high-level plans).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=architecture--representations>Architecture & representations<a href=#architecture--representations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Start from a <strong>decoder-only, autoregressive LLM</strong> (PaLM) and <strong>condition on a prefix</strong> that mixes text and <strong>encoder-produced vectors</strong>.</li>
<li>Provide multiple encoder options:<!-- -->
<ul>
<li><strong>State vectors</strong> (simplest).</li>
<li><strong>ViT</strong> features with a learned <strong>projector ψ</strong> to match LLM embedding dimensionality.</li>
<li><strong>Object-centric, 3D-aware OSRT</strong> (neural scene representations). Supports <strong>entity-label tokens</strong> (<code>&lt;obj j></code>) so the model can refer to specific objects in generated plans.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=training-setup>Training setup<a href=#training-setup class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Train <strong>end-to-end</strong> (encoders + projector + optionally the LLM) to output <strong>sequential decisions as natural text</strong> or answers (VQA, captioning).</li>
<li>Dataset items contain <strong>(continuous observations, text sequence, prefix index)</strong>; loss is <strong>cross-entropy on non-prefix tokens</strong>.</li>
<li>Explore <strong>freezing the LLM</strong> (train encoders/projection only), and <strong>co-training</strong> across diverse tasks ("full mixture"; only ~9% is embodied data).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=planning--control-loop>Planning & control loop<a href=#planning--control-loop class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>For <strong>planning/control</strong>, PaLM-E emits <strong>textual subgoals/skills</strong> drawn from a small skill vocabulary; a separate <strong>low-level policy</strong> executes them.</li>
<li>The system runs <strong>closed-loop</strong>: execute → observe → (re)plan; PaLM-E acts as a <strong>high-level policy</strong> sequencing low-level skills.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=why-not-text-only-llms-or-affordance-only-grounding>Why not text-only LLMs or affordance-only grounding?<a href=#why-not-text-only-llms-or-affordance-only-grounding class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Prior work that feeds <strong>only text to the LLM</strong> (and uses external affordance models) is <strong>insufficient</strong> when <strong>spatial layout</strong> matters.</li>
<li>PaLM-E instead <strong>grounds inside the LLM</strong> by <strong>injecting continuous observations</strong>, enabling <strong>direct plan generation</strong> while leveraging the LLM’s world knowledge.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=environments--use-cases>Environments & use cases<a href=#environments--use-cases class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Three domains</strong>: <strong>TAMP</strong> (grasp/stack planning), <strong>Language-Table</strong> (multi-object tabletop pushing), <strong>Mobile manipulation</strong> (kitchen tasks).</li>
<li><strong>Use cases</strong> to test embodied reasoning: <strong>affordance prediction</strong>, <strong>failure detection</strong>, <strong>long-horizon planning</strong> (low-level policies from <strong>RT-1</strong>).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=results-high-level>Results (high level)<a href=#results-high-level class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Transfer via co-training</strong>: One model trained on mixed tasks/embodiments achieves <strong>higher performance</strong> than task-specialists; "full mixture" yields <code>>2×</code> gains (Fig. 3).</li>
<li><strong>Few-shot/data efficiency</strong>: Solves robotics tasks with <strong>very few examples</strong> (e.g., <strong>10–80</strong> for Language-Table, <strong>320</strong> for TAMP). <strong>OSRT</strong> further improves data efficiency.</li>
<li><strong>Mobile manipulation</strong>: End-to-end embodied planning works in real kitchens, robust to disturbances; PaLM-E beats <strong>PaLI (zero-shot)</strong> and <strong>QT-OPT/CLIP baselines</strong> on affordance/failure detection.</li>
<li><strong>General V+L</strong>: The <strong>562B</strong> generalist achieves <strong>state-of-the-art on OK-VQA</strong> and strong VQAv2/COCO without task-specific finetuning.</li>
<li><strong>Language retention & scaling</strong>: <strong>Freezing LLM</strong> preserves language ability but can struggle on some robotics tasks; <strong>unfrozen + scale up</strong> significantly reduces <strong>catastrophic forgetting</strong>.</li>
<li><strong>Emergent behaviors</strong>: <strong>Multimodal chain-of-thought</strong> and <strong>multi-image reasoning</strong> emerge in <strong>PaLM-E-562B</strong>, despite training on <strong>single-image prompts</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=takeaways>Takeaways<a href=#takeaways class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Injecting <strong>neural scene representations (OSRT)</strong> and <strong>entity-labeled multimodal tokens</strong> is <strong>effective</strong> even without massive embodied data.</li>
<li><strong>Diverse, joint training</strong> transfers vision-language knowledge <strong>into embodied decision-making</strong>, enabling <strong>data-efficient robot planning</strong>.</li>
<li>Two viable paths to retain language skills during multimodal finetuning:<!-- -->
<ol>
<li><strong>Freeze the LLM</strong>, train encoders (max language retention, sometimes weaker robotics),</li>
<li><strong>Unfreeze and scale</strong> the LLM (much less forgetting, strong embodied performance).</li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., & Florence, P. (2023). PaLM-E: An Embodied Multimodal Language Model Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v202/driess23a.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/24/rt-1-review/>RT-1, Robot Transformer 1 Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-24T05:52:19.257Z>August 24, 2025</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=rt-1>RT-1<a href=#rt-1 class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RT-1 discretizes robot actions into 256-bin tokens, creating a shared "action language" across robots.</li>
<li>It absorbs heterogeneous data from simulation and other robot morphologies without losing performance.</li>
<li>It generalizes robustly to new tasks, environments, and long-horizon scenarios (up to 50 steps).</li>
</ul>
<p><img decoding=async loading=lazy alt="RT-1 Architecture" src=/en/assets/images/rt-1-architecture-adc4a381e6221b91a20467516cdd1752.png width=820 height=1315 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=introduction--motivation>Introduction & Motivation<a href=#introduction--motivation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Leveraging large, diverse, task-agnostic datasets enables high performance in zero-shot or small task-specific settings.</li>
<li>Data collection and curation is a critical bottleneck in robotics ("the unsung hero" of large-scale ML).</li>
<li>Transformer-based controllers are powerful but inefficient for real-time robotics, requiring architectural adaptations.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=model--architecture>Model & Architecture<a href=#model--architecture class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RT-1 architecture: EfficientNet + FiLM layers + TokenLearner for compact vision-language tokenization.</li>
<li>Action tokenization: 11 action dimensions (7 arm, 3 base, 1 mode) discretized into 256 bins each.</li>
<li>This abstraction converts continuous robot actions into a discrete "token language", enabling cross-domain and cross-robot transfer.</li>
<li>Real-time feasibility: optimized design achieves ~3Hz inference speed suitable for real-world control.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=experiments--results>Experiments & Results<a href=#experiments--results class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=general-performance>General Performance<a href=#general-performance class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>RT-1 executes over 700 unique instructions at <strong>97% success rate</strong>.</li>
<li>On unseen instructions: <strong>76% success</strong>, outperforming next-best baseline by +24%.</li>
<li>Robustness: 83% success with distractors, 59% with background changes (significantly higher than baselines).</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=absorbing-simulation-data>Absorbing Simulation Data<a href=#absorbing-simulation-data class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Adding sim data does not degrade real-task performance.</li>
<li>Objects/tasks only seen in simulation: performance boosted <strong>23% ⇒ 87%</strong>.</li>
<li>Unseen instructions with sim objects: <strong>7% ⇒ 33%</strong>, showing strong sim-to-real domain transfer.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=absorbing-multi-robot-data>Absorbing Multi-Robot Data<a href=#absorbing-multi-robot-data class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Mixed RT-1 + Kuka datasets: only 2% drop in original tasks.</li>
<li>Bin-picking eval: RT-1 only 22% ⇒ mixed training 39% (almost 2×).</li>
<li>Kuka-only training: 0% on EDR robots ⇒ morphology transfer alone fails.</li>
<li>Mixed data enables RT-1 to <strong>leverage cross-robot experiences</strong> without explicit demonstrations.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=long-horizon-scenarios-saycan-integration>Long-Horizon Scenarios (SayCan Integration)<a href=#long-horizon-scenarios-saycan-integration class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Evaluated in two kitchens:<!-- -->
<ul>
<li>Kitchen1: 67% execution success.</li>
<li>Kitchen2 (novel environment): also 67% execution success.</li>
</ul>
</li>
<li>Outperforms Gato (0% in Kitchen2) and BC-Z (13% in Kitchen2).</li>
<li>Demonstrated execution of <strong>ultra-long tasks up to 50 steps</strong>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=data-quantity-vs-diversity>Data Quantity vs Diversity<a href=#data-quantity-vs-diversity class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p><img decoding=async loading=lazy alt="Data Diversity" src=/en/assets/images/rt-1-data-diversity-299befbcb9d0f7eb7912b666b02988e1.png width=733 height=376 class=img_f7zd></p>
<ul>
<li>Reducing dataset size ⇒ gradual performance/generalization decline.</li>
<li>Reducing task diversity ⇒ <strong>much sharper decline</strong>, especially in generalization.</li>
<li>Key takeaway: <strong>Data diversity is more critical than data quantity.</strong></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=conclusions--limitations>Conclusions & Limitations<a href=#conclusions--limitations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>RT-1 proves large-scale data absorption and strong generalization in robotics.</li>
<li>Limitations:<!-- -->
<ul>
<li>Based on imitation learning ⇒ cannot surpass demonstrator performance.</li>
<li>Generalization limited to recombinations of known concepts ⇒ fails on truly novel motions.</li>
<li>Dataset is large but not dexterous (fine manipulation limited).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=future-directions>Future Directions<a href=#future-directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Enable non-experts to collect training data and prompt models for faster skill scaling.</li>
<li>Increase environmental diversity to strengthen robustness to backgrounds/environments.</li>
<li>Improve reaction speed and context retention via scalable attention and memory.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., & Hsu, J. (2022). Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/24/do-as-i-can-not-as-i-say-review/>Do As I Can, Not As I Say Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-24T02:04:59.631Z>August 24, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=say-can>Say Can<a href=#say-can class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>The core of SayCan is using an LLM to decompose high-level instructions into low-level skills, and reinforcement-learned affordance value functions to evaluate whether each skill is feasible in the current environment.</li>
<li>The Say × Can structure is modular: different LLMs or affordance models can be swapped in, but each module’s inherent biases are carried into the system.</li>
<li>To mitigate limitations, loop-based strategies are essential — CoT and RLHF provide feedback loops for LLMs, while closed-loop feedback enables affordance functions to adapt during execution.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=motivation-why-llms-alone-fall-short>Motivation (Why LLMs alone fall short)<a href=#motivation-why-llms-alone-fall-short class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>LLMs lack embodiment.</strong> They haven’t acted in the physical world, so using them for decision-making on a specific robot is unreliable.</li>
<li><strong>LLMs don’t know robot’s abilities or state.</strong> They may split instructions into subtasks, but without context of capabilities and environment, plans can be irrelevant.</li>
<li><strong>Prompting alone isn’t enough.</strong> Structured prompts help, but they don’t guarantee admissible or executable steps.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=core-proposal-what-saycan-adds>Core Proposal (What SayCan adds)<a href=#core-proposal-what-saycan-adds class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Ground with pretrained skills.</strong> Constrain LLM to propose actions that the robot can actually perform in context.</li>
<li><strong>Say × Can factorization.</strong>
<ul>
<li><strong>Say (task-grounding):</strong> LLM estimates relevance of each skill to the instruction.</li>
<li><strong>Can (world-grounding):</strong> Affordance functions estimate probability of success from current state.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=probabilistic-formulation>Probabilistic Formulation<a href=#probabilistic-formulation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Two probabilities multiplied:</strong>
<ul>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>p</mi><mo stretchy=false>(</mo><msub><mi mathvariant=normal>ℓ</mi><mi>π</mi></msub><mi mathvariant=normal>∣</mi><mi>i</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>p(\ell_\pi|i)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal">p</span><span class=mopen>(</span><span class=mord><span class=mord>ℓ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>π</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mord>∣</span><span class="mord mathnormal">i</span><span class=mclose>)</span></span></span></span>: LLM score of relevance.</li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>p</mi><mo stretchy=false>(</mo><msub><mi>c</mi><mi>π</mi></msub><mi mathvariant=normal>∣</mi><mi>s</mi><mo separator=true>,</mo><msub><mi mathvariant=normal>ℓ</mi><mi>π</mi></msub><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>p(c_\pi|s,\ell_\pi)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal">p</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">c</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>π</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mord>∣</span><span class="mord mathnormal">s</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class=mord>ℓ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>π</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>: affordance score of success.</li>
<li>Select: <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>π</mi><mo>=</mo><mi>arg</mi><mo>⁡</mo><mi>max</mi><mo>⁡</mo><mi>p</mi><mo stretchy=false>(</mo><msub><mi>c</mi><mi>π</mi></msub><mi mathvariant=normal>∣</mi><mi>s</mi><mo separator=true>,</mo><msub><mi mathvariant=normal>ℓ</mi><mi>π</mi></msub><mo stretchy=false>)</mo><mtext> </mtext><mi>p</mi><mo stretchy=false>(</mo><msub><mi mathvariant=normal>ℓ</mi><mi>π</mi></msub><mi mathvariant=normal>∣</mi><mi>i</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>\pi = \arg\max p(c_\pi|s,\ell_\pi)\,p(\ell_\pi|i)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.03588em>π</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mop>ar<span style=margin-right:0.01389em>g</span></span><span class=mspace style=margin-right:0.1667em></span><span class=mop>max</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal">p</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">c</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>π</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mord>∣</span><span class="mord mathnormal">s</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class=mord>ℓ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>π</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mclose>)</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal">p</span><span class=mopen>(</span><span class=mord><span class=mord>ℓ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>π</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mord>∣</span><span class="mord mathnormal">i</span><span class=mclose>)</span></span></span></span>.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=planning-procedure>Planning Procedure<a href=#planning-procedure class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Planning is structured as a <strong>dialog</strong>: user gives high-level instruction, LLM produces a step sequence, loop until "done."</li>
<li>Benefit: <strong>Interpretability</strong>—scores provide transparency.</li>
<li>Caveat: Without affordances, chosen steps may be irrelevant to the current scene.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=affordances-via-rl>Affordances via RL<a href=#affordances-via-rl class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Affordance = value function.</strong> In sparse reward settings, value ≈ success probability.</li>
<li>TD RL and MDP formalism used to learn <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>Q</mi><mi>π</mi></msub><mo stretchy=false>(</mo><mi>s</mi><mo separator=true>,</mo><mi>a</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>Q_\pi(s,a)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mord><span class="mord mathnormal">Q</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>π</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord mathnormal">s</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal">a</span><span class=mclose>)</span></span></span></span>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=implementation>Implementation<a href=#implementation class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Skill training:</strong>
<ul>
<li>BC-Z (behavioral cloning) and MT-Opt (reinforcement learning).</li>
<li>Multi-task BC/RL amortizes training cost.</li>
</ul>
</li>
<li><strong>Language conditioning:</strong> Pretrained sentence encoder frozen, text embeddings as input.</li>
<li><strong>Action space:</strong> 6-DoF end-effector, gripper open/close, base x-y & yaw deltas, terminate.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=metrics>Metrics<a href=#metrics class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Plan success rate:</strong> 2/3 human raters agree that the plan is valid.</li>
<li><strong>Execution success rate:</strong> 2/3 raters agree robot achieved the task.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=key-results>Key Results<a href=#key-results class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Grounding nearly doubles performance</strong> vs non-grounded baselines.</li>
<li><strong>Understands sequence order</strong> (approach → pick → bring).</li>
<li><strong>Failures:</strong> Long-horizon tasks (early termination), negation, ambiguous references.</li>
<li><strong>Error split:</strong> ~65% LLM, 35% affordance.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ablations>Ablations<a href=#ablations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Remove LLM (task-grounding):</strong>
<ul>
<li>BC-NL: 0% all tasks.</li>
<li>BC-USE: 60% on single primitives, 0% otherwise.</li>
</ul>
</li>
<li><strong>Remove affordances (world-grounding):</strong>
<ul>
<li>No-VF: 67%, Generative: 74% vs 84% (SayCan).</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=scaling--models>Scaling & Models<a href=#scaling--models class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>PaLM > FLAN.</strong> PaLM-SayCan achieves 84% plan / 74% execute.</li>
<li>Stronger LMs improve robotics performance.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=extensibility>Extensibility<a href=#extensibility class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Add new skills easily:</strong> register skill, affordance, prompt example.</li>
<li><strong>Chain-of-Thought:</strong> Add "Explanation" → helps with negation and reasoning-heavy queries.</li>
<li><strong>Multilingual:</strong> Almost no performance drop (English, Chinese, French, Spanish).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=open-source-variant>Open-Source Variant<a href=#open-source-variant class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>CLIPort for pick-and-place.</li>
<li>Affordances approximated by ViLD open-vocabulary object detector.</li>
<li>GPT-3 as language model.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=limitations--future-work>Limitations & Future Work<a href=#limitations--future-work class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Limits:</strong> Inherits LLM biases; skill library is bottleneck; hard to react to skill failures.</li>
<li><strong>Closed-loop extensions:</strong> Huang et al. use environment feedback + inner monologue for replanning.</li>
<li><strong>Future directions:</strong> Expand/robustify skills, explore new grounding sources (non-robotic), test if natural language is the right ontology, combine planning + language, use LMs for policy pretraining.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>ichter, b., Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog, A., Ho, D., Ibarz, J., Irpan, A., Jang, E., Julian, R., Kalashnikov, D., Levine, S., Lu, Y., Parada, C., Rao, K., Sermanet, P., Toshev, A. T., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Yan, M., Brown, N., Ahn, M., Cortes, O., Sievers, N., Tan, C., Xu, S., Reyes, D., Rettinghouse, J., Quiambao, J., Pastor, P., Luu, L., Lee, K.-H., Kuang, Y., Jesmonth, S., Joshi, N. J., Jeffrey, K., Ruano, R. J., Hsu, J., Gopalakrishnan, K., David, B., Zeng, A., & Fu, C. K. (2023). Do As I Can, Not As I Say: Grounding Language in Robotic Affordances Proceedings of The 6th Conference on Robot Learning, Proceedings of Machine Learning Research. <code>https://proceedings.mlr.press/v205/ichter23a.html</code></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/21/CLIPort-review/>CLIPort Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-21T13:02:02.850Z>August 21, 2025</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=key-idea>Key Idea<a href=#key-idea class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>CLIPort proposes a <strong>two-stream architecture</strong> for vision-based manipulation:<!-- -->
<ul>
<li><strong>Semantic pathway (what):</strong> leverages CLIP for broad semantic understanding.</li>
<li><strong>Spatial pathway (where):</strong> leverages Transporter for fine-grained spatial reasoning.</li>
</ul>
</li>
<li>This design is <strong>inspired by the two-stream hypothesis in cognitive psychology</strong> (ventral/dorsal pathways).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=framework-contributions>Framework Contributions<a href=#framework-contributions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Benchmark Extension:</strong> Expanded the Ravens benchmark with language-grounding tasks for manipulation.</li>
<li><strong>Two-Stream Architecture:</strong> Uses pre-trained vision-language models (CLIP) to condition precise manipulation policies with language goals.</li>
<li><strong>Empirical Results:</strong> Demonstrates robustness on diverse manipulation tasks, including multi-task settings and real-robot experiments.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=architectural-design>Architectural Design<a href=#architectural-design class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>CLIPort integrates <strong>semantic (CLIP)</strong> with <strong>spatial (Transporter)</strong> features by lateral fusion.</li>
<li>The semantic stream is conditioned with <strong>language features from CLIP’s text encoder</strong> and fused with intermediate spatial features.</li>
<li>Enables <strong>end-to-end learning of affordance predictions</strong> (pick-and-place) without explicit object models, segmentations, or symbolic states.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=key-insights>Key Insights<a href=#key-insights class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Formulates manipulation as <strong>action detection</strong> (where to act), instead of object detection.</li>
<li><strong>Tabula rasa systems</strong> (like plain Transporter) require new demonstrations for every goal/task. CLIPort addresses this with a <strong>strong semantic prior</strong> (from CLIP) to generalize across tasks and concepts.</li>
<li><strong>Language-conditioned policies</strong> provide an intuitive interface for specifying goals and transferring concepts.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=experimental-results>Experimental Results<a href=#experimental-results class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li><strong>Simulation (PyBullet, UR5 robot with suction gripper):</strong>
<ul>
<li>10 language-conditioned tasks with thousands of unique instances.</li>
<li>Multi-task CLIPort outperformed or matched single-task models, even with fewer demonstrations.</li>
<li>CLIP-only or Transporter-only baselines saturate, while CLIPort exceeds 90% success with just 100 demos.</li>
</ul>
</li>
<li><strong>Generalization:</strong>
<ul>
<li>CLIPort generalizes to <strong>unseen attributes</strong> (e.g., new colors, shapes, object categories).</li>
<li>Struggles with <strong>completely novel attributes</strong> (e.g., “pink” or “orange” never seen in training).</li>
</ul>
</li>
<li><strong>Real-World Robot Experiments (Franka Panda):</strong>
<ul>
<li>Achieved ~70% success on real tasks with just 179 demonstrations.</li>
<li>Performance trends were consistent with simulation, validating sim-to-real transfer.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>CLIPort shows that <strong>multi-task, language-conditioned policies</strong> generalize across tasks better than object-centric or tabula rasa methods.</li>
<li>With <strong>action abstraction</strong> and <strong>spatio-semantic priors</strong>, end-to-end models can learn new skills <strong>without requiring hand-engineered pipelines</strong>.</li>
<li>Limitations remain for <strong>dexterous 6-DoF manipulation</strong> and complex continuous control.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Shridhar, M., Manuelli, L., & Fox, D. (2022). Cliport: What and where pathways for robotic manipulation. Conference on robot learning.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/18/mitigating-hallucinations-on-object-attributesusing-multiview-images-review/>Mitigating Hallucinations on Object Attributes Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-18T05:44:39.579Z>August 18, 2025</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=overview>Overview<a href=#overview class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Introduces a <strong>HoOA benchmark</strong> that isolates hallucinations on <strong>object attributes</strong> (color, shape) from existence/relationship errors.</li>
<li>Proposes <strong>MIAVLM</strong>: leverages <strong>multiview images</strong> (generated from a single image’s 3D representation) and a <strong>Multiview Attributes Perceiver (MAP)</strong> to make fusion <strong>order-invariant</strong>.</li>
<li>Adds <strong>negative instructions</strong> during tuning to counter LVLMs’ tendency to answer "Yes".</li>
<li>Results: best HoOA metric (<strong>0.775 / 0.787</strong>) with <strong>fastest inference</strong> (<strong>0.071 / 0.105 s</strong>). "9in1" tiling is ineffective; separate multiview inputs help.</li>
<li>Training: LM loss, Adam (lr=0.001), cosine annealing, 20 epochs, single NVIDIA 3090.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=hallucinations-on-object-attributes-hooa>Hallucinations on Object Attributes (HoOA)<a href=#hallucinations-on-object-attributes-hooa class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=issues>Issues<a href=#issues class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>HoOA = incorrect attribute descriptions for existing objects (distinct from HoOE/HoOR).</li>
<li>Root causes analyzed:<!-- -->
<ul>
<li><strong>Single-view insufficiency</strong>: fine-grained details can be invisible from a single viewpoint.</li>
<li><strong>Instruction bias</strong>: overexposure to positive/affirmative patterns → "Yes" bias.</li>
<li><strong>Order sensitivity</strong>: multi-image inputs change predictions when view order changes.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=mitigation-methods-this-paper>Mitigation Methods (this paper)<a href=#mitigation-methods-this-paper class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li><strong>Multiview prompts</strong>: sample views from a single image’s 3D reconstruction to recover missed details.</li>
<li><strong>MAP (order-invariant fusion)</strong>: learn view weights and fuse per-view features via <strong>weighted sum</strong>; input order has no effect; supports any number of views.</li>
<li><strong>Negative instructions</strong>: incorporate "No"-answerable questions in tuning to suppress "Yes" bias.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=benchmark-hooa>Benchmark (HoOA)<a href=#benchmark-hooa class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=construction>Construction<a href=#construction class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Based on <strong>CelebAText-HQ</strong>; manual attribute descriptions rewritten into <strong>Yes/No</strong> questions.<!-- -->
<ul>
<li><strong>Positive questions</strong> → correct answer "Yes".</li>
<li><strong>Negative questions</strong> → attribute flipped/opposite → correct answer "No" (to expose "Yes" bias).</li>
</ul>
</li>
<li>Scale: 1,430 images, 14,291 positive + 14,291 negative questions.</li>
<li>Split: <strong>9:1</strong> train<!-- -->:test<!-- -->.</li>
<li><strong>Metric</strong>: average of accuracy on positive and negative questions (balanced HoOA score).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=model-miavlm>Model: MIAVLM<a href=#model-miavlm class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=visual-extractor-ve>Visual Extractor (VE)<a href=#visual-extractor-ve class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>6 stacked Transformer <strong>decoder</strong> blocks.</li>
<li><strong>Soft prompts <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>P</mi><mo>∈</mo><msup><mi mathvariant=double-struck>R</mi><mrow><mi>l</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=application/x-tex>P \in \mathbb{R}^{l \times d}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7224em;vertical-align:-0.0391em></span><span class="mord mathnormal" style=margin-right:0.13889em>P</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.8491em></span><span class=mord><span class="mord mathbb">R</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8491em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.01968em>l</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></strong> are <strong>queries</strong>; <strong>image embeddings <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>e_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">e</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span></strong> are <strong>keys/values</strong>.</li>
<li>Per-view cross-attention computed <strong>in parallel</strong> (no autoregressive chaining; no assumed order).</li>
<li>Per-view output:<!-- -->
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi>o</mi><mi>i</mi></msub><mo>=</mo><mrow><mi mathvariant=normal>s</mi><mi mathvariant=normal>o</mi><mi mathvariant=normal>f</mi><mi mathvariant=normal>t</mi><mi mathvariant=normal>m</mi><mi mathvariant=normal>a</mi><mi mathvariant=normal>x</mi></mrow><mtext> ⁣</mtext><mrow><mo fence=true>(</mo><mfrac><mrow><mo stretchy=false>(</mo><mi>P</mi><msub><mi>W</mi><mi>Q</mi></msub><mo stretchy=false>)</mo><mo stretchy=false>(</mo><msub><mi>e</mi><mi>i</mi></msub><msub><mi>W</mi><mi>K</mi></msub><msup><mo stretchy=false>)</mo><mi mathvariant=normal>⊤</mi></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo fence=true>)</mo></mrow><msub><mi>e</mi><mi>i</mi></msub><msub><mi>W</mi><mi>V</mi></msub><mo separator=true>,</mo><mspace width=1em /><msub><mi>O</mi><mrow><mi>V</mi><mi>E</mi></mrow></msub><mo>=</mo><mo stretchy=false>{</mo><msub><mi>o</mi><mn>1</mn></msub><mo separator=true>,</mo><mo>…</mo><mo separator=true>,</mo><msub><mi>o</mi><mi>n</mi></msub><mo stretchy=false>}</mo><mi mathvariant=normal>.</mi></mrow><annotation encoding=application/x-tex>o_i = \mathrm{softmax}\!\left(\frac{(P W_Q)(e_i W_K)^\top}{\sqrt{d}}\right) e_i W_V,\quad O_{VE}=\{o_1,\dots,o_n\}.</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">o</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.4761em;vertical-align:-0.95em></span><span class=mord><span class="mord mathrm">softmax</span></span><span class=mspace style=margin-right:-0.1667em></span><span class=mspace style=margin-right:0.1667em></span><span class=minner><span class="mopen delimcenter" style=top:0em><span class="delimsizing size3">(</span></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.5261em><span style=top:-2.1778em><span class=pstrut style=height:3em></span><span class=mord><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.9322em><span class=svg-align style=top:-3em><span class=pstrut style=height:3em></span><span class=mord style=padding-left:0.833em><span class="mord mathnormal">d</span></span></span><span style=top:-2.8922em><span class=pstrut style=height:3em></span><span class=hide-tail style=min-width:0.853em;height:1.08em><svg xmlns=http://www.w3.org/2000/svg width=400em height=1.08em viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1078em><span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:0.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.13889em>P</span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em>W</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:-0.1389em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mclose>)</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">e</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em>W</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:-0.1389em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.07153em>K</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mclose><span class=mclose>)</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8491em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.93em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style=top:0em><span class="delimsizing size3">)</span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">e</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em>W</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:-0.1389em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.22222em>V</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:1em></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>O</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.22222em>V</span><span class="mord mathnormal mtight" style=margin-right:0.05764em>E</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mopen>{</span><span class=mord><span class="mord mathnormal">o</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=minner>…</span><span class=mspace style=margin-right:0.1667em></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">o</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mclose>}</span><span class=mord>.</span></span></span></span></span>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=multihead-sampler-ms>Multihead Sampler (MS)<a href=#multihead-sampler-ms class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Learns <strong>view weights</strong> for fusion.</li>
<li><strong>Decomposer (2-layer MLP)</strong> maps each view’s <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo stretchy=false>[</mo><mi>C</mi><mi>L</mi><mi>S</mi><mo stretchy=false>]</mo></mrow><annotation encoding=application/x-tex>[CLS]</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mopen>[</span><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style=margin-right:0.05764em>S</span><span class=mclose>]</span></span></span></span> to <strong><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>m</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=application/x-tex>m=4</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal">m</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>4</span></span></span></span></strong> tokens <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo stretchy=false>{</mo><msubsup><mi>e</mi><mi>i</mi><mn>1</mn></msubsup><mo separator=true>,</mo><mo>…</mo><mo separator=true>,</mo><msubsup><mi>e</mi><mi>i</mi><mi>m</mi></msubsup><mo stretchy=false>}</mo></mrow><annotation encoding=application/x-tex>\{e_i^{1},\dots,e_i^{m}\}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0728em;vertical-align:-0.2587em></span><span class=mopen>{</span><span class=mord><span class="mord mathnormal">e</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-2.4413em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2587em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=minner>…</span><span class=mspace style=margin-right:0.1667em></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">e</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6644em><span style=top:-2.4413em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2587em><span></span></span></span></span></span></span><span class=mclose>}</span></span></span></span>.</li>
<li>For each token/head <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>j</mi></mrow><annotation encoding=application/x-tex>j</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.854em;vertical-align:-0.1944em></span><span class="mord mathnormal" style=margin-right:0.05724em>j</span></span></span></span>: compute attention scores vs. <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>P</mi></mrow><annotation encoding=application/x-tex>P</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.13889em>P</span></span></span></span> → mean over prompt tokens → <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mrow><mi mathvariant=normal>w</mi><mi mathvariant=normal>e</mi><mi mathvariant=normal>i</mi><mi mathvariant=normal>g</mi><mi mathvariant=normal>h</mi><mi mathvariant=normal>t</mi><mi mathvariant=normal>s</mi></mrow><mi>j</mi></msup><mo>∈</mo><msup><mi mathvariant=double-struck>R</mi><mi>n</mi></msup></mrow><annotation encoding=application/x-tex>\mathrm{weights}^j \in \mathbb{R}^n</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.1034em;vertical-align:-0.1944em></span><span class=mord><span class=mord><span class="mord mathrm">weights</span></span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.909em><span style=top:-3.1473em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.05724em>j</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6889em></span><span class=mord><span class="mord mathbb">R</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.6644em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span>.</li>
<li>Average across heads:<!-- -->
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi>w</mi><mrow><mi>M</mi><mi>S</mi></mrow></msub><mo>=</mo><mstyle displaystyle=false scriptlevel=0><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mrow><mi mathvariant=normal>w</mi><mi mathvariant=normal>e</mi><mi mathvariant=normal>i</mi><mi mathvariant=normal>g</mi><mi mathvariant=normal>h</mi><mi mathvariant=normal>t</mi><mi mathvariant=normal>s</mi></mrow><mi>j</mi></msup><mo>∈</mo><msup><mi mathvariant=double-struck>R</mi><mi>n</mi></msup><mi mathvariant=normal>.</mi></mrow><annotation encoding=application/x-tex>w_{MS} = \tfrac{1}{m}\sum_{j=1}^{m}\mathrm{weights}^j \in \mathbb{R}^n.</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02691em>w</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:-0.0269em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.05764em>MS</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:3.0652em;vertical-align:-1.4138em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8451em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:0.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.345em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace style=margin-right:0.1667em></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.6514em><span style=top:-1.8723em;margin-left:0em><span class=pstrut style=height:3.05em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.05724em>j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style=top:-3.05em><span class=pstrut style=height:3.05em></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style=top:-4.3em;margin-left:0em><span class=pstrut style=height:3.05em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.4138em><span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class=mord><span class="mord mathrm">weights</span></span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.909em><span style=top:-3.1473em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.05724em>j</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.7144em></span><span class=mord><span class="mord mathbb">R</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.7144em><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class=mord>.</span></span></span></span></span>
</li>
</ul>
<p><img decoding=async loading=lazy alt=MS src=/en/assets/images/vlm-MS-9377813c0cacbbe34ababda969307034.png width=1308 height=866 class=img_f7zd></p>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=map-multiview-attributes-perceiver>MAP (Multiview Attributes Perceiver)<a href=#map-multiview-attributes-perceiver class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li><strong>Order-invariant weighted fusion</strong>:<!-- -->
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mtext>Output</mtext><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>w</mi><mi>i</mi></msub><mtext> </mtext><msub><mi>o</mi><mi>i</mi></msub><mi mathvariant=normal>.</mi></mrow><annotation encoding=application/x-tex>\text{Output}=\sum_{i=1}^{n} w_i\,o_i.</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8778em;vertical-align:-0.1944em></span><span class="mord text"><span class=mord>Output</span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.9291em;vertical-align:-1.2777em></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.6514em><span style=top:-1.8723em;margin-left:0em><span class=pstrut style=height:3.05em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style=top:-3.05em><span class=pstrut style=height:3.05em></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style=top:-4.3em;margin-left:0em><span class=pstrut style=height:3.05em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.2777em><span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02691em>w</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:-0.0269em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">o</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mord>.</span></span></span></span></span>
</li>
<li>Properties: supports <strong>any number of views</strong>; <strong>permutation-invariant</strong> to input order.</li>
<li>By learning weights for each view, MAP highlights informative perspectives and suppresses less useful ones, ensuring consistent predictions even when the view order changes. This directly addresses the input-order sensitivity observed in baselines such as OpenFlamingo.</li>
</ul>
<p><img decoding=async loading=lazy alt=MAP src=/en/assets/images/vlm-MAP-d20f08f8971017b5de5936f013867d6b.png width=1248 height=732 class=img_f7zd></p>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=benchmarks>Benchmarks<a href=#benchmarks class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=baselines--input-modes>Baselines & Input Modes<a href=#baselines--input-modes class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Baselines: <strong>BLIP3</strong>, <strong>OpenFlamingo (4 variants)</strong>, <strong>OPERA</strong>, <strong>Idefics2</strong>, <strong>LLaVA-UHD</strong>.</li>
<li>Two input modes:<!-- -->
<ol>
<li><strong>Original image only</strong>.</li>
<li><strong>Original + 8 generated views</strong>.<!-- -->
<ul>
<li>Models that accept only one image use <strong>9in1</strong> tiling (nine images stitched into one).</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=main-results>Main Results<a href=#main-results class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li><strong>MIAVLM</strong>:<!-- -->
<ul>
<li><strong>HoOA metric:</strong> <strong>0.775 / 0.787</strong> (modes 1 / 2)</li>
<li><strong>Positive accuracy:</strong> 0.752 / 0.762</li>
<li><strong>Negative accuracy:</strong> 0.797 / 0.812</li>
<li><strong>Inference time:</strong> <strong>0.071 / 0.105 s</strong> (fastest)</li>
</ul>
</li>
<li><strong>9in1</strong> tiling <strong>did not improve</strong> results (likely harder to interpret).</li>
<li><strong>Nine separate multiview images</strong> generally <strong>improved</strong> performance.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=ablations>Ablations<a href=#ablations class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li><strong>Negative instructions:</strong> boost <strong>negative-question</strong> accuracy but slightly reduce <strong>positive-question</strong> accuracy; overall HoOA <strong>increases</strong> (approx. <strong>0.665 → 0.787</strong>).</li>
<li><strong>Input-order sensitivity:</strong>
<ul>
<li>MIAVLM is <strong>order-invariant</strong></li>
<li>OpenFlamingos accuracy varies when shuffling view order.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=limitations--notes>Limitations & Notes<a href=#limitations--notes class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Trade-off from negative instructions (negatives ↑, positives ↓).</li>
<li>Effectiveness depends on the quality of <strong>generated views</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=insights>Insights<a href=#insights class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>This approach seems especially suitable for perception, where multiple scene views may arrive in arbitrary order, ensuring consistent attribute recognition.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Tan, Z., Li, Y., Meng, S., Yuan, X., Li, W., Mo, T., Wang, B., & Chu, X. (2025, 6–11 April 2025). Mitigating Hallucinations on Object Attributes using Multiview Images and Negative Instructions. ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><article class=margin-bottom--xl><header><h2 class=title_lZLA><a href=/en/2025/08/17/trustworthiness-in-vision-language-models-review/>Trustworthiness in Vision-Language Models Review</a></h2><div class="container_cDO4 margin-vert--md"><time datetime=2025-08-17T00:02:41.350Z>August 17, 2025</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_FKwP"><div class="avatar margin-bottom--sm"><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer" class=avatar__photo-link><img class="avatar__photo authorImage_ofBO" src="https://avatars.githubusercontent.com/u/11773683?v=4" alt=Gracefullight></a><div class="avatar__intro authorDetails_t3Yw"><div class=avatar__name><a href=https://github.com/gracefullight target=_blank rel="noopener noreferrer"><span class=authorName_HwLZ>Gracefullight</span></a></div><small class=authorTitle_ktHR title=Owner>Owner</small><div class=authorSocials_linN></div></div></div></div></div></header><div class=markdown><h2 class="anchor anchorWithStickyNavbar_IhMp" id=overview>Overview<a href=#overview class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Mitigates exposure of private data, produces harmful outputs, or is vulnerable to attacks.</li>
<li>SOTA models: LLaVA, Flamingo, GPT-4</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=privacy>Privacy<a href=#privacy class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=privacy-issues>Privacy Issues<a href=#privacy-issues class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>risk escalates significantly with relevant images as optimizing in the pixel domain is easier than in text</li>
<li>can unintentionally memorize sensitive data, leading to leaks without knowledge of the model’s specifics</li>
<li>Overfitting may also cause retention of sensitive attributes during inference</li>
<li>gradient-based and backdoor attacks further jeopardize VLM privacy with open-source data</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=privacy-mitigation-methods>Privacy Mitigation Methods<a href=#privacy-mitigation-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>New metrics have been created to assess a model’s ability to reproduce training instances and facilitate cross-model comparisons</li>
<li>models utilizing multiple modalities provide better privacy</li>
<li>safety modules can be integrated to boost resilience against violations</li>
<li>adversarial training can enhance privacy but risks reducing accuracy</li>
<li>New architecture: differentially private CLIP model</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=privacy-future-research-directions>Privacy Future Research Directions<a href=#privacy-future-research-directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Cryptography-based Privacy Preservation<!-- -->
<ul>
<li>Secure multi-party computation (SMPC): divides secret information into shares among multiple parties, ensuring that individual shares reveal nothing unless combined</li>
<li>Homomorphic encryption (HE): allows computations on encrypted data without decryption, and has also been utilized for privacy preservation in transformers</li>
</ul>
</li>
<li>Federated Learning<!-- -->
<ul>
<li>enhances privacy in vision-language models (VLMs) by localizing model training, which protects training data from leakage.</li>
<li>challenges such as communication overhead among devices and statistical heterogeneity from diverse data distributions</li>
</ul>
</li>
<li>Data Manipulation and Finetunning<!-- -->
<ul>
<li>Data pseudonymization: substitutes sensitive information with synthetic alternatives.</li>
<li>Data Sanitization: removes duplicates to reduce memorization and privacy risks.</li>
<li>knowledge sanitization-fine-tuning: provide safe responses when leakage risks arise.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=fairness-and-bias>Fairness and Bias<a href=#fairness-and-bias class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=fairness-and-bias-issues>Fairness and Bias Issues<a href=#fairness-and-bias-issues class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Bias from training data<!-- -->
<ul>
<li>disproportionately features men and lighter-skinned individuals</li>
<li>outdated vocabulary and imbalanced representation</li>
<li>clinical models may favor certain patient groups based on gender, language, etc.</li>
</ul>
</li>
<li>Bias from Model<!-- -->
<ul>
<li>Gender biases</li>
<li>misclassification of race-related elements and biased outputs</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=fairness-and-bias-mitigation-methods>Fairness and Bias Mitigation Methods<a href=#fairness-and-bias-mitigation-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>New Datasets and Benchmarks<!-- -->
<ul>
<li>Harvard-FairVLMed, PATA, and BOLD enhance evaluations but often lack the scale of established benchmarks.</li>
<li>create synthetic datasets to improve fairness assessments<!-- -->
<ul>
<li>gender-balanced dataset generated with DALL-E-3 and another consisting of gender-swapped images</li>
<li>counterfactual image-text pairs that highlight biases in datasets like COCO Captions</li>
</ul>
</li>
<li>new metrics<!-- -->
<ul>
<li>gender polarity</li>
<li>bias distance in embeddings</li>
</ul>
</li>
<li>human evaluation</li>
</ul>
</li>
<li>De-biasing<!-- -->
<ul>
<li>adjust model instructions and architectures for improved fairness</li>
<li>detecting biased prompts in pre-trained models</li>
<li>Post-hoc Bias Mitigation (PBM) effectively reduce bias in image retrieval</li>
<li>Re-sampling underperforming clusters can enhance fairness</li>
<li>modification of facial features also mitigate biases</li>
<li>self-debiasing reduces biased text generation, especially when paired with other methods</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=fairness-future-research-directions>Fairness Future Research Directions<a href=#fairness-future-research-directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Optimized De-biasing<!-- -->
<ul>
<li>Additive residual learning: for fairer image representations.</li>
<li>Calibration loss: retain semantically similar embeddings.</li>
<li>Counterfactual inference framework: help models learn correct responses through cause and effect.</li>
<li>Adversarial classifiers: predict image attributes from visual-textual similarities can be combined with instruction tuning to reduce bias.</li>
</ul>
</li>
<li>Disentangled Representation Learning (DRL): simplifies complex data by breaking it in to independent feature groups, improving model predictions.<!-- -->
<ul>
<li>Traditional DRL<!-- -->
<ul>
<li>Variational autoencoders (VAEs) for feature encoding based on impact</li>
<li>Generative adversarial networks (GANs) for separation.</li>
</ul>
</li>
<li>Attention in text encoders can be adjusted for fairer outputs.</li>
<li>challenges: varying definitions of "disentanglement", ensuring fairness.</li>
</ul>
</li>
<li>Human-in-the-Loop (HITL): integrating human intervention into their training to improve precision and fairness<!-- -->
<ul>
<li>active learning</li>
<li>reinforcement learning with human feedback</li>
<li>explainable AI</li>
<li>challenges: human bias, finance, and ethical and legal issues persist</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=robustness>Robustness<a href=#robustness class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=robustness-issues>Robustness Issues<a href=#robustness-issues class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Out-of-Distribution (OOD) Robustness<!-- -->
<ul>
<li>ChatGPT excels in adversarial tasks but struggles with OOD robustness and informal medical responses</li>
<li>MLLMs often fail to generalize beyond training domains due to mapping issues</li>
<li>vision-language models face difficulties with open-domain concepts, especially when overfitting during fine-tuning</li>
<li>Large pre-trained image classifiers show initial robustness, which diminishes over time</li>
<li>Current visual question answering (VQA) models are limited to specific benchmarks, hindering generalization to OOD datasets</li>
<li>fine-tuning may impair model calibration in OOD contexts.</li>
</ul>
</li>
<li>Adversarial Attack Robustness<!-- -->
<ul>
<li>Studies indicate that open-sourced VLMs show performance gaps in red teaming tasks, highlighting the need for improved safety and security.</li>
<li>misalignment between language and vision modalities creates a "modality gap", complicating adversarial vulnerability.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=robustness-mitigation-methods>Robustness Mitigation Methods<a href=#robustness-mitigation-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Improving Out-of-Distribution Robustness<!-- -->
<ul>
<li>enhance OOD detection and generalization. A simple maximum logit detector has been shown to outperform complex methods for anomaly segmentation</li>
<li>In-context learning (ICL) can also improve multimodal generalization</li>
<li>A fine-tuned CLIP excels in unsupervised OOD detection</li>
<li>The OGEN method synthesizes OOD features</li>
<li>Maximum Concept Matching aligns visual and textual features, and anchor-based finetuning leads to better domain shifts</li>
</ul>
</li>
<li>Defense Against Adversarial Attacks<!-- -->
<ul>
<li>VILLA is a two-stage framework for adversarial training of VLMs, featuring task-agnostic <strong>adversarial pre-training</strong> and <strong>task-specific finetuning</strong>
<ul>
<li>conducts adversarial training in the embedding space rather than on raw image pixels and text tokens, improving the model’s resilience against adversarial examples</li>
<li>SOTA performance across various tasks</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=robustness-future-research-directions>Robustness Future Research Directions<a href=#robustness-future-research-directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Data Augmentation<!-- -->
<ul>
<li>MixGen: a data augmentation method that generates new image-text pairs by interpolating images and concatenating text to preserve semantics.</li>
<li>creating synthetic images involves extracting text prompts via an image captioning model for use in text-to-image diffusion, then mixing these with real datasets.</li>
<li>bimodal augmentation (BiAug): decouples objects and attributes to synthesize vision-language examples and hard negatives, using LLMs and an object detector to generate detailed descriptions and inpaint corresponding images.</li>
</ul>
</li>
<li>Improved Cross-Modal Alignment<!-- -->
<ul>
<li>Sharing learnable parameters</li>
<li>Applying bidirectional constraints</li>
<li>Adjusting cross-modal projections</li>
</ul>
</li>
<li>challenges: addressing the modality gap, which impacts robustness to OOD data and adversarial examples</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=safety>Safety<a href=#safety class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=safety-issues>Safety Issues<a href=#safety-issues class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Toxicity<!-- -->
<ul>
<li>LAION-400M: contains problematic content, including explicit materials and harmful stereotypes</li>
<li>Advanced models like GeminiProVision and GPT-4V show inherent biases</li>
<li>Assigning personas to ChatGPT can increase toxicity and reinforce harmful stereotypes</li>
</ul>
</li>
<li>Jailbreaking Risk<!-- -->
<ul>
<li>Perturbation can be performed effectively, while FigStep converts harmful content into images with an 82.5% attack rate across multiple VLMs</li>
<li>replaces captions with malicious prompts, enabling jailbreaks.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=safety-mitigation-methods>Safety Mitigation Methods<a href=#safety-mitigation-methods class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li><strong>Safety Fine-Tuning</strong>
<ul>
<li>VLGuard</li>
<li>fine-tuned on synthetic data, reducing sensitivity to NSFW inputs and enhancing performance in cross-modal tasks</li>
</ul>
</li>
<li>Other approach<!-- -->
<ul>
<li>Reinforce-Detoxify: uses reinforcement learning to mitigate toxicity and bias in transformer models</li>
<li>simple mitigations improve automatic scores, these methods risk over-filtering marginalized texts and create discrepancies between automatic and human judgments</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_IhMp" id=safety-future-research-directions>Safety Future Research Directions<a href=#safety-future-research-directions class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<ul>
<li>Context Awareness<!-- -->
<ul>
<li>integrating Chain-of-Thought for improved reasoning can enhance CAER tasks with Large VLMs.</li>
<li>Dual-Aligned Prompt Tuning: combines explicit context from pre-trained LLMs with implicit modeling to create more context-aware prompts</li>
<li>Visual In-Context Learning: optimizes image retrieval and summarization to enhance task-specific interactions.</li>
</ul>
</li>
<li>Automated Red Teaming (ART)<!-- -->
<ul>
<li>RTVLM: a dataset that benchmarks VLMs across faithfulness, privacy, safety, and fairness</li>
<li>Arondight: automates multi-modal jailbreak attacks using reinforcement learning and uncovers significant security vulnerabilities</li>
<li>GPT-4 and GPT-4V are more robust against jailbreaks than open-source models</li>
<li>limited transferability of visual jailbreak methods compared to textual ones</li>
<li>connects unsafe outputs to prompts, improving the detection of vulnerabilities in text-to-image models</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_IhMp" id=ref>Ref<a href=#ref class=hash-link aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ul>
<li>Vu, K., & Lai, P. (2025). Trustworthiness in Vision-Language Models. In J. Kertesz, B. Li, T. Supnithi, & A. Takhom, Computational Data and Social Networks Singapore.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class=col><b>Tags:</b><ul class="tags_Vgw9 padding--none margin-left--sm"><li class=tag_wI6s><a rel=tag class="tag_DbKY tagRegular_rujD" href=/en/tags/vlm/>vlm</a></ul></div></footer></article><nav class=pagination-nav aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--next" href=/en/tags/vlm/page/2/><div class=pagination-nav__label>Older Entries</div></a></nav></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Support Me</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://www.buymeacoffee.com/LOUB2kN target=_blank rel="noopener noreferrer" style="cursor: pointer;">
                <img src=https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png style="height: auto !important;width: auto !important;">
              </a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Feeds</div><ul class="footer__items clean-list"><li class=footer__item><a href=https://gracefullight.dev/rss.xml target=_blank rel="noopener noreferrer" class=footer__link-item>RSS<svg width=13.5 height=13.5 aria-hidden=true class=iconExternalLink_E7SL><use href=#theme-svg-external-link /></svg></a><li class=footer__item><a href=https://gracefullight.dev/atom.xml target=_blank rel="noopener noreferrer" class=footer__link-item>Atom<svg width=13.5 height=13.5 aria-hidden=true class=iconExternalLink_E7SL><use href=#theme-svg-external-link /></svg></a></ul></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2016 - 2023 Gracefullight. Built with Docusaurus.</div></div></div></footer></div>