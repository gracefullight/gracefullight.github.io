"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["55464"],{28990:function(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var s=i(98880),t=i(69979),r=i(56774);let o={title:"Octo Review",date:new Date("2025-08-27T10:54:09.447Z"),description:"Octo review",authors:"me",tags:["vlm"]},l=void 0,a={authorsImageUrls:[void 0]},c=[{value:"Octo",id:"octo",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Prior GRPs &amp; Gaps",id:"prior-grps--gaps",level:2},{value:"Contribution (What is Octo?)",id:"contribution-what-is-octo",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Training Data &amp; Objective",id:"training-data--objective",level:2},{value:"Experiments",id:"experiments",level:2},{value:"Results",id:"results",level:2},{value:"Limitations / Future Work",id:"limitations--future-work",level:2},{value:"One-line Takeaway",id:"one-line-takeaway",level:2},{value:"Ref",id:"ref",level:2}];function d(e){let n={br:"br",code:"code",em:"em",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"octo",children:"Octo"}),"\n",(0,t.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Traditional robot learning trains policies ",(0,t.jsx)(n.strong,{children:"from scratch"})," on robot/task-specific datasets \u2192 costly data collection, narrow generalization."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Generalist Robot Policies (GRPs)"})," pretrained on diverse robots/tasks can be ",(0,t.jsx)(n.strong,{children:"finetuned with little in-domain data"})," while generalizing broadly."]}),"\n",(0,t.jsxs)(n.li,{children:["Real-world deployments face challenges across ",(0,t.jsx)(n.strong,{children:"robot embodiments, sensor setups, action spaces, task specs, and environments"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prior-grps--gaps",children:"Prior GRPs & Gaps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["GRPs aim for ",(0,t.jsx)(n.strong,{children:"low-level visuomotor control"})," across tasks, environments, and robotic systems."]}),"\n",(0,t.jsxs)(n.li,{children:["Existing models often have ",(0,t.jsx)(n.strong,{children:"restricted inputs (e.g., a single camera)"}),", ",(0,t.jsx)(n.strong,{children:"lack efficient finetuning to new domains"}),", and importantly, ",(0,t.jsx)(n.strong,{children:"largest models are not publicly available"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"contribution-what-is-octo",children:"Contribution (What is Octo?)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Octo"}),": a large transformer-based policy trained on ",(0,t.jsx)(n.strong,{children:"800k trajectories"})," from the Open X-Embodiment dataset."]}),"\n",(0,t.jsxs)(n.li,{children:["Accepts ",(0,t.jsx)(n.strong,{children:"language instructions or goal images"}),", and can be ",(0,t.jsx)(n.strong,{children:"finetuned within hours on consumer GPUs"})," to new sensors and action spaces."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"First GRP"})," to support ",(0,t.jsx)(n.strong,{children:"effective finetuning to new observations and actions"})," and to be ",(0,t.jsx)(n.strong,{children:"fully open-source"})," (training pipeline, checkpoints, data)."]}),"\n",(0,t.jsxs)(n.li,{children:["Novelty lies in combining: ",(0,t.jsx)(n.strong,{children:"transformer backbone + language/goal image conditioning + diffusion head"})," for expressive action distributions."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input tokenizers"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Language via pretrained ",(0,t.jsx)(n.strong,{children:"T5-base"})]}),"\n",(0,t.jsx)(n.li,{children:"Images via shallow CNN \u2192 patch tokens"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transformer backbone"}),": processes unified token sequence."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Blockwise masking + Readout tokens"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Nonexistent modalities are masked"}),"\n",(0,t.jsxs)(n.li,{children:["Readout tokens ",(0,t.jsx)(n.em,{children:"only attend"})," to past observations/tasks, not vice versa"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Diffusion action head"}),": predicts ",(0,t.jsx)(n.strong,{children:"continuous, multimodal, chunked actions"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Modularity"}),": new sensors/outputs can be added by only training lightweight encoders or heads; pretrained backbone remains unchanged."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Octo Architecture",src:i(87709).Z+"",width:"803",height:"415"})}),"\n",(0,t.jsx)(n.h2,{id:"training-data--objective",children:"Training Data & Objective"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Mixture of ",(0,t.jsx)(n.strong,{children:"25 heterogeneous robot datasets"}),": diverse robots, sensors (with/without wrist cams), labels (with/without language)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conditional diffusion decoding"})," predicts continuous, multimodal action distributions.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Transformer runs ",(0,t.jsx)(n.strong,{children:"one forward pass"}),"; denoising steps are contained in the small diffusion head."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"experiments",children:"Experiments"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Evaluated on ",(0,t.jsx)(n.strong,{children:"7 robotic platforms across 4 institutions"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Key questions:","\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Zero-shot multi-robot control?"}),"\n",(0,t.jsx)(n.li,{children:"Do Octo weights improve finetuning vs. scratch or standard pretrained representations?"}),"\n",(0,t.jsx)(n.li,{children:"Which design choices matter for generalist robot policies?"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"results",children:"Results"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Achieves ",(0,t.jsx)(n.strong,{children:"state-of-the-art zero-shot multi-robot control"}),", competitive with RT-1-X and RT-2-X."]}),"\n",(0,t.jsxs)(n.li,{children:["Provides a ",(0,t.jsx)(n.strong,{children:"versatile policy initialization"}),": significantly outperforms baselines for ",(0,t.jsx)(n.strong,{children:"data-efficient finetuning"})," to new obs/action spaces."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"limitations--future-work",children:"Limitations / Future Work"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Needs ",(0,t.jsx)(n.strong,{children:"better language conditioning"}),", ",(0,t.jsx)(n.strong,{children:"improved wrist camera support"}),", and ",(0,t.jsx)(n.strong,{children:"data beyond optimal demonstrations"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"one-line-takeaway",children:"One-line Takeaway"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Octo = modular, efficient, open-source GRP"}),":",(0,t.jsx)(n.br,{}),"\n","A transformer + diffusion policy trained on large-scale multi-robot data that ",(0,t.jsx)(n.strong,{children:"adapts quickly with little in-domain data"})," to new sensors and action spaces, enabling broad generalization."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"ref",children:"Ref"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Mees, O., Ghosh, D., Pertsch, K., Black, K., Walke, H. R., Dasari, S., Hejna, J., Kreiman, T., Xu, C., & Luo, J. (2024). Octo: An open-source generalist robot policy. First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024."}),"\n"]}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"\uAD6C\uBD84"}),(0,t.jsx)(n.th,{children:(0,t.jsx)(n.strong,{children:"\uB2E8\uC21C \uBE44\uC720 \uBC84\uC804"})}),(0,t.jsx)(n.th,{children:(0,t.jsx)(n.strong,{children:"\uC2E4\uC81C \uD1A0\uD070\uD654 \uBC84\uC804"})})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"\uC5B8\uC5B4"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"[\uBB38\uC7A5]"})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"[l\u2081, l\u2082, l\u2083, \u2026]"})," ",(0,t.jsx)("br",{}),"\u2192 \uBB38\uC7A5\uC744 \uD1A0\uD070\uD654\uD55C \uC5EC\uB7EC \uAC1C"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"\uBAA9\uD45C \uC774\uBBF8\uC9C0"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"[\uBAA9\uD45C]"})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"[g\u2081, g\u2082, g\u2083, \u2026]"})," ",(0,t.jsx)("br",{}),"\u2192 \uC774\uBBF8\uC9C0\uB97C \uD328\uCE58 \uB2E8\uC704\uB85C \uCABC\uAC20 \uC5EC\uB7EC \uAC1C"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"\uAD00\uCC30(\uC2DC\uC810 t)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"[\uAD00\uCC30]"})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"[o\u209C\xb9, o\u209C\xb2, o\u209C\xb3, \u2026]"})," ",(0,t.jsx)("br",{}),"\u2192 \uCE74\uBA54\uB77C \uD504\uB808\uC784/\uC13C\uC11C\uB97C \uD328\uCE58 \uB2E8\uC704\uB85C \uD1A0\uD070\uD654"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"\uB9AC\uB4DC\uD1A0\uD070"})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"[ ]"})," (\uBE48 \uC2AC\uB86F)"]}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"[TR,t]"})," ",(0,t.jsx)("br",{}),"\u2192 \uC2DC\uC810 t\uB9C8\uB2E4 \uD558\uB098 \uCD94\uAC00, \uD589\uB3D9\uC744 \uBF51\uB294 \uC804\uC6A9 \uD1A0\uD070"]})]})]})]})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},87709:function(e,n,i){i.d(n,{Z:()=>s});let s=i.p+"assets/images/octo-architecture-49b9dd94643695f0566e74ac5a0801bd.png"},56774:function(e,n,i){i.d(n,{Z:()=>l,a:()=>o});var s=i(52136);let t={},r=s.createContext(t);function o(e){let n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}},98880:function(e){e.exports=JSON.parse('{"permalink":"/2025/08/27/octo-review","source":"@site/blog/2025/08/27/octo-review.md","title":"Octo Review","description":"Octo review","date":"2025-08-27T10:54:09.447Z","tags":[{"inline":true,"label":"vlm","permalink":"/tags/vlm"}],"readingTime":2.57,"hasTruncateMarker":false,"authors":[{"name":"Eunkwang Shin","title":"Owner","url":"https://github.com/gracefullight","socials":{"linkedin":"https://www.linkedin.com/in/gracefullight/","github":"https://github.com/gracefullight"},"description":"Full Stack JavaScript Developer | Half-time Open Sourcerer.","page":{"permalink":"/authors/me"},"imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me"}],"frontMatter":{"title":"Octo Review","date":"2025-08-27T10:54:09.447Z","description":"Octo review","authors":"me","tags":["vlm"]},"unlisted":false,"nextItem":{"title":"Introduction to AI @004","permalink":"/2025/08/26/introduction-to-ai-004"}}')}}]);