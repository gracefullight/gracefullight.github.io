"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["25923"],{26086:function(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var s=i(17414),t=i(69979),r=i(59338);let o={title:"Vima Review",date:new Date("2025-08-31T10:20:03.726Z"),description:"Vima review",authors:"me",tags:["vlm"]},l=void 0,a={authorsImageUrls:[void 0]},c=[{value:"VIMA",id:"vima",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Key Contributions",id:"key-contributions",level:2},{value:"Design Insights",id:"design-insights",level:2},{value:"Results",id:"results",level:2},{value:"Conclusion",id:"conclusion",level:2},{value:"Ref",id:"ref",level:2}];function d(e){let n={br:"br",code:"code",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"vima",children:"VIMA"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unified Multimodal Prompts"}),": Reformulates diverse robot tasks (language, images, video) into a single sequence modeling problem."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object-Centric Tokenization"}),": Uses object-level tokens (Mask R-CNN + ViT) instead of raw pixels, improving data efficiency and semantic generalization."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cross-Attention Conditioning"}),": Conditions the policy on prompts via cross-attention, maintaining strong zero-shot performance even with small models or novel tasks."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robot task specification comes in many forms: one-shot demonstrations, language instructions, and visual goals."}),"\n",(0,t.jsx)(n.li,{children:"Traditionally, each task required distinct architectures and pipelines, leading to siloed systems with poor generalization."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"VIMA Architecture",src:i(90949).Z+"",width:"1424",height:"988"})}),"\n",(0,t.jsx)(n.h2,{id:"key-contributions",children:"Key Contributions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Multimodal Prompting"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["A novel formulation that unifies diverse robot manipulation tasks into a ",(0,t.jsx)(n.strong,{children:"sequence modeling problem"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"Prompts are defined as interleaved sequences of text and images, enabling flexibility across task formats."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"VIMA-BENCH"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["A large-scale benchmark with ",(0,t.jsx)(n.strong,{children:"17 tasks"})," across six categories (object manipulation, goal reaching, novel concept grounding, video imitation, constraint satisfaction, visual reasoning)."]}),"\n",(0,t.jsxs)(n.li,{children:["Provides ",(0,t.jsx)(n.strong,{children:"650K expert trajectories"})," and a ",(0,t.jsx)(n.strong,{children:"four-level evaluation protocol"})," for systematic generalization."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"VIMA Agent"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["A transformer-based visuomotor agent with ",(0,t.jsx)(n.strong,{children:"encoder-decoder architecture"})," and ",(0,t.jsx)(n.strong,{children:"object-centric design"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Encodes prompts with a pre-trained ",(0,t.jsx)(n.strong,{children:"T5 model"}),", parses images into object tokens via ",(0,t.jsx)(n.strong,{children:"Mask R-CNN + ViT"}),", and decodes actions autoregressively using ",(0,t.jsx)(n.strong,{children:"cross-attention"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"design-insights",children:"Design Insights"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object-Centric Representation"}),": Passing variable-length object token sequences directly to the controller is more effective than pixel-based tokenization."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cross-Attention Conditioning"}),": Stronger prompt focus and efficiency compared to simple concatenation (e.g., GPT-style)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Minimal degradation under distractors or corrupted prompts, aided by T5 backbone and object augmentation."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"results",children:"Results"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Performance"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Outperforms baselines (VIMA-Gato, VIMA-Flamingo, VIMA-GPT) by up to ",(0,t.jsx)(n.strong,{children:"2.9\xd7 success rate"})," in hardest zero-shot generalization."]}),"\n",(0,t.jsxs)(n.li,{children:["With ",(0,t.jsx)(n.strong,{children:"10\xd7 less training data"}),", still ",(0,t.jsx)(n.strong,{children:"2.7\xd7 better"})," than best competitor."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Scaling"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Sample-efficient: with just ",(0,t.jsx)(n.strong,{children:"1% of data"}),", matches baselines trained with 10\xd7 more."]}),"\n",(0,t.jsx)(n.li,{children:"Generalization holds across L1\u2013L4 evaluation, with smaller regression than alternatives."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsxs)(n.p,{children:["VIMA demonstrates that multimodal prompting is a powerful unifying framework for robot learning.",(0,t.jsx)(n.br,{}),"\n","It achieves strong scalability, data efficiency, and generalization, establishing a ",(0,t.jsx)(n.strong,{children:"solid starting point for future generalist robot agents"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"ref",children:"Ref"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A., Zhu, Y., & Fan, L. (2023). VIMA: Robot Manipulation with Multimodal Prompts Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. ",(0,t.jsx)(n.code,{children:"https://proceedings.mlr.press/v202/jiang23b.html"})]}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},90949:function(e,n,i){i.d(n,{Z:()=>s});let s=i.p+"assets/images/vima-architecture-f4e3269ed85bc5b1c954f82ab1ef77cd.png"},59338:function(e,n,i){i.d(n,{Z:()=>l,a:()=>o});var s=i(52136);let t={},r=s.createContext(t);function o(e){let n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}},17414:function(e){e.exports=JSON.parse('{"permalink":"/2025/08/31/vima-review","source":"@site/blog/2025/08/31/vima-review.md","title":"Vima Review","description":"Vima review","date":"2025-08-31T10:20:03.726Z","tags":[{"inline":false,"label":"vlm","permalink":"/tags/vlm","description":"Vision-Language Models"}],"readingTime":1.99,"hasTruncateMarker":false,"authors":[{"name":"Eunkwang Shin","title":"Owner","url":"https://github.com/gracefullight","socials":{"linkedin":"https://www.linkedin.com/in/gracefullight/","github":"https://github.com/gracefullight"},"description":"Full Stack JavaScript Developer | Half-time Open Sourcerer.","page":{"permalink":"/authors/me"},"imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me"}],"frontMatter":{"title":"Vima Review","date":"2025-08-31T10:20:03.726Z","description":"Vima review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"\u03C00 Review","permalink":"/2025/09/01/pi-zero-reivew"},"nextItem":{"title":"RoboFlamingo Review","permalink":"/2025/08/31/roboflamingo-review"}}')}}]);