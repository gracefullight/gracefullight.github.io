"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["82489"],{30048:function(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});var t=i(75528),s=i(69979),r=i(59338);let o={title:"RT-2, Robotic Transformer 2 Review",date:new Date("2025-08-24T13:40:19.433Z"),description:"RT-2 review",authors:"me",tags:["vlm"]},l="RT-2",a={authorsImageUrls:[void 0]},c=[{value:"What RT-2 Is",id:"what-rt-2-is",level:2},{value:"Core Recipe",id:"core-recipe",level:2},{value:"Action as Language (Tokenization)",id:"action-as-language-tokenization",level:2},{value:"Co-Fine-Tuning &amp; Output Constraint",id:"co-fine-tuning--output-constraint",level:2},{value:"Closed-Loop Control &amp; Real-Time Inference",id:"closed-loop-control--real-time-inference",level:2},{value:"Generalization &amp; Benchmarks",id:"generalization--benchmarks",level:2},{value:"Emergent Capabilities",id:"emergent-capabilities",level:2},{value:"Scaling &amp; Ablations",id:"scaling--ablations",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Future Directions (from the text)",id:"future-directions-from-the-text",level:2},{value:"Ref",id:"ref",level:2}];function d(e){let n={code:"code",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Trains a ",(0,s.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," model by co-fine-tuning web-scale VLMs with robot trajectories, and ",(0,s.jsx)(n.strong,{children:"treats robot actions as text tokens"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Yields ",(0,s.jsx)(n.strong,{children:"strong generalization"})," and ",(0,s.jsx)(n.strong,{children:"emergent capabilities"})," (symbol understanding, reasoning, human recognition) beyond what appears in robot data."]}),"\n",(0,s.jsxs)(n.li,{children:["Runs in ",(0,s.jsx)(n.strong,{children:"direct closed-loop control"}),"; largest evaluated model (55B) executes at ~1\u20133 Hz via a cloud (multi-TPU) inference setup."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"RT-2 Architecture",src:i(40882).Z+"",width:"1674",height:"748"})}),"\n",(0,s.jsx)(n.h2,{id:"what-rt-2-is",children:"What RT-2 Is"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["A family of VLA models (RT-2-PaLI-X, RT-2-PaLM-E) that fine-tune large VLMs on robot trajectories to output ",(0,s.jsx)(n.strong,{children:"low-level actions"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Target: ",(0,s.jsx)(n.strong,{children:"generalizable, semantically aware"})," manipulation policies that map images + instructions \u2192 actions end-to-end."]}),"\n",(0,s.jsxs)(n.li,{children:["RT-2 does ",(0,s.jsx)(n.strong,{children:"not rely on a restricted 2D action space or calibrated cameras"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.strong,{children:"unified output space"})," lets language and action tokens share the same model weights, without action-only layers."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"core-recipe",children:"Core Recipe"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Directly train"})," open-vocabulary VQA/dialogue VLMs to ",(0,s.jsx)(n.strong,{children:"output robot actions"})," while they still solve standard vision-language tasks."]}),"\n",(0,s.jsxs)(n.li,{children:["Build on RT-1 protocol/data, but replace the policy backbone with a ",(0,s.jsx)(n.strong,{children:"large VLM"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"action-as-language-tokenization",children:"Action as Language (Tokenization)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Discretize continuous action dims (\u0394pos/\u0394rot, gripper, terminate) into ",(0,s.jsx)(n.strong,{children:"256 bins"}),"; represent each dimension with an ",(0,s.jsx)(n.strong,{children:"integer token"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PaLI-X"}),": reuse numeric tokens (",(0,s.jsx)(n.code,{children:"\u22641000"}),"). ",(0,s.jsx)(n.strong,{children:"PaLM-E"}),": overwrite ",(0,s.jsx)(n.strong,{children:"256 least-frequent tokens"})," as action vocabulary (",(0,s.jsx)(n.strong,{children:"symbol tuning"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:["Form a single output string per step (e.g., ",(0,s.jsx)(n.code,{children:"terminate \u0394posx \u0394posy \u0394posz \u0394rotx \u0394roty \u0394rotz gripper"}),")."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"co-fine-tuning--output-constraint",children:"Co-Fine-Tuning & Output Constraint"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mix robot data with original web VQA/caption data"})," in training batches (up-weight robot samples) to prevent forgetting and improve generalization."]}),"\n",(0,s.jsxs)(n.li,{children:["During decoding on robot tasks, ",(0,s.jsx)(n.strong,{children:"restrict sampling to valid action tokens"})," so outputs are always executable."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"closed-loop-control--real-time-inference",children:"Closed-Loop Control & Real-Time Inference"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["RT-2 is trained and deployed for ",(0,s.jsx)(n.strong,{children:"direct closed-loop control"})," (camera \u2192 action \u2192 camera \u2026), not just high-level planning."]}),"\n",(0,s.jsxs)(n.li,{children:["For large models, inference runs via a ",(0,s.jsx)(n.strong,{children:"multi-TPU cloud service"}),"; ",(0,s.jsx)(n.strong,{children:"RT-2-PaLI-X-55B"})," reaches ",(0,s.jsx)(n.strong,{children:"~1\u20133 Hz"}),"; smaller models ~5 Hz."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"generalization--benchmarks",children:"Generalization & Benchmarks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Matches RT-1 on seen tasks but ",(0,s.jsx)(n.strong,{children:"far exceeds"})," baselines on ",(0,s.jsx)(n.strong,{children:"unseen objects/backgrounds/environments"})," (~",(0,s.jsx)(n.strong,{children:"2\xd7"})," vs RT-1/MOO; up to ",(0,s.jsx)(n.strong,{children:"~6\xd7"})," vs others)."]}),"\n",(0,s.jsxs)(n.li,{children:["Open-source ",(0,s.jsx)(n.strong,{children:"Language-Table"})," sim: co-fine-tuned ",(0,s.jsx)(n.strong,{children:"PaLI-3B"})," outperforms baselines, showing the approach transfers to other robots/sims."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"emergent-capabilities",children:"Emergent Capabilities"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Symbol understanding"})," (e.g., \u201Cmove apple to 3 / heart / star\u201D)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reasoning"})," (visual matching, simple math like \u201Csum of two plus one\u201D, ",(0,s.jsx)(n.strong,{children:"multilingual"})," commands)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human recognition"})," (e.g., \u201Cperson with glasses\u201D); none of these were present as low-level actions in robot data."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Chain-of-thought (CoT) variant"})," adds a ",(0,s.jsx)(n.strong,{children:"Plan"})," step before actions \u2192 supports ",(0,s.jsx)(n.strong,{children:"multi-stage semantic reasoning"})," (e.g., pick a rock as an improvised hammer; pick an energy drink for a tired person)."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"rt-2-cot",src:i(10395).Z+"",width:"1748",height:"934"})}),"\n",(0,s.jsx)(n.h2,{id:"scaling--ablations",children:"Scaling & Ablations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"From-scratch"})," training (even 5B) performs poorly; ",(0,s.jsx)(n.strong,{children:"fine-tuning"})," helps; ",(0,s.jsx)(n.strong,{children:"co-fine-tuning"})," helps ",(0,s.jsx)(n.strong,{children:"most"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Bigger models"})," (",(0,s.jsx)(n.code,{children:"55B > 5B"}),") generalize better."]}),"\n",(0,s.jsxs)(n.li,{children:["PaLM-E variant shows an edge on ",(0,s.jsx)(n.strong,{children:"math reasoning"}),"; PaLI-X stronger on symbols/vision reasoning on average."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Does ",(0,s.jsx)(n.strong,{children:"not"})," learn fundamentally ",(0,s.jsx)(n.strong,{children:"new motor skills"})," beyond the distribution in robot data; mainly transfers ",(0,s.jsx)(n.strong,{children:"semantic/visual knowledge"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compute/latency"})," costly; real-time control can bottleneck. Limited availability of strong open VLMs and convenient FT APIs."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"future-directions-from-the-text",children:"Future Directions (from the text)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Acquire new skills from ",(0,s.jsx)(n.strong,{children:"human videos"})," or richer datasets."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quantization/distillation"})," for faster/cheaper inference."]}),"\n",(0,s.jsxs)(n.li,{children:["More ",(0,s.jsx)(n.strong,{children:"open VLMs / FT APIs"})," to make VLA models broadly buildable."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"ref",children:"Ref"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran, H., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K., Mordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T.-W. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J., Irpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess, D., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., & Han, K. (2023). RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control Proceedings of The 7th Conference on Robot Learning, Proceedings of Machine Learning Research. ",(0,s.jsx)(n.code,{children:"https://proceedings.mlr.press/v229/zitkovich23a.html"})]}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},40882:function(e,n,i){i.d(n,{Z:()=>t});let t=i.p+"assets/images/rt-2-architecture-dd9ff6e2cae963c14c20742089b822df.png"},10395:function(e,n,i){i.d(n,{Z:()=>t});let t=i.p+"assets/images/rt-2-cot-eb2bee68ad29cc277f1a214c22064f0e.png"},59338:function(e,n,i){i.d(n,{Z:()=>l,a:()=>o});var t=i(52136);let s={},r=t.createContext(s);function o(e){let n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}},75528:function(e){e.exports=JSON.parse('{"permalink":"/2025/08/24/rt-2-review","source":"@site/blog/2025/08/24/rt-2-review.md","title":"RT-2, Robotic Transformer 2 Review","description":"RT-2 review","date":"2025-08-24T13:40:19.433Z","tags":[{"inline":true,"label":"vlm","permalink":"/tags/vlm"}],"readingTime":3.56,"hasTruncateMarker":false,"authors":[{"name":"Eunkwang Shin","title":"Owner","url":"https://github.com/gracefullight","socials":{"linkedin":"https://www.linkedin.com/in/gracefullight/","github":"https://github.com/gracefullight"},"description":"Full Stack JavaScript Developer | Half-time Open Sourcerer.","page":{"permalink":"/authors/me"},"imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me"}],"frontMatter":{"title":"RT-2, Robotic Transformer 2 Review","date":"2025-08-24T13:40:19.433Z","description":"RT-2 review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"Fundamentals of software development @005","permalink":"/2025/08/25/fundamentals-of-software-development-005"},"nextItem":{"title":"PaLM-E An Embodied Multimodal Language Model Review","permalink":"/2025/08/24/palm-e-review"}}')}}]);