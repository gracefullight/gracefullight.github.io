"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["44513"],{36341:function(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var r=i(51529),s=i(69979),t=i(59338);let o={title:"Open X-Embodiment review",date:new Date("2025-09-01T03:47:28.350Z"),description:"Open X-Embodiment review",authors:"me",tags:["vlm"]},a=void 0,l={authorsImageUrls:[void 0]},d=[{value:"RT-X",id:"rt-x",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Objectives",id:"objectives",level:2},{value:"Core Approach",id:"core-approach",level:2},{value:"What\u2019s Different From Prior Transfer Methods",id:"whats-different-from-prior-transfer-methods",level:2},{value:"Dataset &amp; Format (Open X-Embodiment)",id:"dataset--format-open-x-embodiment",level:2},{value:"Data Format Consolidation (Coarse Alignment)",id:"data-format-consolidation-coarse-alignment",level:2},{value:"Policy Architectures",id:"policy-architectures",level:2},{value:"Training Setup",id:"training-setup",level:2},{value:"Experimental Questions",id:"experimental-questions",level:2},{value:"Key Results",id:"key-results",level:2},{value:"Generalization &amp; Emergent Skills",id:"generalization--emergent-skills",level:2},{value:"Design Insights (Ablations)",id:"design-insights-ablations",level:2},{value:"Limitations (Open Problems)",id:"limitations-open-problems",level:2},{value:"Ref",id:"ref",level:2}];function c(e){let n={a:"a",code:"code",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"rt-x",children:"RT-X"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"RT-X trains generalist robot policies by co-training RT-1/RT-2 on an X-embodiment mix of multi-robot, multi-task data, enabling efficient adaptation to new robots, tasks, and environments."}),"\n",(0,s.jsx)(n.li,{children:"It standardizes 1M+ trajectories from 22 embodiments into the Open X-Embodiment (RLDS/tfrecord) repository, unifying observations and 7-DoF actions via coarse alignment."}),"\n",(0,s.jsx)(n.li,{children:"Experiments show strong positive transfer and emergent skills (\u22483\xd7 with RT-2-X on cross-robot tasks); performance scales with model capacity, short image histories, and web pretraining, while sensing/actuation diversity and frame alignment remain open problems."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"RT-X Architecture",src:i(10968).Z+"",width:"1552",height:"412"})}),"\n",(0,s.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Seeks a ",(0,s.jsx)(n.strong,{children:"generalist X-robot policy"})," that can be efficiently adapted to new robots, tasks, and environments."]}),"\n",(0,s.jsxs)(n.li,{children:["Mirrors a trend from CV/NLP where ",(0,s.jsx)(n.strong,{children:"general-purpose, web-scale pretrained models"})," outperform narrow, task-specific models."]}),"\n",(0,s.jsxs)(n.li,{children:["Robotics lacks comparably large, diverse ",(0,s.jsx)(n.strong,{children:"interaction datasets"}),", making direct transfer of these lessons challenging."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Positive transfer:"})," Test whether co-training on data from many robots improves performance on each training domain."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ecosystem building:"})," Organize large robotic datasets to enable future X-embodiment research."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"core-approach",children:"Core Approach"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Train ",(0,s.jsx)(n.strong,{children:"RT-1"})," and ",(0,s.jsx)(n.strong,{children:"RT-2"})," on data from ",(0,s.jsx)(n.strong,{children:"9 different manipulators"}),", producing ",(0,s.jsx)(n.strong,{children:"RT-X"})," variants that outperform policies trained only on the evaluation domain and show ",(0,s.jsx)(n.strong,{children:"better generalization"})," and ",(0,s.jsx)(n.strong,{children:"new capabilities"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"whats-different-from-prior-transfer-methods",children:"What\u2019s Different From Prior Transfer Methods"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Many prior works reduce the ",(0,s.jsx)(n.strong,{children:"embodiment gap"})," via specialized mechanisms (shared action spaces, representation learning objectives, policy adaptation using embodiment metadata, decoupled robot/environment representations, domain translation)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RT-X directly trains on X-embodiment data without explicit gap-reduction machinery"})," and still observes ",(0,s.jsx)(n.strong,{children:"positive transfer"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"dataset--format-open-x-embodiment",children:"Dataset & Format (Open X-Embodiment)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"1M+ real robot trajectories, 22 embodiments"})," (single-arm, bimanual, quadrupeds), pooled from ",(0,s.jsx)(n.strong,{children:"60 datasets / 34 labs"}),", standardized for easy use."]}),"\n",(0,s.jsxs)(n.li,{children:["Uses ",(0,s.jsx)(n.a,{href:"https://github.com/google-research/rlds",children:(0,s.jsx)(n.strong,{children:"RLDS"})})," (serialized ",(0,s.jsx)(n.code,{children:"tfrecord"}),"), supporting varied action spaces and input modalities (RGB, depth, point clouds), and efficient parallel loading across major DL frameworks."]}),"\n",(0,s.jsxs)(n.li,{children:["Language annotations are leveraged; ",(0,s.jsx)(n.strong,{children:"PaLM"})," is used to extract objects/behaviors from instructions."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"RLDS",src:i(76795).Z+"",width:"726",height:"353"})}),"\n",(0,s.jsx)(n.h2,{id:"data-format-consolidation-coarse-alignment",children:"Data Format Consolidation (Coarse Alignment)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Observations:"})," History of recent images + language instruction. One ",(0,s.jsx)(n.strong,{children:"canonical camera view"})," per dataset is resized to a common resolution."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Actions:"})," Convert original controls to a ",(0,s.jsx)(n.strong,{children:"7-DoF end-effector vector"})," (x, y, z, roll, pitch, yaw, gripper or their rates). Actions are ",(0,s.jsx)(n.strong,{children:"normalized before discretization"}),"; outputs are ",(0,s.jsx)(n.strong,{children:"de-normalized per embodiment"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deliberate non-alignment:"})," Camera poses/properties are ",(0,s.jsx)(n.strong,{children:"not"})," standardized; action frame alignment across datasets is ",(0,s.jsx)(n.strong,{children:"not"})," enforced. The same action vector may cause ",(0,s.jsx)(n.strong,{children:"different motions"})," on different robots (absolute/relative, position/velocity allowed)."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"policy-architectures",children:"Policy Architectures"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RT-1 (\u224835M params):"})," Transformer for control. Inputs: 15-frame image history + natural-language instruction.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Vision via ImageNet-pretrained ",(0,s.jsx)(n.strong,{children:"EfficientNet"}),"; language via ",(0,s.jsx)(n.strong,{children:"USE"})," embedding."]}),"\n",(0,s.jsxs)(n.li,{children:["Fuse via ",(0,s.jsx)(n.strong,{children:"FiLM"})," \u2192 81 vision\u2013language tokens \u2192 ",(0,s.jsx)(n.strong,{children:"decoder-only Transformer"})," outputs tokenized actions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RT-2 (VLA family):"})," Internet-scale VLM co-fine-tuned to output ",(0,s.jsx)(n.strong,{children:"action as text tokens"})," (e.g., ",(0,s.jsx)(n.code,{children:"1 128 91 241 5 101 127"}),").","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Any pretrained VLM can be adapted; this work uses ",(0,s.jsx)(n.strong,{children:"RT-2\u2013PaLI-X"})," (ViT backbone + UL2 LM; primarily pretrained on WebLI)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"training-setup",children:"Training Setup"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robotics data mixture:"})," Data from ",(0,s.jsx)(n.strong,{children:"9 manipulators"})," (a union of multiple well-known robotics datasets)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Loss:"})," Standard ",(0,s.jsx)(n.strong,{children:"categorical cross-entropy"})," over tokenized actions."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Regimes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RT-1-X:"})," Trained solely on the robotics mixture."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RT-2-X:"})," ",(0,s.jsx)(n.strong,{children:"Co-fine-tuned"})," on a ~1:1 mix of original VLM data and the robotics mixture."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"experimental-questions",children:"Experimental Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Does X-embodiment co-training improve in-domain performance (positive transfer)?"}),"\n",(0,s.jsxs)(n.li,{children:["Does it improve ",(0,s.jsx)(n.strong,{children:"generalization"})," to ",(0,s.jsx)(n.strong,{children:"unseen tasks"}),"?"]}),"\n",(0,s.jsxs)(n.li,{children:["How do ",(0,s.jsx)(n.strong,{children:"model size"}),", ",(0,s.jsx)(n.strong,{children:"architecture"}),", and ",(0,s.jsx)(n.strong,{children:"dataset composition"})," influence performance/generalization?"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-results",children:"Key Results"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Small-scale domains:"})," ",(0,s.jsx)(n.strong,{children:"RT-1-X"})," outperforms the ",(0,s.jsx)(n.strong,{children:"Original Method"})," (the authors\u2019 per-dataset baselines) on ",(0,s.jsx)(n.strong,{children:"4/5"})," datasets with a large average gain \u2192 ",(0,s.jsx)(n.strong,{children:"limited data domains"})," benefit greatly from X-embodiment co-training."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large-scale domains:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RT-1-X"})," does ",(0,s.jsx)(n.strong,{children:"not"})," beat an RT-1 trained only on the embodiment-specific large dataset (suggests underfitting for this class)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RT-2-X"})," (larger capacity) ",(0,s.jsx)(n.strong,{children:"outperforms both"})," Original Method and RT-1 \u2192 X-robot training helps even in ",(0,s.jsx)(n.strong,{children:"data-rich"})," regimes when using ",(0,s.jsx)(n.strong,{children:"sufficient capacity"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"generalization--emergent-skills",children:"Generalization & Emergent Skills"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Unseen objects/backgrounds/environments:"})," RT-2 and RT-2-X perform ",(0,s.jsx)(n.strong,{children:"on par"})," (VLM backbone already strong here)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Emergent skills (transfer across robots):"})," On Google Robot tasks that ",(0,s.jsx)(n.strong,{children:"do not appear"})," in RT-2\u2019s dataset but exist in ",(0,s.jsx)(n.strong,{children:"Bridge"})," (for ",(0,s.jsx)(n.strong,{children:"WidowX"}),"), ",(0,s.jsx)(n.strong,{children:"RT-2-X \u2248 3\xd7"})," RT-2.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Removing ",(0,s.jsx)(n.strong,{children:"Bridge"})," from RT-2-X training ",(0,s.jsx)(n.strong,{children:"significantly reduces"})," hold-out performance \u2192 skills likely ",(0,s.jsx)(n.strong,{children:"transferred"})," from WidowX data."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"design-insights-ablations",children:"Design Insights (Ablations)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Short image history"})," notably ",(0,s.jsx)(n.strong,{children:"improves generalization"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Web pretraining"})," is ",(0,s.jsx)(n.strong,{children:"critical"})," for large models\u2019 high performance."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model capacity matters:"})," ",(0,s.jsx)(n.strong,{children:"55B"})," model succeeds more than ",(0,s.jsx)(n.strong,{children:"5B"})," on emergent skills \u2192 greater capacity \u21D2 greater cross-dataset transfer."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Co-fine-tuning vs. fine-tuning:"})," Similar performance in this study (attributed to the ",(0,s.jsx)(n.strong,{children:"greater diversity"})," of robotics data in RT-2-X vs. prior works)."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"limitations-open-problems",children:"Limitations (Open Problems)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Does ",(0,s.jsx)(n.strong,{children:"not"})," cover robots with ",(0,s.jsx)(n.strong,{children:"very different sensing/actuation modalities"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Does ",(0,s.jsx)(n.strong,{children:"not"})," study generalization to ",(0,s.jsx)(n.strong,{children:"new robots"})," nor define a ",(0,s.jsx)(n.strong,{children:"decision criterion"})," for when positive transfer will occur."]}),"\n",(0,s.jsxs)(n.li,{children:["Camera pose/properties and control frame ",(0,s.jsx)(n.strong,{children:"remain unaligned"}),"; a deliberate but still challenging domain gap to address in future work."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"ref",children:"Ref"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"O\u2019Neill, A., Rehman, A., Maddukuri, A., Gupta, A., Padalkar, A., Lee, A., Pooley, A., Gupta, A., Mandlekar, A., & Jain, A. (2024). Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. 2024 IEEE International Conference on Robotics and Automation (ICRA)."}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},76795:function(e,n,i){i.d(n,{Z:()=>r});let r=i.p+"assets/images/rlds-5e68d1c660ef048892d5594530c62239.png"},10968:function(e,n,i){i.d(n,{Z:()=>r});let r=i.p+"assets/images/rt-x-architecture-cc2128128460577bc8f720626e0d671d.png"},59338:function(e,n,i){i.d(n,{Z:()=>a,a:()=>o});var r=i(52136);let s={},t=r.createContext(s);function o(e){let n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}},51529:function(e){e.exports=JSON.parse('{"permalink":"/2025/09/01/open-x-embodiment-review","source":"@site/blog/2025/09/01/open-x-embodiment-review.md","title":"Open X-Embodiment review","description":"Open X-Embodiment review","date":"2025-09-01T03:47:28.350Z","tags":[{"inline":false,"label":"vlm","permalink":"/tags/vlm","description":"Vision-Language Models"}],"readingTime":4.57,"hasTruncateMarker":false,"authors":[{"name":"Eunkwang Shin","title":"Owner","url":"https://github.com/gracefullight","socials":{"linkedin":"https://www.linkedin.com/in/gracefullight/","github":"https://github.com/gracefullight"},"description":"Full Stack JavaScript Developer | Half-time Open Sourcerer.","page":{"permalink":"/authors/me"},"imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me"}],"frontMatter":{"title":"Open X-Embodiment review","date":"2025-09-01T03:47:28.350Z","description":"Open X-Embodiment review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"Introduction to AI @005","permalink":"/2025/09/02/introduction-to-ai-005"},"nextItem":{"title":"Fundamentals of software development @006","permalink":"/2025/09/01/fundamentals-of-software-development-006"}}')}}]);