"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["25151"],{24781:function(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var s=i(21840),r=i(65813),t=i(661);let o={title:"OpenVLA Review",date:new Date("2025-08-29T07:43:25.796Z"),description:"OpenVLA Review",authors:"me",tags:["vlm"]},l,a={authorsImageUrls:[void 0]},c=[{value:"OpenVLA",id:"openvla",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Model &amp; Training",id:"model--training",level:2},{value:"Architecture &amp; Approach",id:"architecture--approach",level:2},{value:"Performance",id:"performance",level:2},{value:"Efficiency",id:"efficiency",level:2},{value:"Evaluations",id:"evaluations",level:2},{value:"Design Insights",id:"design-insights",level:2},{value:"Limitations &amp; Future Work",id:"limitations--future-work",level:2},{value:"Contributions",id:"contributions",level:2},{value:"Ref",id:"ref",level:2}];function d(e){let n={code:"code",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"openvla",children:"OpenVLA"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"OpenVLA is a 7B open-source VLA model built on Llama2 + DINOv2 + SigLIP, trained on 970k demos, achieving stronger generalization and robustness than closed RT-2-X (55B) and outperforming Diffusion Policy."}),"\n",(0,r.jsx)(n.li,{children:"It introduces efficient adaptation via LoRA (1.4% params, 8\xd7 compute reduction) and 4-bit quantization (half memory, same accuracy), enabling fine-tuning and inference on consumer GPUs."}),"\n",(0,r.jsxs)(n.li,{children:["Limitations remain (single-image input, ",(0,r.jsx)(n.code,{children:"<90%"})," reliability, limited throughput), but OpenVLA provides the first open, scalable framework for generalist robot policies."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"OpenVLA Architecture",src:i(10366).A+"",width:"915",height:"366"})}),"\n",(0,r.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Training robot policies from scratch struggles with robustness and generalization."}),"\n",(0,r.jsxs)(n.li,{children:["Fine-tuning ",(0,r.jsx)(n.strong,{children:"vision-language-action (VLA)"})," models offers reusable, generalizable visuomotor policies."]}),"\n",(0,r.jsxs)(n.li,{children:["Barriers: prior VLAs are ",(0,r.jsx)(n.strong,{children:"closed-source"}),", lack best practices for adaptation, and need server-class hardware."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"model--training",children:"Model & Training"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenVLA"}),": 7B parameters, open-source."]}),"\n",(0,r.jsxs)(n.li,{children:["Built on ",(0,r.jsx)(n.strong,{children:"Llama 2"})," with fused ",(0,r.jsx)(n.strong,{children:"DINOv2 + SigLIP"})," vision encoders."]}),"\n",(0,r.jsxs)(n.li,{children:["Trained on ",(0,r.jsx)(n.strong,{children:"970k robot demonstrations"})," from Open-X Embodiment dataset."]}),"\n",(0,r.jsxs)(n.li,{children:["Represents robot actions as ",(0,r.jsx)(n.strong,{children:"tokens"})," (discretized into 256 bins, replacing unused Llama tokens)."]}),"\n",(0,r.jsxs)(n.li,{children:["Standard ",(0,r.jsx)(n.strong,{children:"next-token prediction"})," objective."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"architecture--approach",children:"Architecture & Approach"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"End-to-end fine-tuning of VLM to generate robot actions as tokens."}),"\n",(0,r.jsx)(n.li,{children:"Differs from modular methods (e.g., Octo) that stitch separate encoders/decoders."}),"\n",(0,r.jsx)(n.li,{children:"Vision features are obtained by encoding the same input image with both SigLIP and DINOv2, then channel-wise concatenated and passed through an MLP projector. This preserves SigLIP\u2019s semantic alignment with language and DINOv2's spatial reasoning, giving the VLM richer multimodal context for manipulation tasks."}),"\n",(0,r.jsx)(n.li,{children:"Uses Prismatic VLM backbone with multi-resolution features (spatial reasoning + semantics)."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"performance",children:"Performance"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Outperforms closed ",(0,r.jsx)(n.strong,{children:"RT-2-X (55B)"})," by ",(0,r.jsx)(n.strong,{children:"+16.5% task success"})," with 7\xd7 fewer parameters."]}),"\n",(0,r.jsxs)(n.li,{children:["Beats ",(0,r.jsx)(n.strong,{children:"Diffusion Policy"})," (from-scratch imitation learning) by ",(0,r.jsx)(n.strong,{children:"+20.4%"})," on multi-task language-grounded settings."]}),"\n",(0,r.jsxs)(n.li,{children:["Demonstrates ",(0,r.jsx)(n.strong,{children:"robust behaviors"})," (distractor resistance, error recovery)."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"efficiency",children:"Efficiency"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Introduces ",(0,r.jsx)(n.strong,{children:"parameter-efficient fine-tuning"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LoRA"})," updates only 1.4% of parameters yet matches full fine-tuning."]}),"\n",(0,r.jsx)(n.li,{children:"Can fine-tune on a single A100 GPU in ~10\u201315 hours (8\xd7 compute reduction)."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Quantization"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"4-bit inference matches bfloat16 accuracy while halving memory footprint."}),"\n",(0,r.jsx)(n.li,{children:"Runs at 3Hz on consumer GPUs (e.g., A5000, 16GB)."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"evaluations",children:"Evaluations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Tested across ",(0,r.jsx)(n.strong,{children:"29 tasks"})," and multiple robots (WidowX, Google robot, Franka)."]}),"\n",(0,r.jsxs)(n.li,{children:["Strong generalization on:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual"})," (unseen backgrounds/distractors)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Motion"})," (new object positions/orientations)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Physical"})," (new object shapes/sizes)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Semantic"})," (unseen tasks, instructions)."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["First generalist open-source VLA achieving ",(0,r.jsx)(n.strong,{children:"\u226550% success rate across all tested tasks"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"design-insights",children:"Design Insights"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fine-tuning the vision encoder"})," (vs. freezing) crucial for robotic control."]}),"\n",(0,r.jsx)(n.li,{children:"Higher image resolution (384px vs. 224px) adds 3\xd7 compute without performance gains."}),"\n",(0,r.jsxs)(n.li,{children:["Training required ",(0,r.jsx)(n.strong,{children:"27 epochs"}),", far more than typical VLM runs, to surpass 95% action token accuracy."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"limitations--future-work",children:"Limitations & Future Work"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Supports only ",(0,r.jsx)(n.strong,{children:"single-image observations"})," (no proprioception, no history)."]}),"\n",(0,r.jsx)(n.li,{children:"Inference throughput (~6Hz on RTX 4090) insufficient for high-frequency control (e.g., ALOHA at 50Hz)."}),"\n",(0,r.jsx)(n.li,{children:"Success rates remain below 90% in challenging tasks."}),"\n",(0,r.jsxs)(n.li,{children:["Open questions:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Impact of base VLM size on performance."}),"\n",(0,r.jsx)(n.li,{children:"Benefits of co-training with Internet-scale data."}),"\n",(0,r.jsx)(n.li,{children:"Best visual features for VLAs."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"contributions",children:"Contributions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["First ",(0,r.jsx)(n.strong,{children:"open-source generalist VLA"})," with strong performance."]}),"\n",(0,r.jsxs)(n.li,{children:["Scalable ",(0,r.jsx)(n.strong,{children:"end-to-end training"})," pipeline (action-as-token)."]}),"\n",(0,r.jsxs)(n.li,{children:["Demonstrates ",(0,r.jsx)(n.strong,{children:"LoRA + quantization"})," for consumer-grade GPU adaptation."]}),"\n",(0,r.jsxs)(n.li,{children:["Provides ",(0,r.jsx)(n.strong,{children:"code, checkpoints, and data curation recipes"})," to support future research."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"ref",children:"Ref"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E. P., Sanketi, P. R., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake, R., Sadigh, D., Levine, S., Liang, P., & Finn, C. (2025). OpenVLA: An Open-Source Vision-Language-Action Model Proceedings of The 8th Conference on Robot Learning, Proceedings of Machine Learning Research. ",(0,r.jsx)(n.code,{children:"https://proceedings.mlr.press/v270/kim25c.html"})]}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},10366:function(e,n,i){i.d(n,{A:()=>s});let s=i.p+"assets/images/open-vla-architecture-df4421cc82c1ebceca0ccf1061cd4593.png"},661:function(e,n,i){i.d(n,{R:()=>o,x:()=>l});var s=i(59729);let r={},t=s.createContext(r);function o(e){let n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}},21840:function(e){e.exports=JSON.parse('{"permalink":"/2025/08/29/open-vla-review","source":"@site/blog/2025/08/29/open-vla-review.md","title":"OpenVLA Review","description":"OpenVLA Review","date":"2025-08-29T07:43:25.796Z","tags":[{"inline":false,"label":"vlm","permalink":"/tags/vlm","description":"Vision-Language Models"}],"readingTime":2.96,"hasTruncateMarker":false,"authors":[{"name":"Eunkwang Shin","title":"Owner","url":"https://github.com/gracefullight","socials":{"linkedin":"https://www.linkedin.com/in/gracefullight/","github":"https://github.com/gracefullight","email":"mailto:gracefullight.dev@gmail.com"},"description":"Full Stack JavaScript Developer | Half-time Open Sourcerer.","page":{"permalink":"/authors/me"},"imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me"}],"frontMatter":{"title":"OpenVLA Review","date":"2025-08-29T07:43:25.796Z","description":"OpenVLA Review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"RoboFlamingo Review","permalink":"/2025/08/31/roboflamingo-review"},"nextItem":{"title":"\uB370\uC774\uD130 \uC2DC\uAC01\uD654 \uC758\uC0AC \uACB0\uC815 \uD2B8\uB9AC","permalink":"/2025/08/28/data-visualization-desicion-tree"}}')}}]);