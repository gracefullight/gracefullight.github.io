"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["53013"],{5929:function(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>d});var s=i(39631),t=i(69979),r=i(56774);let l={title:"PaLM-E An Embodied Multimodal Language Model Review",date:new Date("2025-08-24T10:33:27.131Z"),description:"PaLM-E An Embodied Multimodal Language Model Review",authors:"me",tags:["vlm"]},o=void 0,a={authorsImageUrls:[void 0]},d=[{value:"PaLM-E",id:"palm-e",level:2},{value:"Core idea",id:"core-idea",level:2},{value:"Architecture &amp; representations",id:"architecture--representations",level:2},{value:"Training setup",id:"training-setup",level:2},{value:"Planning &amp; control loop",id:"planning--control-loop",level:2},{value:"Why not text-only LLMs or affordance-only grounding?",id:"why-not-text-only-llms-or-affordance-only-grounding",level:2},{value:"Environments &amp; use cases",id:"environments--use-cases",level:2},{value:"Results (high level)",id:"results-high-level",level:2},{value:"Takeaways",id:"takeaways",level:2}];function c(e){let n={code:"code",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"palm-e",children:"PaLM-E"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"ViT (e.g., ViT-4B, ViT-22B) extracts image embeddings."}),"\n",(0,t.jsx)(n.li,{children:"OSRT builds object-centric slot representations."}),"\n",(0,t.jsx)(n.li,{children:"These are injected into the LLM embedding space (PaLM variants: 8B, 62B, 540B) for high-level abstraction and planning, with execution delegated to low-level policies (e.g., RT-1)."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"PaLM-E Architecture",src:i(52118).Z+"",width:"1284",height:"650"})}),"\n",(0,t.jsx)(n.h2,{id:"core-idea",children:"Core idea"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Build ",(0,t.jsx)(n.strong,{children:"embodied language models"})," by ",(0,t.jsx)(n.strong,{children:"injecting continuous sensor inputs"})," (images, states, other modalities) directly into a ",(0,t.jsx)(n.strong,{children:"pretrained LLM\u2019s embedding space"}),", linking ",(0,t.jsx)(n.strong,{children:"words \u2194 percepts"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Inputs are ",(0,t.jsx)(n.strong,{children:"multimodal sentences"})," that ",(0,t.jsx)(n.strong,{children:"interleave"})," text tokens with encoded visual/state tokens; outputs are ",(0,t.jsx)(n.strong,{children:"text"})," (answers or high-level plans)."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"architecture--representations",children:"Architecture & representations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Start from a ",(0,t.jsx)(n.strong,{children:"decoder-only, autoregressive LLM"})," (PaLM) and ",(0,t.jsx)(n.strong,{children:"condition on a prefix"})," that mixes text and ",(0,t.jsx)(n.strong,{children:"encoder-produced vectors"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Provide multiple encoder options:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"State vectors"})," (simplest)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ViT"})," features with a learned ",(0,t.jsx)(n.strong,{children:"projector \u03C8"})," to match LLM embedding dimensionality."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object-centric, 3D-aware OSRT"})," (neural scene representations). Supports ",(0,t.jsx)(n.strong,{children:"entity-label tokens"})," (",(0,t.jsx)(n.code,{children:"<obj j>"}),") so the model can refer to specific objects in generated plans."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"training-setup",children:"Training setup"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Train ",(0,t.jsx)(n.strong,{children:"end-to-end"})," (encoders + projector + optionally the LLM) to output ",(0,t.jsx)(n.strong,{children:"sequential decisions as natural text"})," or answers (VQA, captioning)."]}),"\n",(0,t.jsxs)(n.li,{children:["Dataset items contain ",(0,t.jsx)(n.strong,{children:"(continuous observations, text sequence, prefix index)"}),"; loss is ",(0,t.jsx)(n.strong,{children:"cross-entropy on non-prefix tokens"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Explore ",(0,t.jsx)(n.strong,{children:"freezing the LLM"})," (train encoders/projection only), and ",(0,t.jsx)(n.strong,{children:"co-training"}),' across diverse tasks ("full mixture"; only ~9% is embodied data).']}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"planning--control-loop",children:"Planning & control loop"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["For ",(0,t.jsx)(n.strong,{children:"planning/control"}),", PaLM-E emits ",(0,t.jsx)(n.strong,{children:"textual subgoals/skills"})," drawn from a small skill vocabulary; a separate ",(0,t.jsx)(n.strong,{children:"low-level policy"})," executes them."]}),"\n",(0,t.jsxs)(n.li,{children:["The system runs ",(0,t.jsx)(n.strong,{children:"closed-loop"}),": execute \u2192 observe \u2192 (re)plan; PaLM-E acts as a ",(0,t.jsx)(n.strong,{children:"high-level policy"})," sequencing low-level skills."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"why-not-text-only-llms-or-affordance-only-grounding",children:"Why not text-only LLMs or affordance-only grounding?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Prior work that feeds ",(0,t.jsx)(n.strong,{children:"only text to the LLM"})," (and uses external affordance models) is ",(0,t.jsx)(n.strong,{children:"insufficient"})," when ",(0,t.jsx)(n.strong,{children:"spatial layout"})," matters."]}),"\n",(0,t.jsxs)(n.li,{children:["PaLM-E instead ",(0,t.jsx)(n.strong,{children:"grounds inside the LLM"})," by ",(0,t.jsx)(n.strong,{children:"injecting continuous observations"}),", enabling ",(0,t.jsx)(n.strong,{children:"direct plan generation"})," while leveraging the LLM\u2019s world knowledge."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"environments--use-cases",children:"Environments & use cases"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Three domains"}),": ",(0,t.jsx)(n.strong,{children:"TAMP"})," (grasp/stack planning), ",(0,t.jsx)(n.strong,{children:"Language-Table"})," (multi-object tabletop pushing), ",(0,t.jsx)(n.strong,{children:"Mobile manipulation"})," (kitchen tasks)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use cases"})," to test embodied reasoning: ",(0,t.jsx)(n.strong,{children:"affordance prediction"}),", ",(0,t.jsx)(n.strong,{children:"failure detection"}),", ",(0,t.jsx)(n.strong,{children:"long-horizon planning"})," (low-level policies from ",(0,t.jsx)(n.strong,{children:"RT-1"}),")."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"results-high-level",children:"Results (high level)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transfer via co-training"}),": One model trained on mixed tasks/embodiments achieves ",(0,t.jsx)(n.strong,{children:"higher performance"}),' than task-specialists; "full mixture" yields ',(0,t.jsx)(n.code,{children:">2\xd7"})," gains (Fig. 3)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Few-shot/data efficiency"}),": Solves robotics tasks with ",(0,t.jsx)(n.strong,{children:"very few examples"})," (e.g., ",(0,t.jsx)(n.strong,{children:"10\u201380"})," for Language-Table, ",(0,t.jsx)(n.strong,{children:"320"})," for TAMP). ",(0,t.jsx)(n.strong,{children:"OSRT"})," further improves data efficiency."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mobile manipulation"}),": End-to-end embodied planning works in real kitchens, robust to disturbances; PaLM-E beats ",(0,t.jsx)(n.strong,{children:"PaLI (zero-shot)"})," and ",(0,t.jsx)(n.strong,{children:"QT-OPT/CLIP baselines"})," on affordance/failure detection."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"General V+L"}),": The ",(0,t.jsx)(n.strong,{children:"562B"})," generalist achieves ",(0,t.jsx)(n.strong,{children:"state-of-the-art on OK-VQA"})," and strong VQAv2/COCO without task-specific finetuning."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language retention & scaling"}),": ",(0,t.jsx)(n.strong,{children:"Freezing LLM"})," preserves language ability but can struggle on some robotics tasks; ",(0,t.jsx)(n.strong,{children:"unfrozen + scale up"})," significantly reduces ",(0,t.jsx)(n.strong,{children:"catastrophic forgetting"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Emergent behaviors"}),": ",(0,t.jsx)(n.strong,{children:"Multimodal chain-of-thought"})," and ",(0,t.jsx)(n.strong,{children:"multi-image reasoning"})," emerge in ",(0,t.jsx)(n.strong,{children:"PaLM-E-562B"}),", despite training on ",(0,t.jsx)(n.strong,{children:"single-image prompts"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"takeaways",children:"Takeaways"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Injecting ",(0,t.jsx)(n.strong,{children:"neural scene representations (OSRT)"})," and ",(0,t.jsx)(n.strong,{children:"entity-labeled multimodal tokens"})," is ",(0,t.jsx)(n.strong,{children:"effective"})," even without massive embodied data."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Diverse, joint training"})," transfers vision-language knowledge ",(0,t.jsx)(n.strong,{children:"into embodied decision-making"}),", enabling ",(0,t.jsx)(n.strong,{children:"data-efficient robot planning"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Two viable paths to retain language skills during multimodal finetuning:","\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Freeze the LLM"}),", train encoders (max language retention, sometimes weaker robotics),"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unfreeze and scale"})," the LLM (much less forgetting, strong embodied performance)."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},52118:function(e,n,i){i.d(n,{Z:()=>s});let s=i.p+"assets/images/palm-e-architecture-ca3f220adc8e114bc89a0589703866fb.png"},56774:function(e,n,i){i.d(n,{Z:()=>o,a:()=>l});var s=i(52136);let t={},r=s.createContext(t);function l(e){let n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}},39631:function(e){e.exports=JSON.parse('{"permalink":"/2025/08/24/palm-e-review","source":"@site/blog/2025/08/24/palm-e-review.md","title":"PaLM-E An Embodied Multimodal Language Model Review","description":"PaLM-E An Embodied Multimodal Language Model Review","date":"2025-08-24T10:33:27.131Z","tags":[{"inline":true,"label":"vlm","permalink":"/tags/vlm"}],"readingTime":2.86,"hasTruncateMarker":false,"authors":[{"name":"Eunkwang Shin","title":"Owner","url":"https://github.com/gracefullight","socials":{"linkedin":"https://www.linkedin.com/in/gracefullight/","github":"https://github.com/gracefullight"},"description":"Full Stack JavaScript Developer | Half-time Open Sourcerer.","page":{"permalink":"/authors/me"},"imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me"}],"frontMatter":{"title":"PaLM-E An Embodied Multimodal Language Model Review","date":"2025-08-24T10:33:27.131Z","description":"PaLM-E An Embodied Multimodal Language Model Review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"RT-2, Robotic Transformer 2 Review","permalink":"/2025/08/24/rt-2-review"},"nextItem":{"title":"RT-1, Robot Transformer 1 Review","permalink":"/2025/08/24/rt-1-review"}}')}}]);