"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["87844"],{90717(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});var s=i(87705),t=i(74848),r=i(28453);let l={title:"\u03C00 Review",date:new Date("2025-08-31T14:24:33.716Z"),description:"\u03C00 Review",authors:"me",tags:["vlm"]},a,o={authorsImageUrls:[void 0]},c=[{value:"\u03C00",id:"\u03C00",level:2},{value:"Problem &amp; Motivation",id:"problem--motivation",level:2},{value:"Core Proposal",id:"core-proposal",level:2},{value:"Training Recipe (Pre- vs Post-Training)",id:"training-recipe-pre--vs-post-training",level:2},{value:"Data &amp; Backbone",id:"data--backbone",level:2},{value:"Modeling Details",id:"modeling-details",level:2},{value:"High-Level Language Policy",id:"high-level-language-policy",level:2},{value:"Evaluation Setup &amp; Baselines",id:"evaluation-setup--baselines",level:2},{value:"Key Results",id:"key-results",level:2},{value:"Takeaways &amp; Limitations",id:"takeaways--limitations",level:2},{value:"Ref",id:"ref",level:2}];function d(e){let n={code:"code",h2:"h2",li:"li",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"\u03C00",children:"\u03C00"}),"\n",(0,t.jsx)(n.h2,{id:"problem--motivation",children:"Problem & Motivation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Achieving real-world generality in robot learning is blocked by ",(0,t.jsx)(n.strong,{children:"data scarcity, generalization, and robustness"})," limits."]}),"\n",(0,t.jsxs)(n.li,{children:["Human intelligence most outpaces machines in ",(0,t.jsx)(n.strong,{children:"versatility"}),"\u2014solving diverse, physically situated tasks under constraints, language commands, and perturbations."]}),"\n",(0,t.jsxs)(n.li,{children:["In NLP/CV, ",(0,t.jsx)(n.strong,{children:"foundation models"})," pre-trained on diverse multi-task data, then ",(0,t.jsx)(n.strong,{children:"fine-tuned (aligned)"})," on curated datasets, outperform narrow specialists; the same paradigm is hypothesized for robotics."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"core-proposal",children:"Core Proposal"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["A ",(0,t.jsx)(n.strong,{children:"novel flow-matching architecture"})," built on a pre-trained ",(0,t.jsx)(n.strong,{children:"Vision-Language Model (VLM)"})," to inherit Internet-scale semantics."]}),"\n",(0,t.jsxs)(n.li,{children:["Further training adds ",(0,t.jsx)(n.strong,{children:"robot actions"}),", turning the model into a ",(0,t.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," policy."]}),"\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.strong,{children:"cross-embodiment training"})," to combine data from many robot types (single/dual-arm, mobile), despite differing configuration/action spaces."]}),"\n",(0,t.jsxs)(n.li,{children:["Employ ",(0,t.jsx)(n.strong,{children:"action chunking"})," + ",(0,t.jsx)(n.strong,{children:"flow matching"})," (diffusion variant) to model complex, continuous, high-frequency actions."]}),"\n",(0,t.jsxs)(n.li,{children:["Introduce an ",(0,t.jsx)(n.strong,{children:"Action Expert"})," (separate weights for action/state tokens), akin to a ",(0,t.jsx)(n.strong,{children:"Mixture-of-Experts"}),", augmenting the standard VLM."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"training-recipe-pre--vs-post-training",children:"Training Recipe (Pre- vs Post-Training)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pre-training"})," on highly diverse data builds broad, general physical abilities."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Post-training"})," on curated, task-specific data instills ",(0,t.jsx)(n.strong,{children:"fluent, efficient strategies"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Rationale: high-quality-only training lacks recovery behaviors; low-quality-only training lacks efficiency/robustness; ",(0,t.jsx)(n.strong,{children:"combining both"})," yields desired behavior."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"data--backbone",children:"Data & Backbone"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["~",(0,t.jsx)(n.strong,{children:"10,000 hours"})," of demonstrations + the ",(0,t.jsx)(n.strong,{children:"OXE"})," dataset; data spans ",(0,t.jsx)(n.strong,{children:"7 robot configurations"})," and ",(0,t.jsx)(n.strong,{children:"68 tasks"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["VLM backbone initialized from ",(0,t.jsx)(n.strong,{children:"PaliGemma (3B)"}),"; add ",(0,t.jsx)(n.strong,{children:"~300M"})," parameters for the action expert (total ",(0,t.jsx)(n.strong,{children:"~3.3B"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:["Pre-training mixture: weighted combination of internal datasets + full OXE; ",(0,t.jsx)(n.strong,{children:"n^0.43"})," weighting to down-weight overrepresented task-robot pairs."]}),"\n",(0,t.jsxs)(n.li,{children:["Unify interfaces: zero-pad ",(0,t.jsx)(n.strong,{children:"qt/at"})," to the largest robot dimension (18); mask missing image slots; late-fusion encoders map images/states to the same token space as language."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"modeling-details",children:"Modeling Details"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conditional flow matching"})," models the continuous distribution over action chunks."]}),"\n",(0,t.jsxs)(n.li,{children:["Train with a ",(0,t.jsx)(n.strong,{children:"diffusion-style loss"})," on individual sequence elements (instead of cross-entropy), with separate weights for diffusion-related tokens."]}),"\n",(0,t.jsxs)(n.li,{children:["Flow path uses a ",(0,t.jsx)(n.strong,{children:"linear-Gaussian"})," schedule; sample noisy actions with \u03B5\u223CN(0, I); predict denoising vector field; ",(0,t.jsx)(n.strong,{children:"Euler integration"})," from \u03C4=0\u21921 at inference."]}),"\n",(0,t.jsxs)(n.li,{children:["Efficient inference by ",(0,t.jsx)(n.strong,{children:"caching"})," K/V for the observation prefix; action tokens recomputed per integration step."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"high-level-language-policy",children:"High-Level Language Policy"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Because the policy consumes language, a ",(0,t.jsx)(n.strong,{children:"high-level VLM"})," can decompose tasks (e.g., bussing) into intermediate language subgoals (SayCan-style planning), improving performance on complex, temporally extended tasks."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-setup--baselines",children:"Evaluation Setup & Baselines"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Out-of-box"})," (direct prompting), ",(0,t.jsx)(n.strong,{children:"fine-tuning"})," on downstream tasks, and ",(0,t.jsx)(n.strong,{children:"with high-level VLM"})," providing intermediate commands."]}),"\n",(0,t.jsxs)(n.li,{children:["Compare against ",(0,t.jsx)(n.strong,{children:"OpenVLA (7B, autoregressive discretization; no action chunks/high-frequency control)"})," and ",(0,t.jsx)(n.strong,{children:"Octo (93M; diffusion)"}),", trained on the same mixture."]}),"\n",(0,t.jsxs)(n.li,{children:["Include a ",(0,t.jsx)(n.strong,{children:"compute-parity"})," \u03C00 (160k steps vs 700k) and a ",(0,t.jsx)(n.strong,{children:"\u03C00-small"})," variant (no VLM init)."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-results",children:"Key Results"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Out-of-box"}),": \u03C00 outperforms all baselines; even compute-parity \u03C00 beats OpenVLA/Octo; \u03C00-small still surpasses them\u2014highlighting the benefits of ",(0,t.jsx)(n.strong,{children:"expressive architectures + diffusion/flow matching + VLM pre-training"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language following"}),": \u03C00 clearly exceeds \u03C00-small across conditions:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\u03C00-flat"}),": only overall task command."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\u03C00-human"}),": human-provided intermediate steps."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\u03C00-HL"}),": high-level VLM-provided steps (fully autonomous)."]}),"\n",(0,t.jsxs)(n.li,{children:["Better language-following accuracy ",(0,t.jsx)(n.strong,{children:"directly translates"})," into stronger autonomous performance with high-level guidance."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"New dexterous tasks"})," (e.g., bowls stacking, towel folding, microwave, drawer items, paper towel replacement):","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Fine-tuned \u03C00 generally outperforms ",(0,t.jsx)(n.strong,{children:"OpenVLA"}),", ",(0,t.jsx)(n.strong,{children:"Octo"}),", and small-data methods ",(0,t.jsx)(n.strong,{children:"ACT"})," / ",(0,t.jsx)(n.strong,{children:"Diffusion Policy"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Pre-training helps most when tasks resemble pre-training data; pretrained \u03C00 often beats from-scratch by up to ",(0,t.jsx)(n.strong,{children:"2\xd7"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complex multi-stage tasks"})," (laundry folding, table bussing, box building, to-go box, eggs):","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u03C00 solves many tasks; ",(0,t.jsx)(n.strong,{children:"full pre-training + fine-tuning"})," performs best."]}),"\n",(0,t.jsxs)(n.li,{children:["Gains from pre-training are ",(0,t.jsx)(n.strong,{children:"especially large"})," on harder tasks; absolute performance varies with task difficulty and pre-training coverage."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"takeaways--limitations",children:"Takeaways & Limitations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u03C00 mirrors LLM training: ",(0,t.jsx)(n.strong,{children:"pre-train for knowledge"}),", ",(0,t.jsx)(n.strong,{children:"post-train for alignment"})," (instruction-following and execution)."]}),"\n",(0,t.jsxs)(n.li,{children:["Limitations/open questions:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Optimal ",(0,t.jsx)(n.strong,{children:"composition/weighting"})," of pre-training data remains unclear."]}),"\n",(0,t.jsxs)(n.li,{children:["Not all tasks work reliably; difficult to predict ",(0,t.jsx)(n.strong,{children:"how much/what kind"})," of data is needed for near-perfect performance."]}),"\n",(0,t.jsxs)(n.li,{children:["Uncertain ",(0,t.jsx)(n.strong,{children:"positive transfer"})," across very diverse tasks/robots and to distinct domains (e.g., driving, navigation, legged locomotion)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"ref",children:"Ref"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B., Jakubczak, S., Jones, T., Ke, L., Levine, S., Li\u2011Bell, A., Mothukuri, M., Nair, S., Pertsch, K., Shi, L. X, \u2026 Zhilinsky, U. (2025, June 21). \u03C0\u2080: A vision\u2011language\u2011action flow model for general robot control Robotics: Science and Systems (RSS), Los Angeles, CA, United States. ",(0,t.jsx)(n.code,{children:"https://roboticsconference.org/program/papers/10/"})]}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},28453(e,n,i){i.d(n,{R:()=>l,x:()=>a});var s=i(96540);let t={},r=s.createContext(t);function l(e){let n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}},87705(e){e.exports=JSON.parse('{"permalink":"/2025/09/01/pi-zero-reivew","source":"@site/blog/2025/09/01/pi-zero-reivew.md","title":"\u03C00 Review","description":"\u03C00 Review","date":"2025-08-31T14:24:33.716Z","tags":[{"inline":false,"label":"vlm","permalink":"/tags/vlm","description":"Vision-Language Models"}],"readingTime":3.81,"hasTruncateMarker":false,"authors":[{"name":"Eunkwang Shin","title":"Owner","url":"https://github.com/gracefullight","socials":{"linkedin":"https://www.linkedin.com/in/gracefullight/","github":"https://github.com/gracefullight","email":"mailto:gracefullight.dev@gmail.com"},"description":"Full Stack JavaScript Developer | Half-time Open Sourcerer.","page":{"permalink":"/authors/me"},"imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me"}],"frontMatter":{"title":"\u03C00 Review","date":"2025-08-31T14:24:33.716Z","description":"\u03C00 Review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"FSD +006","permalink":"/2025/09/01/fundamentals-of-software-development-006"},"nextItem":{"title":"Vima Review","permalink":"/2025/08/31/vima-review"}}')}}]);