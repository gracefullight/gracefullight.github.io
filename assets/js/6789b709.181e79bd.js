"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["17771"],{99325:function(e,i,n){n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>g,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var s=n(55229),a=n(69979),t=n(56774);let r={title:"vision-language models for vision tasks review",date:new Date("2025-08-16T07:40:55.588Z"),description:"vision-language models for vision tasks review",authors:"me",tags:["vlm"]},l=void 0,o={authorsImageUrls:[void 0]},c=[{value:"Overview",id:"overview",level:2},{value:"The development of visual recognition paradigms",id:"the-development-of-visual-recognition-paradigms",level:2},{value:"VLM Overview",id:"vlm-overview",level:2},{value:"VLM pre-training Objectives",id:"vlm-pre-training-objectives",level:2},{value:"Contrastive Objectives",id:"contrastive-objectives",level:3},{value:"Image Contrastive Learning",id:"image-contrastive-learning",level:4},{value:"Image-Text Contrastive Learning",id:"image-text-contrastive-learning",level:4},{value:"Image-Text-Label Contrastive Learning",id:"image-text-label-contrastive-learning",level:4},{value:"Generative Objectives",id:"generative-objectives",level:3},{value:"Alignment Objectives",id:"alignment-objectives",level:3},{value:"VLM Pre-Training Frameworks",id:"vlm-pre-training-frameworks",level:3},{value:"Evaluation",id:"evaluation",level:2},{value:"Zero-shot Prediction",id:"zero-shot-prediction",level:3},{value:"Linear Probing",id:"linear-probing",level:3},{value:"Datasets",id:"datasets",level:2},{value:"Ref",id:"ref",level:2}];function d(e){let i={blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsxs)(i.blockquote,{children:["\n",(0,a.jsx)(i.p,{children:"Most visual recognition studies rely heavily on crowdlabelled data in DNN"}),"\n"]}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Background development of visual recognition paradigms"}),"\n",(0,a.jsx)(i.li,{children:"Foundations its architecture"}),"\n",(0,a.jsx)(i.li,{children:"Datasets in VLM pre-training and evaluations"}),"\n",(0,a.jsx)(i.li,{children:"Review and categorization of existing pre-training methods"}),"\n",(0,a.jsx)(i.li,{children:"Benchmarking analysis discussion"}),"\n",(0,a.jsx)(i.li,{children:"Reach challenges & potential research direction"}),"\n",(0,a.jsxs)(i.li,{children:["Training hard","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"New learning paradigm"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Vision-Language Model Pre-training and Zero-shot Prediction","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Increasing attention"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["VLMs with transfer learning","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Prompt tuning"}),"\n",(0,a.jsx)(i.li,{children:"Visual adaption"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["VLMs with knowledge distillation","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"distill knowledge from VLMs to downstream tasks"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"the-development-of-visual-recognition-paradigms",children:"The development of visual recognition paradigms"}),"\n",(0,a.jsx)(i.mermaid,{value:'graph LR\n  Hand-craftedFeatures["Hand-crafted Features"] --\x3e DNN[DNN + Labeled data] --\x3e SupervisedPretraining["Supervised Pre-training"] --\x3e Self-supervisedPretraining["Self-supervised Pre-training"] --\x3e VisionLanguageZeroShot["Vision-language zero-shot."]'}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Traditional ML: Hand-crafted features for prediction."}),"\n",(0,a.jsx)(i.li,{children:"Deep Learning: Deep networks (e.g., ResNet) with large-scale labeled data."}),"\n",(0,a.jsx)(i.li,{children:"Supervised Pre-training + Fine-tuning: Learned representations transferred to downstream tasks."}),"\n",(0,a.jsx)(i.li,{children:"Unsupervised / Self-supervised Pre-training + Fine-tuning: Objectives like masked modeling and contrastive learning to learn representations."}),"\n",(0,a.jsxs)(i.li,{children:["Vision-Language Models & Zero-shot: Leverage large-scale web data, enabling zero-shot prediction without task-specific fine-tuning.","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Collecting large-scale informative image-text data"}),"\n",(0,a.jsx)(i.li,{children:"Designing high-capacity models for effective learning from Bigdata."}),"\n",(0,a.jsx)(i.li,{children:"Designing new pre-training objectives for learning effective VLMs."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"Illustration of development of VLMs for visual recognition",src:n(59075).Z+"",width:"701",height:"347"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["CLIP: Image-text contrastive objective and learns by pulling the paired images and texts close and pushing others faraway in the embedding space.","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"enables effective usage of web data and allows zero-shot predictions without task-specific finetuning."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"vlm-overview",children:"VLM Overview"}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"VLM Overview",src:n(11152).Z+"",width:"1400",height:"728"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Given Image-text pairs."}),"\n",(0,a.jsx)(i.li,{children:"Employs a text encoder and an image encoder to extract image and text features."}),"\n",(0,a.jsx)(i.li,{children:"Learns the vision-language correlation with certain pre-training objectives."}),"\n",(0,a.jsx)(i.li,{children:"GAP: Global Average Pooling, a technique used to reduce the spatial dimensions of feature maps while retaining important information."}),"\n",(0,a.jsx)(i.li,{children:"ViT: Vision Transformer: Transformers for image recognition at scale."}),"\n",(0,a.jsxs)(i.li,{children:["CNN Based: VGG, ",(0,a.jsx)(i.strong,{children:"ResNet"}),", EfficientNet","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"ResNet: Adopts skip connections between convolutional blocks which mitigates gradient vanishing and explosion and enables DNN training."}),"\n",(0,a.jsx)(i.li,{children:"ResNet-D: Replace global average pooling with transformer multi-head attention."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Transformer Based: ViT","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Adding a normalization layer before the transformer encoder."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"vlm-pre-training-objectives",children:"VLM pre-training Objectives"}),"\n",(0,a.jsx)(i.h3,{id:"contrastive-objectives",children:"Contrastive Objectives"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Pros","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Enforce positive pairs to have similar embeddings in contrast to negative pairs."}),"\n",(0,a.jsx)(i.li,{children:"Encourages VLMs to learn discriminative vision and language features, where more discriminative features lead to more confident and accurate zero-shot predictions."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Cons","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Joint optimizing positive and negative pairs is complicated and challenging."}),"\n",(0,a.jsx)(i.li,{children:"Involves a heuristic temperature hyper-parameter for controlling the feature discriminability."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"image-contrastive-learning",children:"Image Contrastive Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Forcing a query image to be close with its positive keys (its data augmentations)"}),"\n",(0,a.jsx)(i.li,{children:"Faraway from its negative keys (other images)"}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Learn discriminative features"})," in image modality, which often serves as an auxiliary objective for fully exploiting the image data potential."]}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"image-text-contrastive-learning",children:"Image-Text Contrastive Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Pulling the embeddings of paired images and texts close while pushing others away."}),"\n",(0,a.jsx)(i.li,{children:"Minimizing a symmetrical image-text infoNCE loss"}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Learn vision-language correlation"})," by contrasting image-text pairs.","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"CLIP: A symmetrical image-text infoNCE loss"}),"\n",(0,a.jsx)(i.li,{children:"ALIGN: scales up the VLM pre-training with large-scale (but noisy image-text pair with noise-robust contrastive learning)"}),"\n",(0,a.jsx)(i.li,{children:"DeCLIP: Nearest-neighbor supervision to utilize the information from similar pairs, enabling effective pre-training on limited data."}),"\n",(0,a.jsx)(i.li,{children:"OTTER: Optimal transport to pseudo-pair images and texts reducing the required training data."}),"\n",(0,a.jsx)(i.li,{children:"ZeroVL: Limited data resource via debiased data sampling and data augmentation with coin flipping mixup."}),"\n",(0,a.jsx)(i.li,{children:"FILIP: Region-word alignment into contrastive learning, enabling to learn fine-grained vision-language corresponding knowledge."}),"\n",(0,a.jsx)(i.li,{children:"Pyramid-CLIP: Multiple semantic levels and performs both cross-level and peer-level contrastive learning for effective VLM pre-training."}),"\n",(0,a.jsx)(i.li,{children:"LA-CLIP, ALIP: LLM to augment synthetic captions for given images while RA-CLIP retrieves relevant image-text pairs for image-text pair augmentation."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"CLIP",src:n(28623).Z+"",width:"848",height:"528"})}),"\n",(0,a.jsx)(i.h4,{id:"image-text-label-contrastive-learning",children:"Image-Text-Label Contrastive Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Supervised Contrastive Learning into image-text contrastive learning."}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Learn discriminative and task-specific features"})," by exploiting both supervised labels and unsupervised image-text pairs.","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"UniCL: pre-training allows learning both discriminative and task-specific (image classification) features simultaneously with around 900M image-text pairs."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"Image-Text-Label Contrastive Learning",src:n(86307).Z+"",width:"846",height:"558"})}),"\n",(0,a.jsx)(i.h3,{id:"generative-objectives",children:"Generative Objectives"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Masked Image Modelling","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Cross-patch correlation by masking and reconstructing images."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Masked Language Modelling","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Adopted pre-training objectives in NLP."}),"\n",(0,a.jsx)(i.li,{children:"Randomly masking a certain percentage of input tokens and predicting them. (15% in BERT)"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Masked Cross-Modal Modelling","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Integrates masked image modelling and masked language modelling."}),"\n",(0,a.jsx)(i.li,{children:"Given an image-text pair, it randomly masks a subset of image patches and a subset of text tokens and then learns to reconstruct them."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"alignment-objectives",children:"Alignment Objectives"}),"\n",(0,a.jsxs)(i.blockquote,{children:["\n",(0,a.jsx)(i.p,{children:"Align image\u2013text pairs in the embedding space."}),"\n"]}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Image-Text Matching","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["models the ",(0,a.jsx)(i.strong,{children:"overall correlation"})," between an entire image and an entire sentence. (\uC804\uC5ED\uC801 \uC0C1\uAD00\uAD00\uACC4)"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["captures ",(0,a.jsx)(i.strong,{children:"fine-grained correlations"})," between image regions and specific words. (\uC9C0\uC5ED\uC801 \uC0C1\uAD00\uAD00\uACC4)"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"vlm-pre-training-frameworks",children:"VLM Pre-Training Frameworks"}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"VLM pre-training frameworks",src:n(19238).Z+"",width:"715",height:"273"})}),"\n",(0,a.jsx)(i.h2,{id:"evaluation",children:"Evaluation"}),"\n",(0,a.jsx)(i.h3,{id:"zero-shot-prediction",children:"Zero-shot Prediction"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:'Image Classification: classify images into pre-defined categories like "prompt engineering".'}),"\n",(0,a.jsx)(i.li,{children:"Semantic Segmentation: by comparing the embeddings of the given image pixels and texts."}),"\n",(0,a.jsx)(i.li,{children:"Object Detection: localize and classify objects in images with the object locating ability learned from auxiliary datasets."}),"\n",(0,a.jsxs)(i.li,{children:["Image-Text Retrieval","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Text-to-image retrieval that retrieves images based on texts"}),"\n",(0,a.jsx)(i.li,{children:"Image-to-text retrieval that retrieves texts based on images."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"linear-probing",children:"Linear Probing"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"freezes the pre-trained VLM"}),"\n",(0,a.jsx)(i.li,{children:"trains a linear classifier to classify the VLM-encoded embeddings to assess the VLM representations."}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"datasets",children:"Datasets"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["For Pre-training VLMs","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"CLIP, 2021, 400M, English"}),"\n",(0,a.jsx)(i.li,{children:"ALIGN, 2021, 1.8B, English"}),"\n",(0,a.jsx)(i.li,{children:"FILIP, 2021, 300M, English"}),"\n",(0,a.jsx)(i.li,{children:"WebLi, 2022, 12B, 129 Languages"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["For VLM Evaluation","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Image Classification","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"PSACAL VOC 2007 Classification, 11-point mAP"}),"\n",(0,a.jsx)(i.li,{children:"Oxford-IIIT PETS, Mean Per Class"}),"\n",(0,a.jsx)(i.li,{children:"EuroSAT, Accuracy"}),"\n",(0,a.jsx)(i.li,{children:"Hateful Memes, ROC AUC"}),"\n",(0,a.jsx)(i.li,{children:"Country211, Accuracy"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Image-Text Retrieval","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Flickr30k, Recall"}),"\n",(0,a.jsx)(i.li,{children:"COCO Caption, Recall"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Action Recognition","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"UCF101, Accuracy"}),"\n",(0,a.jsx)(i.li,{children:"Kinetics700, Mean(top1, top5)"}),"\n",(0,a.jsx)(i.li,{children:"RareAct, mWAP, mSAP"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Object Detection","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"COCO 2017 Detection, box mAP"}),"\n",(0,a.jsx)(i.li,{children:"LVIS, box mAP"}),"\n",(0,a.jsx)(i.li,{children:"ODinW, box mAP"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Semantic Segmentation","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Cityscapes, Mean IoU"}),"\n",(0,a.jsx)(i.li,{children:"ADE20K, Mean IoU"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"ref",children:"Ref"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Zhang, J., Huang, J., Jin, S., & Lu, S. (2024). Vision-Language Models for Vision Tasks: A Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(8), 5625\u20135644. ",(0,a.jsx)(i.code,{children:"<https://doi.org/10.1109/TPAMI.2024.3369699>"})]}),"\n"]})]})}function g(e={}){let{wrapper:i}={...(0,t.a)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},28623:function(e,i,n){n.d(i,{Z:()=>s});let s=n.p+"assets/images/vlm-clip-002c83bc065d184a2350741cacc71908.png"},86307:function(e,i,n){n.d(i,{Z:()=>s});let s=n.p+"assets/images/vlm-image-text-label-83c01a33a07520e54ac28a0ffbb1ceaa.png"},11152:function(e,i,n){n.d(i,{Z:()=>s});let s=n.p+"assets/images/vlm-overview-2abb2d65aeef690c49e399a7ca3ac86c.png"},59075:function(e,i,n){n.d(i,{Z:()=>s});let s=n.p+"assets/images/vlm-paradigm-46050660982130f887307cc0442975c0.png"},19238:function(e,i,n){n.d(i,{Z:()=>s});let s=n.p+"assets/images/vlm-pretraining-frameworks-39991946607c915ff2a7126f16d59c30.png"},56774:function(e,i,n){n.d(i,{Z:()=>l,a:()=>r});var s=n(52136);let a={},t=s.createContext(a);function r(e){let i=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:i},e.children)}},55229:function(e){e.exports=JSON.parse('{"permalink":"/2025/08/16/vision-language-models-for-vision-tasks-review","source":"@site/blog/2025/08/16/vision-language-models-for-vision-tasks-review.md","title":"vision-language models for vision tasks review","description":"vision-language models for vision tasks review","date":"2025-08-16T07:40:55.588Z","tags":[{"inline":true,"label":"vlm","permalink":"/tags/vlm"}],"readingTime":5.14,"hasTruncateMarker":false,"authors":[{"name":"Eunkwang Shin","title":"Owner","url":"https://github.com/gracefullight","socials":{"linkedin":"https://www.linkedin.com/in/gracefullight/","github":"https://github.com/gracefullight"},"description":"Full Stack JavaScript Developer | Half-time Open Sourcerer.","page":{"permalink":"/authors/me"},"imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me"}],"frontMatter":{"title":"vision-language models for vision tasks review","date":"2025-08-16T07:40:55.588Z","description":"vision-language models for vision tasks review","authors":"me","tags":["vlm"]},"unlisted":false,"nextItem":{"title":"Introduction to AI @003","permalink":"/2025/08/16/introduction-to-ai-003"}}')}}]);