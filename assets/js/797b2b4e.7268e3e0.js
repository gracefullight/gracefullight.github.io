"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["24506"],{99184(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var s=i(95318),l=i(74848),t=i(28453);let o={title:"RoboFlamingo Review",date:new Date("2025-08-31T05:35:06.415Z"),description:"RoboFlamingo Review",authors:"me",tags:["vlm"]},r,a={authorsImageUrls:[void 0]},c=[{value:"RoboFlamingo",id:"roboflamingo",level:2},{value:"Key Idea",id:"key-idea",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Performance",id:"performance",level:2},{value:"Flexibility of Deployment",id:"flexibility-of-deployment",level:2},{value:"Conclusion",id:"conclusion",level:2},{value:"Ref",id:"ref",level:2}];function d(e){let n={h2:"h2",li:"li",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.h2,{id:"roboflamingo",children:"RoboFlamingo"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["RoboFlamingo ",(0,l.jsx)(n.strong,{children:"decouples vision-language understanding and control"}),", using OpenFlamingo for perception and a lightweight policy head for sequential decision-making."]}),"\n",(0,l.jsxs)(n.li,{children:["Unlike prior VLM-based approaches, it requires only ",(0,l.jsx)(n.strong,{children:"small-scale imitation fine-tuning"})," on language-conditioned manipulation data, without large-scale co-fine-tuning."]}),"\n",(0,l.jsxs)(n.li,{children:["This design enables ",(0,l.jsx)(n.strong,{children:"data-efficient, zero-shot generalizable, and deployable"})," robot manipulation policies on modest compute resources."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"key-idea",children:"Key Idea"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Proposes ",(0,l.jsx)(n.strong,{children:"RoboFlamingo"}),", a simple framework to adapt existing VLMs for robotic manipulation with lightweight fine-tuning."]}),"\n",(0,l.jsxs)(n.li,{children:["Built on ",(0,l.jsx)(n.strong,{children:"OpenFlamingo"}),", decoupling ",(0,l.jsx)(n.strong,{children:"vision-language understanding"})," from ",(0,l.jsx)(n.strong,{children:"decision-making"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:["Pre-trained VLM handles ",(0,l.jsx)(n.strong,{children:"language and visual comprehension"}),", while a dedicated ",(0,l.jsx)(n.strong,{children:"policy head models sequential history"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:["Fine-tuned only on ",(0,l.jsx)(n.strong,{children:"language-conditioned manipulation datasets"})," using imitation learning."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Requires only a ",(0,l.jsx)(n.strong,{children:"small amount of demonstrations"})," to adapt to downstream manipulation tasks."]}),"\n",(0,l.jsxs)(n.li,{children:["Provides ",(0,l.jsx)(n.strong,{children:"open-loop control"})," capability \u2192 deployable on low-performance platforms."]}),"\n",(0,l.jsxs)(n.li,{children:["Can be trained/evaluated on a ",(0,l.jsx)(n.strong,{children:"single GPU server"}),", making it a cost-effective and accessible solution."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Evaluated on ",(0,l.jsx)(n.strong,{children:"CALVIN benchmark"})," (34 tasks, 1000 instruction chains)."]}),"\n",(0,l.jsxs)(n.li,{children:["RoboFlamingo achieves ",(0,l.jsx)(n.strong,{children:"2\xd7 performance improvements"})," over previous state-of-the-art methods."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"performance",children:"Performance"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Imitation Learning"}),": Outperforms all baselines across all metrics."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Zero-shot Generalization"}),":","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Vision"}),": Stronger generalization in ABC\u2192D setting."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Language"}),": Robust to GPT-4 generated synonymous instructions."]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Ablation Studies"}),":","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Ignoring history (MLP w/o hist) gives worst results."}),"\n",(0,l.jsx)(n.li,{children:"LSTM and GPT-based policy heads perform best (LSTM chosen as default)."}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"VL pre-training"})," is crucial for downstream manipulation."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Larger VLMs"})," show better data efficiency."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Instruction fine-tuning"})," improves both seen and unseen tasks."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"flexibility-of-deployment",children:"Flexibility of Deployment"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Supports ",(0,l.jsx)(n.strong,{children:"open-loop control"})," by predicting entire action sequences with a single inference \u2192 reduces latency and test-time compute."]}),"\n",(0,l.jsxs)(n.li,{children:["Direct open-loop use without retraining can degrade performance; mitigated with ",(0,l.jsx)(n.strong,{children:"jump-step demonstrations"}),"."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Demonstrates that pre-trained VLMs enable ",(0,l.jsx)(n.strong,{children:"data efficiency"})," and strong ",(0,l.jsx)(n.strong,{children:"zero-shot generalization"})," in robotic manipulation."]}),"\n",(0,l.jsxs)(n.li,{children:["RoboFlamingo is presented as an ",(0,l.jsx)(n.strong,{children:"intuitive, efficient, and open solution"}),", with high potential when combined with large-scale real robot data."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"ref",children:"Ref"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Li, X., Liu, M., Zhang, H., Yu, C., Xu, J., Wu, H., Cheang, C., Jing, Y., Zhang, W., & Liu, H. (2024). Vision-language foundation models as effective robot imitators. International Conference on Learning Representations (ICLR 2024), Vienna, Austria."}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}},28453(e,n,i){i.d(n,{R:()=>o,x:()=>r});var s=i(96540);let l={},t=s.createContext(l);function o(e){let n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:o(e.components),s.createElement(t.Provider,{value:n},e.children)}},95318(e){e.exports=JSON.parse('{"permalink":"/2025/08/31/roboflamingo-review","source":"@site/blog/2025/08/31/roboflamingo-review.md","title":"RoboFlamingo Review","description":"RoboFlamingo Review","date":"2025-08-31T05:35:06.415Z","tags":[{"inline":false,"label":"vlm","permalink":"/tags/vlm","description":"Vision-Language Models"}],"readingTime":1.83,"hasTruncateMarker":false,"authors":[{"name":"Eunkwang Shin","title":"Owner","url":"https://github.com/gracefullight","socials":{"linkedin":"https://www.linkedin.com/in/gracefullight/","github":"https://github.com/gracefullight","email":"mailto:gracefullight.dev@gmail.com"},"description":"Full Stack JavaScript Developer | Half-time Open Sourcerer.","page":{"permalink":"/authors/me"},"imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me"}],"frontMatter":{"title":"RoboFlamingo Review","date":"2025-08-31T05:35:06.415Z","description":"RoboFlamingo Review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"Vima Review","permalink":"/2025/08/31/vima-review"},"nextItem":{"title":"OpenVLA Review","permalink":"/2025/08/29/open-vla-review"}}')}}]);