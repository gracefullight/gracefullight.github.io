"use strict";(self.webpackChunkgracefullight_github_io=self.webpackChunkgracefullight_github_io||[]).push([["27187"],{4124(e,i,n){n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});var s=n(89838),a=n(74848),t=n(28453);let r={title:"Trustworthiness in Vision-Language Models Review",date:new Date("2025-08-17T00:02:41.350Z"),description:"Trustworthiness in Vision-Language Models Review",authors:"me",tags:["vlm"]},l,o={authorsImageUrls:[void 0]},d=[{value:"Overview",id:"overview",level:2},{value:"Privacy",id:"privacy",level:2},{value:"Privacy Issues",id:"privacy-issues",level:3},{value:"Privacy Mitigation Methods",id:"privacy-mitigation-methods",level:3},{value:"Privacy Future Research Directions",id:"privacy-future-research-directions",level:3},{value:"Fairness and Bias",id:"fairness-and-bias",level:2},{value:"Fairness and Bias Issues",id:"fairness-and-bias-issues",level:3},{value:"Fairness and Bias Mitigation Methods",id:"fairness-and-bias-mitigation-methods",level:3},{value:"Fairness Future Research Directions",id:"fairness-future-research-directions",level:3},{value:"Robustness",id:"robustness",level:2},{value:"Robustness Issues",id:"robustness-issues",level:3},{value:"Robustness Mitigation Methods",id:"robustness-mitigation-methods",level:3},{value:"Robustness Future Research Directions",id:"robustness-future-research-directions",level:3},{value:"Safety",id:"safety",level:2},{value:"Safety Issues",id:"safety-issues",level:3},{value:"Safety Mitigation Methods",id:"safety-mitigation-methods",level:3},{value:"Safety Future Research Directions",id:"safety-future-research-directions",level:3},{value:"Ref",id:"ref",level:2}];function c(e){let i={h2:"h2",h3:"h3",li:"li",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Mitigates exposure of private data, produces harmful outputs, or is vulnerable to attacks."}),"\n",(0,a.jsx)(i.li,{children:"SOTA models: LLaVA, Flamingo, GPT-4"}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"privacy",children:"Privacy"}),"\n",(0,a.jsx)(i.h3,{id:"privacy-issues",children:"Privacy Issues"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"risk escalates significantly with relevant images as optimizing in the pixel domain is easier than in text"}),"\n",(0,a.jsx)(i.li,{children:"can unintentionally memorize sensitive data, leading to leaks without knowledge of the model\u2019s specifics"}),"\n",(0,a.jsx)(i.li,{children:"Overfitting may also cause retention of sensitive attributes during inference"}),"\n",(0,a.jsx)(i.li,{children:"gradient-based and backdoor attacks further jeopardize VLM privacy with open-source data"}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"privacy-mitigation-methods",children:"Privacy Mitigation Methods"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"New metrics have been created to assess a model\u2019s ability to reproduce training instances and facilitate cross-model comparisons"}),"\n",(0,a.jsx)(i.li,{children:"models utilizing multiple modalities provide better privacy"}),"\n",(0,a.jsx)(i.li,{children:"safety modules can be integrated to boost resilience against violations"}),"\n",(0,a.jsx)(i.li,{children:"adversarial training can enhance privacy but risks reducing accuracy"}),"\n",(0,a.jsx)(i.li,{children:"New architecture: differentially private CLIP model"}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"privacy-future-research-directions",children:"Privacy Future Research Directions"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Cryptography-based Privacy Preservation","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Secure multi-party computation (SMPC): divides secret information into shares among multiple parties, ensuring that individual shares reveal nothing unless combined"}),"\n",(0,a.jsx)(i.li,{children:"Homomorphic encryption (HE): allows computations on encrypted data without decryption, and has also been utilized for privacy preservation in transformers"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Federated Learning","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"enhances privacy in vision-language models (VLMs) by localizing model training, which protects training data from leakage."}),"\n",(0,a.jsx)(i.li,{children:"challenges such as communication overhead among devices and statistical heterogeneity from diverse data distributions"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Data Manipulation and Finetunning","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Data pseudonymization: substitutes sensitive information with synthetic alternatives."}),"\n",(0,a.jsx)(i.li,{children:"Data Sanitization: removes duplicates to reduce memorization and privacy risks."}),"\n",(0,a.jsx)(i.li,{children:"knowledge sanitization-fine-tuning: provide safe responses when leakage risks arise."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"fairness-and-bias",children:"Fairness and Bias"}),"\n",(0,a.jsx)(i.h3,{id:"fairness-and-bias-issues",children:"Fairness and Bias Issues"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Bias from training data","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"disproportionately features men and lighter-skinned individuals"}),"\n",(0,a.jsx)(i.li,{children:"outdated vocabulary and imbalanced representation"}),"\n",(0,a.jsx)(i.li,{children:"clinical models may favor certain patient groups based on gender, language, etc."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Bias from Model","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Gender biases"}),"\n",(0,a.jsx)(i.li,{children:"misclassification of race-related elements and biased outputs"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"fairness-and-bias-mitigation-methods",children:"Fairness and Bias Mitigation Methods"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["New Datasets and Benchmarks","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Harvard-FairVLMed, PATA, and BOLD enhance evaluations but often lack the scale of established benchmarks."}),"\n",(0,a.jsxs)(i.li,{children:["create synthetic datasets to improve fairness assessments","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"gender-balanced dataset generated with DALL-E-3 and another consisting of gender-swapped images"}),"\n",(0,a.jsx)(i.li,{children:"counterfactual image-text pairs that highlight biases in datasets like COCO Captions"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["new metrics","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"gender polarity"}),"\n",(0,a.jsx)(i.li,{children:"bias distance in embeddings"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.li,{children:"human evaluation"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["De-biasing","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"adjust model instructions and architectures for improved fairness"}),"\n",(0,a.jsx)(i.li,{children:"detecting biased prompts in pre-trained models"}),"\n",(0,a.jsx)(i.li,{children:"Post-hoc Bias Mitigation (PBM) effectively reduce bias in image retrieval"}),"\n",(0,a.jsx)(i.li,{children:"Re-sampling underperforming clusters can enhance fairness"}),"\n",(0,a.jsx)(i.li,{children:"modification of facial features also mitigate biases"}),"\n",(0,a.jsx)(i.li,{children:"self-debiasing reduces biased text generation, especially when paired with other methods"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"fairness-future-research-directions",children:"Fairness Future Research Directions"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Optimized De-biasing","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Additive residual learning: for fairer image representations."}),"\n",(0,a.jsx)(i.li,{children:"Calibration loss: retain semantically similar embeddings."}),"\n",(0,a.jsx)(i.li,{children:"Counterfactual inference framework: help models learn correct responses through cause and effect."}),"\n",(0,a.jsx)(i.li,{children:"Adversarial classifiers: predict image attributes from visual-textual similarities can be combined with instruction tuning to reduce bias."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Disentangled Representation Learning (DRL): simplifies complex data by breaking it in to independent feature groups, improving model predictions.","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Traditional DRL","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Variational autoencoders (VAEs) for feature encoding based on impact"}),"\n",(0,a.jsx)(i.li,{children:"Generative adversarial networks (GANs) for separation."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.li,{children:"Attention in text encoders can be adjusted for fairer outputs."}),"\n",(0,a.jsx)(i.li,{children:'challenges: varying definitions of "disentanglement", ensuring fairness.'}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Human-in-the-Loop (HITL): integrating human intervention into their training to improve precision and fairness","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"active learning"}),"\n",(0,a.jsx)(i.li,{children:"reinforcement learning with human feedback"}),"\n",(0,a.jsx)(i.li,{children:"explainable AI"}),"\n",(0,a.jsx)(i.li,{children:"challenges: human bias, finance, and ethical and legal issues persist"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"robustness",children:"Robustness"}),"\n",(0,a.jsx)(i.h3,{id:"robustness-issues",children:"Robustness Issues"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Out-of-Distribution (OOD) Robustness","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"ChatGPT excels in adversarial tasks but struggles with OOD robustness and informal medical responses"}),"\n",(0,a.jsx)(i.li,{children:"MLLMs often fail to generalize beyond training domains due to mapping issues"}),"\n",(0,a.jsx)(i.li,{children:"vision-language models face difficulties with open-domain concepts, especially when overfitting during fine-tuning"}),"\n",(0,a.jsx)(i.li,{children:"Large pre-trained image classifiers show initial robustness, which diminishes over time"}),"\n",(0,a.jsx)(i.li,{children:"Current visual question answering (VQA) models are limited to specific benchmarks, hindering generalization to OOD datasets"}),"\n",(0,a.jsx)(i.li,{children:"fine-tuning may impair model calibration in OOD contexts."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Adversarial Attack Robustness","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Studies indicate that open-sourced VLMs show performance gaps in red teaming tasks, highlighting the need for improved safety and security."}),"\n",(0,a.jsx)(i.li,{children:'misalignment between language and vision modalities creates a "modality gap", complicating adversarial vulnerability.'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"robustness-mitigation-methods",children:"Robustness Mitigation Methods"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Improving Out-of-Distribution Robustness","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"enhance OOD detection and generalization. A simple maximum logit detector has been shown to outperform complex methods for anomaly segmentation"}),"\n",(0,a.jsx)(i.li,{children:"In-context learning (ICL) can also improve multimodal generalization"}),"\n",(0,a.jsx)(i.li,{children:"A fine-tuned CLIP excels in unsupervised OOD detection"}),"\n",(0,a.jsx)(i.li,{children:"The OGEN method synthesizes OOD features"}),"\n",(0,a.jsx)(i.li,{children:"Maximum Concept Matching aligns visual and textual features, and anchor-based finetuning leads to better domain shifts"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Defense Against Adversarial Attacks","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["VILLA is a two-stage framework for adversarial training of VLMs, featuring task-agnostic ",(0,a.jsx)(i.strong,{children:"adversarial pre-training"})," and ",(0,a.jsx)(i.strong,{children:"task-specific finetuning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"conducts adversarial training in the embedding space rather than on raw image pixels and text tokens, improving the model\u2019s resilience against adversarial examples"}),"\n",(0,a.jsx)(i.li,{children:"SOTA performance across various tasks"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"robustness-future-research-directions",children:"Robustness Future Research Directions"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Data Augmentation","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"MixGen: a data augmentation method that generates new image-text pairs by interpolating images and concatenating text to preserve semantics."}),"\n",(0,a.jsx)(i.li,{children:"creating synthetic images involves extracting text prompts via an image captioning model for use in text-to-image diffusion, then mixing these with real datasets."}),"\n",(0,a.jsx)(i.li,{children:"bimodal augmentation (BiAug): decouples objects and attributes to synthesize vision-language examples and hard negatives, using LLMs and an object detector to generate detailed descriptions and inpaint corresponding images."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Improved Cross-Modal Alignment","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Sharing learnable parameters"}),"\n",(0,a.jsx)(i.li,{children:"Applying bidirectional constraints"}),"\n",(0,a.jsx)(i.li,{children:"Adjusting cross-modal projections"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.li,{children:"challenges: addressing the modality gap, which impacts robustness to OOD data and adversarial examples"}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"safety",children:"Safety"}),"\n",(0,a.jsx)(i.h3,{id:"safety-issues",children:"Safety Issues"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Toxicity","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"LAION-400M: contains problematic content, including explicit materials and harmful stereotypes"}),"\n",(0,a.jsx)(i.li,{children:"Advanced models like GeminiProVision and GPT-4V show inherent biases"}),"\n",(0,a.jsx)(i.li,{children:"Assigning personas to ChatGPT can increase toxicity and reinforce harmful stereotypes"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Jailbreaking Risk","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Perturbation can be performed effectively, while FigStep converts harmful content into images with an 82.5% attack rate across multiple VLMs"}),"\n",(0,a.jsx)(i.li,{children:"replaces captions with malicious prompts, enabling jailbreaks."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"safety-mitigation-methods",children:"Safety Mitigation Methods"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Safety Fine-Tuning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"VLGuard"}),"\n",(0,a.jsx)(i.li,{children:"fine-tuned on synthetic data, reducing sensitivity to NSFW inputs and enhancing performance in cross-modal tasks"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Other approach","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Reinforce-Detoxify: uses reinforcement learning to mitigate toxicity and bias in transformer models"}),"\n",(0,a.jsx)(i.li,{children:"simple mitigations improve automatic scores, these methods risk over-filtering marginalized texts and create discrepancies between automatic and human judgments"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"safety-future-research-directions",children:"Safety Future Research Directions"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Context Awareness","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"integrating Chain-of-Thought for improved reasoning can enhance CAER tasks with Large VLMs."}),"\n",(0,a.jsx)(i.li,{children:"Dual-Aligned Prompt Tuning: combines explicit context from pre-trained LLMs with implicit modeling to create more context-aware prompts"}),"\n",(0,a.jsx)(i.li,{children:"Visual In-Context Learning: optimizes image retrieval and summarization to enhance task-specific interactions."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["Automated Red Teaming (ART)","\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"RTVLM: a dataset that benchmarks VLMs across faithfulness, privacy, safety, and fairness"}),"\n",(0,a.jsx)(i.li,{children:"Arondight: automates multi-modal jailbreak attacks using reinforcement learning and uncovers significant security vulnerabilities"}),"\n",(0,a.jsx)(i.li,{children:"GPT-4 and GPT-4V are more robust against jailbreaks than open-source models"}),"\n",(0,a.jsx)(i.li,{children:"limited transferability of visual jailbreak methods compared to textual ones"}),"\n",(0,a.jsx)(i.li,{children:"connects unsafe outputs to prompts, improving the detection of vulnerabilities in text-to-image models"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"ref",children:"Ref"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Vu, K., & Lai, P. (2025). Trustworthiness in\xa0Vision-Language Models. In J. Kertesz, B. Li, T. Supnithi, & A. Takhom, Computational Data and Social Networks Singapore."}),"\n"]})]})}function h(e={}){let{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},28453(e,i,n){n.d(i,{R:()=>r,x:()=>l});var s=n(96540);let a={},t=s.createContext(a);function r(e){let i=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:i},e.children)}},89838(e){e.exports=JSON.parse('{"permalink":"/2025/08/17/trustworthiness-in-vision-language-models-review","source":"@site/blog/2025/08/17/trustworthiness-in-vision-language-models-review.md","title":"Trustworthiness in Vision-Language Models Review","description":"Trustworthiness in Vision-Language Models Review","date":"2025-08-17T00:02:41.350Z","tags":[{"inline":false,"label":"vlm","permalink":"/tags/vlm","description":"Vision-Language Models"}],"readingTime":5.9,"hasTruncateMarker":false,"authors":[{"name":"Eunkwang Shin","title":"Owner","url":"https://github.com/gracefullight","socials":{"linkedin":"https://www.linkedin.com/in/gracefullight/","github":"https://github.com/gracefullight","email":"mailto:gracefullight.dev@gmail.com"},"description":"Full Stack JavaScript Developer | Half-time Open Sourcerer.","page":{"permalink":"/authors/me"},"imageURL":"https://avatars.githubusercontent.com/u/11773683?v=4","key":"me"}],"frontMatter":{"title":"Trustworthiness in Vision-Language Models Review","date":"2025-08-17T00:02:41.350Z","description":"Trustworthiness in Vision-Language Models Review","authors":"me","tags":["vlm"]},"unlisted":false,"prevItem":{"title":"Zotero \uCD08\uAE30 \uC138\uD305","permalink":"/2025/08/17/zotero-initial-setup"},"nextItem":{"title":"Vision-Language Models for Vision Tasks Review","permalink":"/2025/08/16/vision-language-models-for-vision-tasks-review"}}')}}]);