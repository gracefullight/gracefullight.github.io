---
title: sLLM
date: 2024-05-05T21:00:13.862+09:00
description: small LLM
authors: me
tags:
  - pe
  - pe/algo
---

## sLLM 개요

### sLLM 개념

> small Large Language Model

- 기업 전용 특화모델로 활용 가능한 경량화된 거대 언어모델
- 지식 증류(Knowledge Distillation) 기법을 통해 LLM의 성능을 작고 효율적인 모델로 전달

### sLLM 배경

- LLM은 거대 언어모델로 천문학적인 비용과 학습 시간 필요
- 매개변수를 줄이고, 미세조정하여 정확도 향상

## sLLM 생성 과정

```mermaid
graph LR
  데이터수집[1 데이터 수집 및 전처리]
  사전학습[2 대형 LLM 사전 학습 데이터 활용]
  증류[3 지식 증류/Knowledge Distillation]
  경량화[4 PEFT/매개변수 효율적 미세조정]
  배포[5 최적화 및 배포]

  데이터수집 -->|기업 맞춤 데이터| 사전학습
  사전학습 -->|대규모 학습된 LLM 활용| 증류
  증류 -->|학생 모델 학습| 경량화
  경량화 -->|양자화/파라미터 감소| 배포
```

## LLM과 sLLM 비교

| 구분 | LLM | sLLM |
| --- | --- | --- |
| 훈련 데이터 크기 | 대규모, 대용량 | 상대적으로 작은 규모, 소용량 |
| 파라미터 크기 | 수천억 개 | 수십억 개 |
| 성능 | 더많은 컨텍스트와 언어이해능력 | 작은 모델로 일부 성능 제한 |
| 배포 용이성 | 대용량으로 배포 어려움 | 작은 규모로 배포 용이 |
| 사용성 | 학습 리소스 등 자원 사용량이 많아 운영 제약 | 경량화 된 모델로 제한된 자원으로 활용 가능 |
| 서비스 제공 | 클라우드 기반에서 범용 서비스 적합 | 온프레미스 방식으로 기업내 구축 가능 |
| 예시 | ChatGPT, Gemini | LLama, Phi-3 |
